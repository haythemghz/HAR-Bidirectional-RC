\documentclass[pdflatex]{sn-jnl} % Math and Physical Sciences Numbered Reference Style

% Standard Packages
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage[title]{appendix}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{manyfoot}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{tabularx}
\usepackage{ulem}
\usepackage{array}
\usepackage{adjustbox}
\usepackage{natbib}
\usepackage[bookmarks=false]{hyperref}
\usepackage{caption}
\usepackage{float}
\usepackage{multicol}
\usepackage{eqparbox}
\usepackage{diagbox}
\usepackage{color, colortbl}
\usepackage{epsf}
\usepackage{pdfpages}
\usepackage{calc}
\usepackage{psfrag}
\usepackage{url}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{lipsum}
\usepackage{stfloats}
\usepackage{verbatim}

\begin{document}

\title[Bidirectional Reservoir Computing for Enhanced Human Action Recognition Using Skeleton Data]{Bidirectional Reservoir Computing for Enhanced Human Action Recognition Using Skeleton Data}

\author[1,2]{\fnm{Haythem} \sur{Ghazouani}}\email{haythem.ghazouani@enicar.u-carthage.tn}

\author[1,2]{\fnm{Walid} \sur{Barhoumi}}\email{walid.barhoumi@enicarthage.rnu.tn}

\equalcont{These authors contributed equally to this work.}

\affil[1]{\orgdiv{Universit de Tunis El Manar}, \orgname{Institut Suprieur d'Informatique, Research Team on Intelligent Systems in Imaging and Artificial Vision (SIIVA), LR16ES06 Laboratoire de recherche en Informatique, Modlisation et Traitement de l'Information et de la Connaissance (LIMTIC)}, \orgaddress{\street{2 Rue Abou Rayhane Bayrouni}, \city{Ariana}, \postcode{2080}, \country{Tunisia}}}

\affil[2]{\orgdiv{Universit de Carthage}, \orgname{Ecole Nationale d'Ingnieurs de Carthage}, \orgaddress{\street{45 Rue des Entrepreneurs}, \city{Tunis-Carthage}, \postcode{2035}, \country{Tunisia}}}

\abstract{
The quest for effective recognition of human actions %human action recognition %(HAR) 
has led researchers to explore skeleton-based approaches that effectively address the privacy concerns, computational burdens, and environmental sensitivities inherent in %RGB and
video-based methods. 
However, %the journey toward 
robust temporal modeling has been fraught with challenges, as conventional approaches such as recurrent neural networks and Long Short-Term Memory (LSTM) networks struggle with vanishing gradients, exploding gradients, and the intricate task of capturing long-term dependencies. 
%in human motion sequences. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This work introduces a %novel 
bidirectional reservoir computing framework that fundamentally reimagines how we process temporal information in skeleton-based action recognition. Our approach integrates three innovative components: a bidirectional reservoir architecture that processes sequences in both forward and backward directions, capturing the full temporal context of human actions; an adaptive multi-view dimensionality reduction module that  %intelligently 
combines principal component analysis with Tucker decomposition to distill essential motion patterns; and an advanced 
readout mechanism enhanced with advanced training strategies. Through extensive experimentation across benchmark datasets, our framework %demonstrates 
shows remarkable performance, achieving 98.91\% accuracy on UTD-MHAD, 97.5\% on MSR Action3D, and 98.5\% on CZU-MHAD, while %dramatically 
drastically reducing training time by 95\% and inference time by 93\% compared to %traditional 
LSTM networks. Comprehensive ablation studies reveal that bidirectional processing alone contributes a substantial 3.2\% improvement in accuracy, underscoring the importance of leveraging both past and future temporal contexts to understand human action. %understanding.
}

\keywords{Human Action Recognition, Skeleton Data, Reservoir Computing, Bidirectional Processing, Echo State Networks, Temporal Modeling}

\maketitle

\section{Introduction}
In the ever-evolving landscape of computer vision and human behavior analysis, the ability to automatically recognize and interpret human actions has emerged as one of the most compelling and challenging research frontiers. This capability holds the promise of revolutionizing numerous domains, from healthcare monitoring systems to intelligent surveillance networks.

The development of effective Human Action Recognition (HAR) has been closely tied to the choice of input modality. Early approaches primarily relied on RGB video sequences due to their rich visual content; however, such methods often suffer from variations in lighting, background clutter, and occlusion. These limitations have motivated a shift toward alternative data representations. In particular, skeleton-based HAR has emerged as a promising direction, emphasizing the geometric configuration and temporal evolution of body joints rather than appearance-based cues.
The transition from RGB-based to skeleton-based HAR represents more than a mere technical choice; it embodies a philosophical shift in how we conceptualize human action recognition. Traditional RGB-based approaches, while visually intuitive, carry inherent limitations that have become increasingly apparent as the field has matured. Consider the privacy implications that have become paramount in our data-conscious society. RGB video captures not only the intended action but also personally identifiable information, facial features, and contextual details that may be irrelevant to the recognition task yet problematic from a privacy standpoint. Healthcare applications, where HAR systems might monitor patients in their homes or clinical settings, particularly benefit from skeleton-based approaches that preserve the essential motion information while protecting individual privacy.
Skeleton-based methods exhibit a high degree of environmental robustness. In contrast to RGB-based systems, which are sensitive to lighting variations, background clutter, and changes in camera viewpoint, skeleton-based methods tend to maintain stable performance across diverse environments. This resilience is attributed to their reliance on the geometric configuration of the body joints, which remains largely invariant to visual disturbances that typically affect appearance-based methods. This property is particularly beneficial for cross-dataset generalization and long-term deployment scenarios, where appearance changes are common and unavoidable.
From a computational perspective, the efficiency gains are remarkable. Where RGB-based deep learning models must process high-resolution frames through complex convolutional architectures, requiring substantial computational resources, skeleton-based methods work with compact representations of human pose through joint coordinates, reducing input dimensionality by orders of magnitude while preserving the essential motion information needed for accurate recognition.
An important advantage of skeleton-based approaches is their inherent invariance to appearance-related variations, which often hinder the performance of RGB-based systems. Factors such as clothing, body shape, and demographic characteristics---which can significantly affect traditional appearance-based methods---become largely irrelevant when recognition is based on the underlying motion patterns. This property is particularly beneficial for cross-dataset generalization and long-term deployment scenarios, where appearance changes are common and unavoidable.
Despite the advantages of skeleton-based HAR, their practical deployment remains constrained by several technical challenges. Most significant of these is the temporal modeling of human actions, specifically the need to effectively capture the complex temporal dependencies that characterize various types of human movement. Traditional Recurrent Neural Networks (RNNs) exhibit fundamental limitations in the context of skeleton-based HAR. The vanishing gradient problem \cite{bengio1994learning} becomes particularly pronounced when modeling long action sequences. As gradients propagate backward through time, they tend to decay exponentially, hindering the network's ability to learn long-term dependencies that are essential for distinguishing between similar actions or recognizing those that evolve over extended durations.
The introduction of Long Short-Term Memory (LSTM) networks constituted a key development in addressing the vanishing gradient problem, owing to their gating mechanisms that regulate information flow across time steps. However, this advancement introduces considerable computational overhead. Indeed, LSTM training complexity scales quadratically with sequence length and cubically with the hidden state dimension, making them inefficient for long or high-dimensional sequences. In addition, LSTMs require careful hyperparameter tuning, including learning rate scheduling, gradient clipping, and gate initialization, which makes them sensitive to configuration choices and challenging to optimize in practice.
Furthermore, a particularly significant limitation of current approaches lies in their unidirectional processing paradigm. Most existing methods process temporal sequences in a single direction, from past to present, failing to leverage future context information that could significantly improve action recognition accuracy. While bidirectional LSTMs exist and can address this limitation, they come with doubled computational cost and parameter count, exacerbating the scalability issues already present in standard LSTM architectures.
The challenge is further compounded by the high-dimensional feature spaces generated by deep temporal models. These high-dimensional representations, while potentially expressive, create difficulties for effective learning when training data is limited, which is a common scenario in HAR where motion capture data collection is expensive and time-consuming. Traditional dimensionality reduction techniques often fail to preserve complex spatio-temporal relationships that are essential for accurate action recognition, leading to a loss of discriminative information.

\textbf{Research Gap:} Despite these advancements, a critical gap remains: current RC-based approaches for HAR are predominantly unidirectional or lack deep integration with modern dimensionality reduction techniques capable of handling complex 3D skeletal data. Furthermore, while bidirectional RNNs exist, they suffer from high computational overhead. There is currently no framework that effectively combines the computational efficiency of RC with the temporal robustness of bidirectional processing and the representational power of tensor decomposition for skeleton-based HAR.

In response to these challenges, we propose a fundamentally different approach to skeleton-based HAR based on Reservoir Computing (RC). The proposed bidirectional RC framework addresses key limitations of traditional sequence models by introducing architectural innovations that enhance temporal modeling while maintaining computational efficiency.

The core idea of our approach is that effective temporal modeling does not require the complex, parameter-intensive architectures typical of conventional deep learning models. By leveraging the principles of RC, we decouple temporal feature extraction from supervised learning, enabling a more computationally efficient and structurally elegant solution without compromising performance. In fact, the proposed framework incorporates three key contributions designed to work in synergy. First, we propose the first comprehensive bidirectional RC architecture for skeleton-based HAR. It processes input sequences in both temporal directions, capturing richer temporal dependencies while preserving the lightweight computational profile of reservoir models. By integrating both past and future context at each time step, the model overcomes the limitations of unidirectional processing, resulting in more accurate and context-aware action recognition.
Second, we design an adaptive multi-view dimensionality reduction module that combines Principal Component Analysis (PCA) with Tucker decomposition to efficiently reduce the high-dimensional reservoir states while preserving critical spatiotemporal features. This module mitigates the curse of dimensionality commonly encountered in high-capacity temporal models, enabling scalable learning across complex action recognition tasks.
Third, we design advanced readout mechanisms that transform the reservoir's temporal representations into action class predictions. Using non-linear activation functions and optimized training strategies, these mechanisms ensure effective utilization of the rich dynamics captured by the bidirectional reservoir, resulting in accurate and robust classification performance.
Indeed, we demonstrate state-of-the-art accuracy across multiple benchmark datasets alongside substantial improvements in computational efficiency, showing that high performance and efficiency can be simultaneously achieved.

The remainder of this paper unfolds as follows. Section~\ref{sec:related_work} provides a comprehensive survey of related work, positioning our contributions within the broader research landscape. Section~\ref{sec:background} establishes the theoretical foundations that underpin our approach. Section~\ref{sec:methodology} presents the proposed bidirectional reservoir computing framework in detail. Section~\ref{sec:experiments} reports extensive experimental results that validate the proposed approach. Finally, Section~\ref{sec:conclusion} concludes the paper and outlines future research directions.

\section{Related Work}\label{sec:related_work}
The development of effective HAR has been driven by continuous innovation and evolving methodologies, with a persistent focus on accurately modeling the temporal dynamics of human movement. This section reviews the progression of skeleton-based HAR techniques, from early handcrafted feature approaches to deep learning models, and highlights the emerging potential of reservoir computing methods.

\subsection{Handcrafted Feature Approaches}\label{subsec:traditional_approaches}
Early research in skeleton-based HAR primarily focused on handcrafted feature extraction, where geometric and temporal patterns distinguishing human actions were manually identified and encoded. Although these approaches were limited by their dependence on domain expertise and manual engineering, they established foundational concepts that continue to inform contemporary methods.

Xia et al. \cite{xia2012view} made significant contributions to view-invariant HAR by developing histograms of 3D joint locations that could maintain recognition accuracy across different camera viewpoints. Their approach demonstrated the importance of geometric invariance in skeleton-based recognition, though it struggled with the subtle motion patterns that characterize complex actions. It highlighted a fundamental challenge that would persist throughout the evolution of HAR methods: balancing computational efficiency with the ability to capture nuanced temporal dynamics.
The concept of actionlets, introduced by Wang et al. \cite{wang2012mining}, represented an important conceptual advance by decomposing complex actions into simpler motion primitives. This hierarchical approach to action understanding provided insights into the compositional nature of human movement, suggesting that complex actions could be understood as combinations of simpler components. However, the approach remained limited by its dependence on predefined motion primitives and its inability to adapt to action patterns not anticipated during the design phase. 
Vemulapalli et al. \cite{vemulapalli2014human} introduced a mathematically elegant approach to skeleton-based HAR by representing skeleton sequences as curves in the Lie group SE(3), enabling the use of advanced geometric tools for action analysis. Their method explicitly modeled 3D geometric relationships between body parts using rotations and translations, with classification performed using dynamic time warping, Fourier temporal pyramid representation, and linear SVM. While this method demonstrated the potential of principled mathematical frameworks for HAR, the computational complexity of the underlying geometric operations limited its scalability to large datasets.
These traditional approaches, though innovative for their time, exhibited common limitations that motivated the transition to more advanced techniques. Their reliance on manual feature engineering made performance heavily dependent on domain expertise and prior knowledge of the target actions. Furthermore, handcrafted features frequently failed to capture the full complexity of human motion, especially for actions involving subtle temporal dynamics or intricate multi-joint coordination.

\subsection{Deep Learning: RNN and Graph-Based Methods}\label{subsec:deep_learning_approaches}
The advent of deep learning fundamentally transformed skeleton-based HAR by enabling the development of sophisticated architectures capable of automatically learning temporal dependencies and spatial relationships within skeletal data.
In particular, the introduction of RNNs to skeleton-based HAR marked a pivotal advancement in the field development. Du et al. \cite{du2015hierarchical} spearheaded this transition with a hierarchical RNN architecture that partitioned the human body into five anatomical segments, each modeled by a dedicated RNN to capture its temporal dynamics. This approach highlighted the potential of automatic temporal feature learning and introduced the important concept of hierarchical processing in HAR.
Building upon these foundations, Shahroudy et al. \cite{shahroudy2016ntu} introduced the large-scale NTU RGB+D dataset and proposed a part-aware LSTM network that splits the body into five parts, using specially designed LSTM cells to extract context features for each body part. This approach marked a significant advancement by modeling long-term temporal correlations of features for each body part, achieving strong performance on action classification tasks. Their work underscored the importance of part-based modeling in enhancing HAR systems.
Zhang et al. \cite{zhang2017view} addressed another critical challenge in skeleton-based HAR by proposing view-adaptive RNNs that learned view-invariant representations through adversarial training. Their work tackled the important problem of viewpoint variations in skeleton-based HAR, demonstrating how adversarial learning principles could be applied to achieve robustness across different camera perspectives.
The culmination of RNN-based approaches came with the work of Liu et al. \cite{liu2016spatio}, who developed spatio-temporal LSTM networks with trust gates for 3D human action recognition. Their method employed a tree-structure based traversal method to model the kinematic relationships and spatial dependencies between joints, effectively feeding the skeletal structure into sequential LSTM networks while maintaining temporal modeling capabilities.
The recognition that human skeletons possess inherent graph structure led to the development of Graph Convolutional Network (GCN) approaches that could explicitly model the spatial relationships between joints. Yan et al. \cite{yan2018spatial} introduced Spatial-Temporal Graph Convolutional Networks (ST-GCN), treating skeleton data as graphs where joints serve as nodes and bones as edges. This approach naturally captured spatial relationships between joints while employing temporal convolutions for temporal modeling. Shi et al. \cite{shi2019two} extended this paradigm with two-stream adaptive graph convolutional networks that modeled both first-order information (joint coordinates) and second-order information (bone lengths and directions) simultaneously. Their method demonstrated the value of multi-stream processing in capturing different aspects of human movement, with joint and bone streams processed separately before fusion, achieving state-of-the-art performance on multiple benchmarks.
Cheng et al. \cite{cheng2020skeleton} further advanced graph-based methods with shift graph convolutional networks that employed learnable shift operations to capture multi-scale temporal patterns. Their approach demonstrated the importance of multi-scale temporal modeling in skeleton-based HAR.
The success of transformer architectures in natural language processing inevitably led to their exploration in skeleton-based HAR. Plizzari et al. \cite{plizzari2021spatial} pioneered the application of transformers to skeleton-based action recognition, demonstrating competitive performance while providing interpretable attention maps that revealed which temporal segments and spatial joints were most important for different actions.

\subsection{Reservoir Computing for Temporal Sequence Processing}\label{subsec:reservoir_computing_approaches}
While the mainstream HAR research community was exploring complex deep learning architectures, a parallel stream of research was investigating RC as an alternative paradigm for temporal sequence processing. This approach, rooted in the principles of dynamical systems theory, offered a fundamentally different perspective on temporal modeling.
The theoretical foundations of RC were established through the pioneering work of Jaeger \cite{jaeger2001echo} on Echo State Networks (ESN) and Maass et al. \cite{maass2002real} on liquid state machines. These approaches demonstrated that effective temporal processing could be achieved through fixed, randomly initialized recurrent networks that project input sequences into high-dimensional spaces where simple linear readout layers could perform classification or regression.
The effectiveness of the RC paradigm lay in its separation of temporal feature extraction from supervised learning. Unlike traditional RNNs where all parameters must be learned through backpropagation, RC fixes the temporal processing components and only trains the final readout layer. This separation not only dramatically reduces computational complexity but also provides theoretical guarantees about the temporal processing capabilities of the system.
Lukoeviius and Jaeger \cite{lukosevicius2009reservoir} provided comprehensive theoretical foundations for RC, establishing the importance of the echo state property and spectral radius tuning for stable reservoir dynamics. They demonstrated that properly configured reservoirs could achieve universal approximation capabilities for temporal sequences while maintaining computational efficiency that far exceeded RNN approaches.
The application of reservoir computing to pattern recognition tasks demonstrated the practical potential of this paradigm. Verstraeten et al. \cite{verstraeten2007experimental} provided an experimental unification of different reservoir computing methods, comparing various implementations across multiple benchmark tasks. Their comprehensive analysis established RC as a viable approach for temporal sequence processing across diverse application domains.
Gallicchio and Micheli \cite{gallicchio2017deep} explored deep reservoir architectures through critical experimental analysis, demonstrating how multiple reservoir layers could be stacked to create hierarchical representations of time. Their research showed that deep layered organization of RC models influences the occurrence of multiple time-scales and increases the richness of dynamics, measured as the entropy of recurrent unit activations, while also improving short-term memory capacity.
The application of RC to HAR has been limited but promising. Picco et al. \cite{PICCO2023662} introduced novel training methods for RC in HAR contexts, using "timesteps of interest" to effectively combine short and long time scales. Their approach achieved high accuracy on video-based datasets while maintaining real-time processing capabilities.
Antonik et al. \cite{Antonik2019} explored photonic hardware implementations of RC for HAR, demonstrating the potential for ultra-fast processing using optical components. Most relevant to our work, Gallicchio et al. \cite{gallicchio2016reservoir} developed reservoir computing approaches for human gesture recognition from Kinect data, representing one of the few works directly addressing skeleton-based HAR with RC. However, their approach lacked comprehensive evaluation and comparison with state-of-the-art deep learning methods.
Overall, the survey of the literature reveals several significant research gaps that motivate the proposed work. Limited bidirectional processing in RC exists, as while bidirectional processing has proven valuable in RNN-based approaches, most RC-based HAR methods employ unidirectional processing, potentially missing valuable future context information that could improve recognition accuracy. Insufficient comparative analysis is evident, as existing RC studies for HAR lack comprehensive comparison with state-of-the-art deep learning methods, particularly bidirectional LSTMs and GRU networks, making it difficult to assess the true potential of RC approaches.
Scalability challenges persist, as current approaches struggle with the high-dimensional reservoir states generated by complex temporal sequences, requiring more sophisticated dimensionality reduction strategies that can preserve essential temporal dynamics while improving computational efficiency. Limited theoretical understanding exists, as most studies lack detailed computational complexity analysis and theoretical justification for design choices, hindering the development of principled approaches to RC for HAR.
The proposed work addresses these gaps by introducing a comprehensive bidirectional RC framework with extensive experimental evaluation, theoretical analysis, and systematic comparison with state-of-the-art approaches. Table~\ref{tab:related_work_summary} summarizes the evolution of skeleton-based HAR methods and positions our contribution within this research landscape.
This historical perspective positions our work as a natural progression in the evolution of HAR research, uniting the computational efficiency of RC with the advanced temporal modeling strategies developed over decades of research in RNN-based and graph-based methods.
\begin{table}[!htbp]
\centering
\caption{Evolution of skeleton-based HAR approaches: from traditional methods to the proposed framework.}
\label{tab:related_work_summary}
\begin{tabular}{p{2cm}p{3cm}p{3cm}p{3cm}}
\toprule
\textbf{Era} & \textbf{Representative Approaches} & \textbf{Key Innovations} & \textbf{Limitations} \\
\midrule
\textbf{Traditional Machine Learning} & Handcrafted features \cite{xia2012view}, Actionlets \cite{wang2012mining}, Lie groups \cite{vemulapalli2014human} & View invariance, hierarchical decomposition, mathematical foundations & Manual engineering, limited scalability, poor generalization \\
\midrule
\textbf{RNN Era} & Hierarchical RNNs \cite{du2015hierarchical}, Part-aware LSTMs \cite{shahroudy2016ntu}, View-adaptive RNNs \cite{zhang2017view} & Automatic feature learning, attention mechanisms, view adaptation & Vanishing gradients, computational complexity, hyperparameter sensitivity \\
\midrule
\textbf{Graph Era} & ST-GCN \cite{yan2018spatial}, Two-stream GCNs \cite{shi2019two}, Multi-scale approaches \cite{cheng2020skeleton} & Structural modeling, multi-stream processing, multi-scale patterns & Complex design, high memory requirements, limited interpretability \\
\midrule
\textbf{Transformer Era} & Skeleton transformers \cite{plizzari2021spatial} & Long-range dependencies, interpretable attention & Data requirements, computational complexity \\
\midrule
\textbf{RC Exploration} & Timesteps of interest \cite{PICCO2023662}, Photonic RC \cite{Antonik2019}, Bidirectional ESNs \cite{gallicchio2016reservoir}, PAR-Net \cite{khan2024parnet} & Computational efficiency, hardware potential, bidirectional processing & Limited evaluation, lack of deep learning comparison \\
\bottomrule
\end{tabular}
\end{table}

Recent advancements have further explored efficient architectures. Li et al. \cite{li2025harmamba} introduced HARMamba, leveraging bidirectional state-space models for wearable sensor HAR, demonstrating the growing importance of bidirectional modeling in resource-constrained environments. Similarly, Khan and Lee \cite{khan2024parnet} proposed PAR-Net, combining CNNs with ESNs, yet primarily focused on physical activity rather than complex skeletal actions. Unlike these approaches, our framework integrates bidirectional reservoirs with tensor-based dimensionality reduction to specifically address the high-dimensional challenges of skeleton data.


\section{Theoretical Background for Temporal Dynamics Modeling}


\label{sec:background}

%\textcolor{red}{The quest to understand and model temporal dynamics in %human action recognition 
%HAR requires a deep understanding of the mathematical foundations that govern sequential data processing.} 
%This section establishes the theoretical groundwork for our approach, weaving together concepts from \textcolor{red}{dynamical systems theory}, RNNs, %recurrent neural networks, 
%and reservoir computing to create a comprehensive framework for understanding temporal modeling within human actions. 
%\textcolor{red}{ITEMS 3}
%HAR.

%\subsection{Mathematical Representation of Human Actions}

\label{subsec:mathematical_representation}

Human actions, when viewed through the %lens of 
skeleton-based representation, reveal themselves as elegant mathematical patterns that evolve through time and space. Each moment in an action sequence captures a snapshot of human pose through the 3D coordinates of anatomical joints, creating a sequence of spatiotemporal feature vectors that encodes the essence of human movement. 
%Consider the mathematical representation of this temporal evolution. A
Formally, at any discrete time step $t$, the human skeleton configuration can be captured through a collection of $N$ anatomical joints $\mathcal{J} = \{j_1, j_2, \ldots, j_N\}$, where each joint $j_i$ is characterized by its Cartesian coordinates $(x_i, y_i, z_i)$ in %three-dimensional 
3D space. This spatial configuration gives rise to a feature vector $\mathbf{x}(t) \in \mathbb{R}^{3N}$ that encapsulates the complete pose information: % at time $t$:

\begin{equation}
\mathbf{x}(t) = [x_1(t), y_1(t), z_1(t), x_2(t), y_2(t), z_2(t), \ldots, x_N(t), y_N(t), z_N(t)]^T
\end{equation}

The temporal dimension emerges as we observe the evolution of these pose configurations across time. A complete action sequence spanning $T$ time steps forms a spatiotemporal matrix $\mathbf{X} \in \mathbb{R}^{T \times 3N}$ that captures the full trajectory of human motion: % movement:

\begin{equation}
\mathbf{X} = [\mathbf{x}(1), \mathbf{x}(2), \ldots, \mathbf{x}(T)]^T
\end{equation}

This mathematical representation reveals the fundamental challenge of skeleton-based HAR: learning a mapping function $f: \mathbb{R}^{T \times 3N} \rightarrow \{1, 2, \ldots, C\}$ that can assign each spatiotemporal sequence $\mathbf{X}$ to one of $C$ action classes. The complexity of this mapping lies not merely in the high-dimensional nature of the input space, but in the intricate temporal dependencies that characterize human movement patterns.

\subsection{Recurrent Neural Networks}
\label{subsec:rnn_temporal_modeling}

The evolution of temporal modeling in deep learning has been driven by the challenge of capturing long-term dependencies. Recurrent Neural Networks (RNNs) represent the foundational architecture for sequential modeling, processing pose sequences in skeleton-based HAR through recursive state transitions:

\begin{equation}
\mathbf{h}(t) = \sigma(\mathbf{W}_{ih}\mathbf{x}(t) + \mathbf{W}_{hh}\mathbf{h}(t-1) + \mathbf{b}_h),
\qquad
\hat{\mathbf{y}} = \text{softmax}(\mathbf{W}_{out}\mathbf{h}(T) + \mathbf{b}_{out}),
\label{eq:rnn_forward}
\end{equation}

where $\mathbf{h}(t)$ denotes the hidden state at time $t$, $\mathbf{W}_{ih}$ and $\mathbf{W}_{hh}$ are the input and recurrent weights, and $\sigma(\cdot)$ is a nonlinear activation. Each hidden state acts as a memory unit that integrates information across time, enabling temporal representation learning.
However, training through Backpropagation Through Time (BPTT) involves repeated multiplication of Jacobian matrices, leading to vanishing or exploding gradients:

\begin{equation}
\frac{\partial \mathcal{L}}{\partial \mathbf{h}(t)} = 
\Bigg(\prod_{k=t}^{T-1} \frac{\partial \mathbf{h}(k+1)}{\partial \mathbf{h}(k)}\Bigg)
\frac{\partial \mathcal{L}}{\partial \mathbf{h}(T)}.
\label{eq:bptt_gradient}
\end{equation}

When the spectral radius of these Jacobians is below one, gradients decay exponentially, hindering long-term learning; when it exceeds one, training becomes unstable. These limitations motivated the development of gated mechanisms and, subsequently, the reservoir computing paradigm, which captures temporal dependencies without backpropagation through time.


\subsection{Long Short-Term Memory and Gated Recurrent Architectures}
\label{subsec:lstm_gated_architectures}

The introduction of Long Short-Term Memory (LSTM) networks marked a major milestone in sequential modeling, addressing the vanishing gradient problem through gating mechanisms that regulate information flow over time. The LSTM architecture introduces three gates; forget, input, and output; that modulate the cell state $\mathbf{C}_t$:

\begin{equation}
\begin{aligned}
\mathbf{z}_t &= 
\begin{bmatrix}\mathbf{h}_{t-1} \\ \mathbf{x}_t\end{bmatrix}, \quad
\begin{bmatrix}\mathbf{f}_t \\ \mathbf{i}_t \\ \mathbf{o}_t\end{bmatrix} 
= \sigma\!\left(
\begin{bmatrix}\mathbf{W}_f \\ \mathbf{W}_i \\ \mathbf{W}_o\end{bmatrix}\mathbf{z}_t +
\begin{bmatrix}\mathbf{b}_f \\ \mathbf{b}_i \\ \mathbf{b}_o\end{bmatrix}
\right), \\[1mm]
\tilde{\mathbf{C}}_t &= \tanh(\mathbf{W}_C \mathbf{z}_t + \mathbf{b}_C), \quad
\mathbf{C}_t = \mathbf{f}_t \odot \mathbf{C}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{C}}_t, \quad
\mathbf{h}_t = \mathbf{o}_t \odot \tanh(\mathbf{C}_t).
\end{aligned}
\label{eq:lstm_core}
\end{equation}


The additive update of the cell state in Eq.~\eqref{eq:lstm_core} ensures stable gradient propagation across long sequences, mitigating the vanishing and exploding gradient problems inherent in vanilla RNNs. This design enables the learning of long-term dependencies at the expense of increased computational cost.
To further enhance temporal context modeling, Bidirectional LSTMs process sequences in both temporal directions:

\begin{equation}
\mathbf{h}_t = [\,\overrightarrow{\text{LSTM}}(\mathbf{x}_t, \overrightarrow{\mathbf{h}}_{t-1});\,
\overleftarrow{\text{LSTM}}(\mathbf{x}_t, \overleftarrow{\mathbf{h}}_{t+1})\,],
\label{eq:bilstm}
\end{equation}

combining forward and backward hidden representations to exploit both past and future information.

Gated Recurrent Units (GRUs) simplify this formulation by merging the forget and input gates into a single update gate, maintaining comparable modeling capacity with fewer parameters:

\begin{equation}
\begin{aligned}
\begin{bmatrix}\mathbf{r}_t \\ \mathbf{z}_t\end{bmatrix}
&= \sigma\!\left(
\begin{bmatrix}\mathbf{W}_r \\ \mathbf{W}_z\end{bmatrix}
\begin{bmatrix}\mathbf{h}_{t-1} \\ \mathbf{x}_t\end{bmatrix}
\right), \quad
\tilde{\mathbf{h}}_t = \tanh\!\left(\mathbf{W}
\begin{bmatrix}\mathbf{r}_t \odot \mathbf{h}_{t-1} \\ \mathbf{x}_t\end{bmatrix}\right), \\[1mm]
\mathbf{h}_t &= (1-\mathbf{z}_t)\odot \mathbf{h}_{t-1} + \mathbf{z}_t \odot \tilde{\mathbf{h}}_t.
\end{aligned}
\label{eq:gru}
\end{equation}


Although GRUs reduce computational cost compared to LSTMs, with training complexity $\mathcal{O}(3TD^2E)$, both architectures remain constrained by the need for gradient propagation through time, motivating the exploration of alternative paradigms such as reservoir computing.


\subsection{Reservoir Computing Principles}

\label{subsec:reservoir_computing_principles}

The limitations of traditional RNN-based approaches have motivated the exploration of alternative paradigms for temporal sequence processing. Reservoir Computing represents a fundamental departure from gradient-based learning, offering a computationally efficient framework that separates temporal feature extraction from supervised learning.
The core insight underlying %Reservoir Computing 
RC is that effective temporal processing does not necessarily require the optimization of all network parameters through gradient descent. Instead, a fixed randomly initialized recurrent network (the reservoir) can serve as a rich dynamical system that projects input sequences into high-dimensional spaces where simple linear readout layers can perform classification or regression. 
This separation of concerns offers several theoretical and practical advantages. From a theoretical perspective, it eliminates the need for gradient propagation through the temporal sequence, avoiding the vanishing and exploding gradient problems that plague traditional RNNs. From a practical perspective, it dramatically reduces computational complexity by limiting parameter optimization to the final readout layer. 
%Echo State Networks 
ESNs represent the most widely studied implementation of %Reservoir Computing 
RC principles. An ESN consists of three components: an input layer that projects inputs to the reservoir space, a reservoir of recurrently connected processing units, and a readout layer that maps reservoir states to outputs. 
The reservoir state evolution follows a simple update rule:

\begin{equation}
\mathbf{r}(t) = (1-\alpha)\mathbf{r}(t-1) + \alpha \cdot f(\mathbf{W}_{in}\mathbf{x}(t) + \mathbf{W}_{res}\mathbf{r}(t-1) + \mathbf{b}_{res})
\end{equation}

where $\mathbf{r}(t) \in \mathbb{R}^H$ is the reservoir state, $\alpha \in %(
[0,1]$ is the leak rate, $\mathbf{W}_{in} \in \mathbb{R}^{H \times 3N}$ is the input weight matrix, $\mathbf{W}_{res} \in \mathbb{R}^{H \times H}$ is the reservoir weight matrix, and $f(\cdot)$ is the reservoir activation function. 
The key insight is that $\mathbf{W}_{in}$ and $\mathbf{W}_{res}$ are fixed and randomly initialized, never updated during training. Only the readout layer weights are learned through simple linear regression:

\begin{equation}
\mathbf{W}_{out}^* = \arg\min_{\mathbf{W}} \|\mathbf{R}\mathbf{W}^T - \mathbf{Y}\|_F^2 + \lambda\|\mathbf{W}\|_F^2
\end{equation}

where $\mathbf{R} \in \mathbb{R}^{P \times H}$ contains reservoir states for $P$ training samples, $\mathbf{Y} \in \mathbb{R}^{P \times C}$ contains target labels, and $\lambda$ is the regularization parameter. 
The effectiveness of reservoir computing depends critically on the Echo State Property (ESP), which ensures that reservoir dynamics depend only on recent inputs rather than initial conditions. Mathematically, the ESP requires that for any two reservoir trajectories $\mathbf{r}_1(t)$ and $\mathbf{r}_2(t)$ driven by the same input sequence but starting from different initial states:

\begin{equation}
\lim_{t \rightarrow \infty} \|\mathbf{r}_1(t) - \mathbf{r}_2(t)\| = 0
\end{equation}

A sufficient condition for the ESP is that the spectral radius $\rho(\mathbf{W}_{res}) < 1$, where $\rho(\mathbf{W}_{res})$ is the largest absolute eigenvalue of the reservoir weight matrix. This condition ensures that the reservoir operates in a stable dynamical regime where perturbations decay over time.
\subsection{Computational Complexity Analysis}
\label{subsec:computational_complexity}

The computational footprint of sequential models differs markedly between traditional RNNs/LSTMs and Reservoir Computing. Vanilla RNNs trained with BackPropagation Through Time (BPTT) have a training complexity of $\mathcal{O}(T \cdot D^2 \cdot E)$, where $D$ is the hidden size, $T$ is the sequence length, and $E$ is the number of epochs. LSTMs require four times more parameters than vanilla RNNs, resulting in a training complexity of $\mathcal{O}(4 \cdot T \cdot D^2 \cdot E)$. Bidirectional LSTMs double this cost to $\mathcal{O}(8 \cdot T \cdot D^2 \cdot E)$.
In contrast, RC reduces training complexity significantly by avoiding gradient-based optimization of recurrent connections. For a reservoir of size $H$, sparsity $s$, and $P$ training samples, the training complexity is $\mathcal{O}(T \cdot H \cdot s \cdot P + H^3)$, where the $H^3$ term corresponds to the ridge regression matrix inversion in the readout layer and is typically negligible compared to backpropagation in RNNs. For inference, RC reduces complexity from $\mathcal{O}(T \cdot D^2)$ in RNNs to $\mathcal{O}(T \cdot H \cdot s + H \cdot C)$, where $C$ is the number of output classes. 
\textbf{Comparison Conditions:} It is important to note that our efficiency claims (95\% training reduction) are based on comparisons where the number of parameters or hidden units is comparable ($H \approx D_{LSTM}$), ensuring a fair evaluation of the architectural differences rather than model size.

\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Training Complexity} & \textbf{Inference Complexity} \\
\midrule
Vanilla RNN & $\mathcal{O}(T \cdot D^2 \cdot E)$ & $\mathcal{O}(T \cdot D^2)$ \\
LSTM & $\mathcal{O}(4 \cdot T \cdot D^2 \cdot E)$ & $\mathcal{O}(T \cdot D^2)$ \\
Bi-LSTM & $\mathcal{O}(8 \cdot T \cdot D^2 \cdot E)$ & $\mathcal{O}(2 \cdot T \cdot D^2)$ \\
Reservoir Computing & $\mathcal{O}(T \cdot H \cdot s \cdot P + H^3)$ & $\mathcal{O}(T \cdot H \cdot s + H \cdot C)$ \\
Bidirectional RC & $\mathcal{O}(2 \cdot T \cdot H \cdot s \cdot P + (2H)^3)$ & $\mathcal{O}(2 \cdot T \cdot H \cdot s + 2 \cdot H \cdot C)$ \\
\bottomrule
\end{tabular}
\caption{Computational complexity comparison of sequential models. Complexity values are expressed in terms of sequence length $T$, hidden size $D$, reservoir size $H$, sparsity of reservoir connections $s$, number of training samples $P$, number of output classes $C$, and number of training epochs $E$.}
\begin{tablenotes}
\small
\item \textbf{Notes:} 
\begin{itemize}
    \item $T$ = sequence length; $D$ = hidden state dimension (RNN/LSTM); $H$ = reservoir size (RC); 
    \item $s$ = sparsity of reservoir recurrent connections; $P$ = number of training samples; 
    \item $C$ = number of output classes; $E$ = number of training epochs.
    \item The $H^3$ term in RC corresponds to the matrix inversion in ridge regression for the readout layer.
\end{itemize}
\end{tablenotes}
\end{table}


Bidirectional RC can be implemented by using two separate reservoirs: one for the forward pass and another for the backward pass:

\begin{align}
\begin{bmatrix}
\overrightarrow{\mathbf{r}}(t) \\[2pt]
\overleftarrow{\mathbf{r}}(t)
\end{bmatrix}
&=
\begin{bmatrix}
f(\mathbf{W}_{in}^f \mathbf{x}(t) + \mathbf{W}_{res}^f \overrightarrow{\mathbf{r}}(t-1)) \\[2pt]
f(\mathbf{W}_{in}^b \mathbf{x}(t) + \mathbf{W}_{res}^b \overleftarrow{\mathbf{r}}(t+1))
\end{bmatrix}, 
\qquad
\mathbf{r}(t) = [\overrightarrow{\mathbf{r}}(t); \overleftarrow{\mathbf{r}}(t)].
\end{align}

The total training complexity of bidirectional RC is $\mathcal{O}(2 \cdot T \cdot H \cdot s \cdot P + (2H)^3)$, which remains significantly more efficient than Bi-LSTM. This efficiency motivates the proposed bidirectional RC framework, which combines the low training cost of reservoirs with the enhanced temporal modeling provided by bidirectionality.



\section{Proposed Method}\label{sec:methodology}

Building upon the theoretical foundations established in the previous section, we present herein the proposed bidirectional RC framework for skeleton-based HAR. The proposed approach represents a synthesis of computational efficiency and temporal modeling reliability, through architectural designs with principled dimensionality reduction techniques to create a system that achieves superior performance while maintaining practical applicability. 
Indeed, the suggested framework, illustrated in Figure~\ref{fig:framework_architecture}, illustrates the proposed approach that addresses the key limitations of existing methods while introducing novel capabilities that advance the state-of-the-art in skeleton-based HAR.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{flowchart.png}
    \caption{Architecture of the proposed bidirectional RC framework.}
    \label{fig:framework_architecture}
\end{figure}

\subsection{Bidirectional Reservoir Architecture}\label{subsec:bidirectional_reservoir}

At the core of our framework lies a bidirectional reservoir architecture that fundamentally reimagines how temporal information flows through RC systems. Unlike traditional unidirectional approaches that process sequences in a single temporal direction, our architecture employs two parallel reservoirs that process the same input sequence in opposite temporal directions, creating a rich representation that captures both past and future temporal contexts. 
To address the practical challenge of variable sequence lengths common in skeleton-based HAR datasets, we first normalize all sequences to a fixed length $T_{max}$ through temporal interpolation:
\begin{equation}
\mathbf{X}_{norm} = \text{Interpolate}(\mathbf{X}, T_{max})
\label{eq:interpolation}
\end{equation}
where $T_{max}$ is set to the 95th percentile of sequence lengths in the training set to minimize information loss while ensuring computational tractability. 

The bidirectional architecture consists of two specialized reservoir streams, each optimized for processing temporal information in its respective direction. The forward reservoir processes the normalized skeleton sequence $\mathbf{X}_{norm} = [\mathbf{x}(1), \mathbf{x}(2), \ldots, \mathbf{x}(T_{max})]$ in chronological order, accumulating information about the temporal evolution of the action from its beginning toward its completion. Simultaneously, the backward reservoir processes the same sequence in reverse chronological order, capturing information about how the action unfolds when viewed from its completion toward its beginning.

\begin{equation}
\begin{aligned}
\overrightarrow{\mathbf{r}}(t) &= (1-\alpha_f)\overrightarrow{\mathbf{r}}(t-1) + 
\alpha_f \tanh(\mathbf{W}_{in}^f\mathbf{x}(t) + \mathbf{W}_{res}^f\overrightarrow{\mathbf{r}}(t-1) + \mathbf{b}^f), \\[1mm]
\overleftarrow{\mathbf{r}}(t) &= (1-\alpha_b)\overleftarrow{\mathbf{r}}(t+1) + 
\alpha_b \tanh(\mathbf{W}_{in}^b\mathbf{x}(t) + \mathbf{W}_{res}^b\overleftarrow{\mathbf{r}}(t+1) + \mathbf{b}^b).
\end{aligned}
\label{eq:bidirectional_reservoir}
\end{equation}


Here, $\overrightarrow{\mathbf{r}}(t)$ and $\overleftarrow{\mathbf{r}}(t) \in \mathbb{R}^H$ represent the forward and backward reservoir states, $\alpha_f, \alpha_b \in [0,1]$ are the respective leak rates, and $\mathbf{W}_{in}^{f,b} \in \mathbb{R}^{H \times 3N}$, $\mathbf{W}_{res}^{f,b} \in \mathbb{R}^{H \times H}$ are the input and reservoir weight matrices for each stream. 

This dual-stream processing creates a comprehensive temporal representation that captures the full context of human actions. For instance, when recognizing a "throwing" action, the forward reservoir  captures the preparatory phase and the buildup of motion, while the backward reservoir  captures the follow-through and completion patterns. 
The effectiveness of our bidirectional reservoir architecture depends critically on proper parameter initialization and configuration. We employ a principled approach to reservoir design that ensures optimal dynamical properties while maintaining computational efficiency. The input weight matrices are drawn from a uniform distribution with carefully chosen bounds:
\begin{equation}
\mathbf{W}_{in}^{f,b} \sim \mathcal{U}(-\sigma_{in}, \sigma_{in})
\label{eq:input_weights}
\end{equation}
where $\sigma_{in}$ is the input scaling parameter that controls the magnitude of input projections into the reservoir space. This parameter is crucial for ensuring that the reservoir operates in an appropriate dynamical regime: too small values lead to linear dynamics that cannot capture complex temporal patterns, while too large values can drive the reservoir into chaotic regimes difficult to control. 
The reservoir weight matrices are constructed as sparse random matrices with carefully controlled spectral properties:
\begin{equation}
\mathbf{W}_{res}^{f,b} \sim \rho \cdot \frac{\text{SparseRandom}(\gamma, \sigma_{res})}{\rho(\text{SparseRandom}(\gamma, \sigma_{res}))}.
\label{eq:reservoir_init}
\end{equation}

where $\gamma \in [0.01, 0.1]$ is the sparsity level (percentage of non-zero connections), $\sigma_{res}$ controls the magnitude of reservoir connections, and $\rho \in [0.8, 1.2]$ is the desired spectral radius. The spectral radius normalization (Eq.~\ref{eq:reservoir_init}) ensures that the reservoir operates near the edge of stability, maximizing its computational capacity while maintaining the echo state property (ESP).
\textbf{Stability Analysis:} Since the forward and backward reservoirs operate independently without recurrent cross-connections, the global stability of the bidirectional system is guaranteed provided that each individual reservoir satisfies the ESP ($\rho < 1$).


\subsection{Bidirectional State Fusion Strategies}\label{subsec:fusion_strategies}

The integration of forward and backward reservoir states represents a critical design choice that significantly impacts the framework's performance. The challenge lies in effectively combining the complementary temporal perspectives captured by each reservoir stream while maintaining computational efficiency and preserving the most discriminative information for action recognition.
We explore three  integration strategies, each offering different trade-offs between representational capacity, computational efficiency, and recognition performance. The choice of fusion strategy fundamentally determines how the bidirectional temporal information is synthesized into a unified representation suitable for downstream processing.

The concatenation strategy combines the forward and backward states through a simple concatenation at each time step (31). It preserves all information from both temporal directions but doubles the dimensionality of the state representation, creating complete temporal sequences $\mathbf{R}_{concat} = [\mathbf{r}_{concat}(1), \mathbf{r}_{concat}(2), \ldots, \mathbf{r}_{concat}(T_{max})] \in \mathbb{R}^{T_{max} \times 2H}$ for each sample. 
The primary advantage of concatenation lies in its information preservation properties. By maintaining separate representations for forward and backward temporal contexts, this approach allows downstream components to learn optimal combinations of temporal information without imposing any a priori assumptions about the relative importance of different temporal directions. Unlike averaging, which might cancel out opposing dynamics, concatenation preserves the distinct features of both the preparatory and follow-through phases.
This flexibility is particularly valuable for complex actions where discriminative information may be distributed across different temporal phases. 
However, the doubled dimensionality creates computational challenges, particularly for the subsequent dimensionality reduction and classification stages. The increased feature space requires more sophisticated regularization strategies and can lead to higher memory consumption during training and inference.

The weighted combination strategy employs a learnable parameter $\beta \in [0,1]$ to fuse the forward and backward reservoir states at each time step:
\begin{equation}
\mathbf{r}_{weighted}(t) = \beta \, \overrightarrow{\mathbf{r}}(t) + (1-\beta) \, \overleftarrow{\mathbf{r}}(t), \quad
\beta \leftarrow \beta - \eta \frac{\partial \mathcal{L}}{\partial \beta},
\end{equation}
where $\eta$ is the learning rate and $\mathcal{L}$ is the classification loss. 
This strategy maintains the original dimensionality, adaptively balances temporal directions, and allows the system to discover the optimal contribution of forward and backward information. The learned $\beta$ provides interpretable insights: values near $0.5$ indicate balanced importance, while values closer to $0$ or $1$ suggest dominance of backward or forward information.
%\subsubsection{Attention-based Fusion Strategy}

The attention-based fusion strategy dynamically integrates forward and backward reservoir states at each time step, enabling the system to adaptively focus on the most informative temporal direction. Letting $\mathbf{r}_{att}(t)$ denote the fused reservoir state, the attention mechanism can be written as:

\begin{equation}
\mathbf{r}_{att}(t) = 
\text{softmax}\Big(\mathbf{v}_a^T \tanh(\mathbf{W}_a[\overrightarrow{\mathbf{r}}(t); \overleftarrow{\mathbf{r}}(t)] + \mathbf{b}_a)\Big) \odot 
\begin{bmatrix}
\overrightarrow{\mathbf{r}}(t) \\ 
\overleftarrow{\mathbf{r}}(t)
\end{bmatrix} 
\in \mathbb{R}^H,
\end{equation}

where the softmax produces attention weights $\mathbf{a}(t) = [a_1(t), a_2(t)]$ for the forward and backward reservoirs, and $\odot$ denotes element-wise weighting. The fused sequence $\mathbf{R}_{att} \in \mathbb{R}^{T_{max} \times H}$ is then used for downstream temporal modeling. Learnable parameters $\mathbf{W}_a \in \mathbb{R}^{H_a \times 2H}$, $\mathbf{v}_a \in \mathbb{R}^{H_a}$, and $\mathbf{b}_a \in \mathbb{R}^{H_a}$ allow the system to capture complex, time-dependent interactions between forward and backward contexts. The attention weights provide interpretability: high $a_1(t)$ indicates forward context dominance, while high $a_2(t)$ indicates backward context dominance.
%\subsubsection{Fusion Strategy Selection Criteria}

The choice of fusion strategy balances representation richness, efficiency, and interpretability. Concatenation preserves full forward and backward context, weighted combination introduces a single learnable parameter for adaptive yet lightweight temporal balancing, and attention-based fusion highlights the most relevant temporal phases for interpretable reasoning. All approaches remain compatible with downstream dimensionality reduction and classification, maintaining the efficiency of the reservoir computing framework. The strategies will be evaluated experimentally, with the best-performing approach selected.




\subsection{Adaptive Multi-view Dimensionality Reduction}\label{subsec:dimensionality_reduction}

The bidirectional reservoir architecture generates high-dimensional temporal sequences. These sequences are rich in temporal information but create challenges for effective learning and computational efficiency. 
Our adaptive multi-view dimensionality reduction module addresses these challenges through a two-stage approach. It first performs temporal compression and then applies tensor decomposition techniques. In this context, "multi-view" refers to the simultaneous processing of the data tensor along distinct modes (views): the temporal mode (time steps), the sample mode (batch), and the feature mode (reservoir states).
The bidirectional reservoir produces a complete temporal sequence for each input sample. Depending on the integration strategy, the resulting dimensionality can vary: 
$\mathbf{R}_i \in \mathbb{R}^{T_{max} \times 2H}$ for concatenation, and 
$\mathbf{R}_i \in \mathbb{R}^{T_{max} \times H}$ for weighted or attention-based fusion. In typical configurations, $T_{max} = 300$ and $H = 1000$. These dimensions already indicate a large feature space.
When extended to multiple samples, this results in substantial computational and storage demands. High-dimensional feature spaces often lead to sparse data distributions, making learning more difficult and reducing generalization. The increased number of features also raises computational costs during training and inference. Moreover, such rich representations increase the risk of overfitting, especially with limited training data. Finally, the high-dimensional feature matrices require significant memory, making storage and scalability more challenging.

\subsubsection{Stage 1: Temporal Principal Component Analysis}

We apply PCA along the temporal dimension of each reservoir sequence to compress temporal dynamics.  
For each sample \(i\) compute the temporal covariance
\begin{equation}
\mathbf{C}_i \;=\; \frac{1}{T_{max}-1}\,(\mathbf{R}_i-\boldsymbol{\mu}_i)^\top(\mathbf{R}_i-\boldsymbol{\mu}_i) \in \mathbb{R}^{d\times d},
\label{eq:cov}
\end{equation}
where \(\boldsymbol{\mu}_i\) is the temporal mean and \(d\) is the per-time-step feature dimension (\(H\) or \(2H\)).

Perform eigendecomposition and retain the minimal number of components \(K_i\) that preserve fraction \(\theta_{PCA}\) of the variance:
\begin{equation}
\mathbf{V}_i,\boldsymbol{\Lambda}_i = \mathrm{eig}(\mathbf{C}_i), \qquad
K_i = \min\!\left\{ k : \frac{\sum_{j=1}^{k}\lambda_{i,j}}{\sum_{j=1}^{d}\lambda_{i,j}} \ge \theta_{PCA} \right\},
\label{eq:local_pca}
\end{equation}
with \(\lambda_{i,j}\) sorted descendingly. The per-sample reduced temporal representation is \(\mathbf{R}_{PCA,i}=\mathbf{R}_i\mathbf{V}_i[:,1:K_i]\).

To obtain a consistent projection dimension across samples, compute a global rank and projection from the pooled covariance:
\begin{equation}
K = \mathrm{median}\{K_1,\dots,K_P\}, \quad
\mathbf{C}_{global} = \frac{1}{P}\sum_{i=1}^P \mathbf{C}_i,\quad
\mathbf{V}_{global},\boldsymbol{\Lambda}_{global}=\mathrm{eig}(\mathbf{C}_{global}),
\label{eq:global_pca}
\end{equation}
and form the final, fixed-size temporal embedding
\begin{equation}
\mathbf{R}_{PCA,i} \;=\; \mathbf{R}_i\,\mathbf{V}_{global}[:,1:K] \in \mathbb{R}^{T_{max}\times K}.
\label{eq:final_projection}
\end{equation}

This preserves dominant temporal dynamics while enforcing a uniform dimensionality \(K\) for downstream tensor operations.



\subsubsection{Stage 2: Tucker Tensor Decomposition}

Following temporal PCA, we construct a three-dimensional tensor $\mathcal{R} \in \mathbb{R}^{P \times T_{max} \times K}$ from all PCA-reduced samples and apply Tucker decomposition to capture multilinear relationships across samples, time, and features:
\begin{equation}
\mathcal{R} \approx \mathcal{G} \times_1 \mathbf{U}^{(1)} \times_2 \mathbf{U}^{(2)} \times_3 \mathbf{U}^{(3)},
\end{equation}
where $\mathcal{G} \in \mathbb{R}^{R_1 \times R_2 \times R_3}$ is the core tensor capturing the essential interactions between different modes, and $\mathbf{U}^{(1)} \in \mathbb{R}^{P \times R_1}$, $\mathbf{U}^{(2)} \in \mathbb{R}^{T_{max} \times R_2}$, $\mathbf{U}^{(3)} \in \mathbb{R}^{K \times R_3}$ are the mode-wise factor matrices that encode the principal directions of variation. 

Tucker decomposition provides a powerful framework for simultaneous dimensionality reduction across all tensor modes while preserving the intrinsic multilinear structure of the data. The decomposition is typically performed via Higher-Order Singular Value Decomposition (HOSVD), where each factor matrix is obtained from the singular value decomposition (SVD) of the corresponding mode unfolding:
\begin{equation}
\mathbf{U}^{(i)} = \text{SVD}(\mathcal{R}_{(i)})[:, :R_i], \quad i = 1, 2, 3,
\end{equation}
with $\mathcal{R}_{(i)}$ denoting the mode-$i$ matricization of $\mathcal{R}$. The reduced dimensions $R_1 \ll P$, $R_2 \ll T_{max}$, and $R_3 \ll K$ are adaptively selected to preserve a target proportion of variance in each mode:
\begin{equation}
R_i = \arg\min_{r} \left\{ \frac{\sum_{j=1}^r \sigma_{i,j}^2}{\sum_{j=1}^{d_i} \sigma_{i,j}^2} \geq \theta_i \right\}, \quad \theta_i \in [0.9, 0.99],
\end{equation}
where $\sigma_{i,j}$ are the singular values from the mode-$i$ unfolding, and $d_i$ is its original dimensionality. This adaptive criterion ensures that sufficient information is retained for accurate recognition while achieving optimal compression. 

Finally, each sample's low-dimensional representation is obtained by projection through the learned factor matrices:
\begin{equation}
\mathbf{R}_{final,i} = \mathcal{R}_i \times_2 \mathbf{U}^{(2)} \times_3 \mathbf{U}^{(3)} \in \mathbb{R}^{R_2 \times R_3}.
\end{equation}
This two-stage dimensionality reduction pipeline preserves the most informative spatiotemporal patterns while typically reducing the original dimensionality by 90--95\% and retaining about 95\% of the variance. \textbf{Computational Note:} While Tucker decomposition introduces some overhead, it is applied only once per batch after the reservoir projection and is distinctly cheaper ($\mathcal{O}(K^3)$ or similar tensor ops) than the iterative gradient calculations ($\mathcal{O}(T \cdot D^2)$) required for training LSTM parameters.



%This two-stage dimensionality reduction approach ensures that we preserve the most important spatiotemporal patterns while achieving substantial dimensionality reduction, typically reducing the original dimensionality by 90-95\% while retaining 95\% of the variance.

\subsection{Enhanced Representation Learning}\label{subsec:representation_learning}

The dimensionality-reduced reservoir states serve as the foundation for enhanced representation learning, designed to capture action dynamics at multiple temporal scales. Human actions inherently exhibit structure across different temporal resolutions, ranging from fine-grained joint movements to coarse-grained action phases. To address this, our multi-scale temporal pooling and attention strategy aggregates discriminative features from the reduced temporal sequences $\mathbf{R}_{final,i} \in \mathbb{R}^{R_2 \times R_3}$.

\paragraph{Global Statistical Pooling.}
We first compute global statistics that summarize the entire temporal sequence:
\begin{equation}
\mathbf{f}_{global} = \tfrac{1}{R_2}\!\sum_{t=1}^{R_2}\!\mathbf{R}_{final,i}(t,:), \quad
\mathbf{f}_{max} = \max_{t}\mathbf{R}_{final,i}(t,:), \quad
\mathbf{f}_{std} = \sqrt{\tfrac{1}{R_2 - 1}\!\sum_{t=1}^{R_2}\!\big(\mathbf{R}_{final,i}(t,:) - \mathbf{f}_{global}\big)^2}.
\label{eq:stat_features}
\end{equation}


Here, $\mathbf{f}_{global}$ captures overall action characteristics invariant to execution speed, $\mathbf{f}_{max}$ emphasizes salient moments (e.g., action peaks), and $\mathbf{f}_{std}$ quantifies temporal variability.

\paragraph{Local Multi-Scale Pattern Extraction.}
To model local temporal dependencies, we apply 1D convolutions with multiple kernel sizes, each followed by global max pooling:
\begin{multline}
\mathbf{f}_{local} = 
\big[ 
\text{GlobalMaxPool}(\text{Conv1D}(\mathbf{R}_{final,i}, k{=}3)); \\ 
\text{GlobalMaxPool}(\text{Conv1D}(\mathbf{R}_{final,i}, k{=}5)); \\ 
\text{GlobalMaxPool}(\text{Conv1D}(\mathbf{R}_{final,i}, k{=}7))
\big].
\end{multline}

This multi-kernel approach captures fine-to-coarse temporal patterns, where smaller kernels detect rapid transitions and larger ones capture broader motion phases.

\paragraph{Temporal Attention Mechanism.}
Recognizing that not all time steps contribute equally to action recognition, we incorporate a learnable temporal attention mechanism:
\begin{equation}
\begin{aligned}
\mathbf{e}(t) &= \mathbf{v}_a^{\top} \tanh(\mathbf{W}_a \mathbf{R}_{final,i}(t, :) + \mathbf{b}_a), \\
\alpha(t) &= \frac{\exp(\mathbf{e}(t))}{\sum_{k=1}^{R_2} \exp(\mathbf{e}(k))}, \\
\mathbf{f}_{attention} &= \sum_{t=1}^{R_2} \alpha(t)\, \mathbf{R}_{final,i}(t, :).
\end{aligned}
\label{eq:attention}
\end{equation}

where $\mathbf{W}_a \in \mathbb{R}^{H_a \times R_3}$, $\mathbf{v}_a \in \mathbb{R}^{H_a}$, and $\mathbf{b}_a \in \mathbb{R}^{H_a}$ are learnable parameters (with $H_a = 64$). This mechanism adaptively highlights the most discriminative temporal segments, enhancing both accuracy and interpretability.

\paragraph{Final Representation.}
Finally, all feature components are concatenated to form the comprehensive enhanced representation, 
\(\mathbf{f}_{\text{final}} = [\mathbf{f}_{\text{global}};\ \mathbf{f}_{\text{max}};\ \mathbf{f}_{\text{std}};\ \mathbf{f}_{\text{local}};\ \mathbf{f}_{\text{attention}}]\). 
This fused representation jointly encodes global statistics, local dynamics, and attention-weighted salient information, providing a rich and compact embedding that effectively supports downstream action recognition tasks.


\subsection{Proposed Readout Mechanisms}\label{subsec:readout_mechanisms}

The readout stage in classical reservoir computing is typically linear, acting as a simple regression layer over fixed high-dimensional dynamics. While efficient, such linear mappings often fail to exploit the nonlinear structure of the spatiotemporal embeddings produced by the enhanced reservoir. To overcome this limitation, we introduce a nonlinear and adaptive readout architecture that learns hierarchical transformations of the enhanced representation $\mathbf{f}_{final}$, yielding superior discriminative capability while preserving the efficiency advantages of reservoir computing.
%\paragraph{Nonlinear Readout via Structured Multi-Layer Perceptron.}
Unlike traditional readouts that rely on a single linear mapping, the proposed readout is a compact yet expressive Multi-Layer Perceptron (MLP) designed to extract class-discriminative manifolds from $\mathbf{f}_{final}$. The architecture incorporates adaptive normalization, noise-aware dropout, and two complementary nonlinear activation paradigms---Maxout and Kernel Activation Function (KAF)---that jointly enable flexible local and global modeling of class boundaries. The transformations can be expressed as:
\begin{equation}
\begin{aligned}
\mathbf{z}_1 &= \text{Maxout}(\text{BatchNorm}(\mathbf{W}_1 \mathbf{f}_{final} + \mathbf{b}_1)), \\
\mathbf{z}_2 &= \text{KAF}(\text{BatchNorm}(\mathbf{W}_2 (\mathbf{z}_1 \odot \mathbf{m}_1) + \mathbf{b}_2)), \\
\mathbf{y} &= \text{softmax}(\mathbf{W}_3 (\mathbf{z}_2 \odot \mathbf{m}_2) + \mathbf{b}_3).
\end{aligned}
\label{eq:mlp_full}
\end{equation}

where $\mathbf{m}_1$ and $\mathbf{m}_2$ are dropout masks with probabilities $p_1 = 0.3$ and $p_2 = 0.2$, respectively. This combination of structured dropout and normalization makes the readout resilient to reservoir variability while ensuring consistent convergence across tasks.
%\paragraph{Maxout for Piecewise Linear Partitioning.}
The first nonlinear stage employs the Maxout activation, which partitions the feature space into multiple locally linear regions:
\begin{equation}
\text{Maxout}(\mathbf{x}) = \max_{i \in [1,k]} (\mathbf{W}_i^{maxout} \mathbf{x} + \mathbf{b}_i^{maxout}),
\end{equation}
where $k=5$ determines the number of partitions. Within the reservoir framework, this mechanism adaptively decomposes the spatiotemporal representation into submanifolds, enhancing separability among complex action classes that exhibit overlapping temporal dynamics.
%\paragraph{Kernel Activation Function for Adaptive Nonlinearity.}
The second nonlinear transformation introduces data-driven flexibility through the Kernel Activation Function (KAF), which learns smooth nonlinear mappings shaped by the underlying feature distribution:
\begin{equation}
\text{KAF}(\mathbf{x}) = \sum_{i=1}^{D} \alpha_i \, \phi(\|\mathbf{x} - \mathbf{c}_i\|),
\end{equation}
where $\alpha_i$ are learnable coefficients, $\mathbf{c}_i$ are kernel centers initialized via k-means, $D = 20$ denotes the number of kernels, and $\phi(z) = \exp(-\gamma z^2)$ is a Gaussian kernel with learnable bandwidth $\gamma$. By integrating KAF into the readout, the model gains the capacity to adjust activation curvature dynamically, yielding improved discrimination between temporally similar actions.

%\paragraph{Regularization and Spectral Stabilization.}
The training objective of the readout combines standard cross-entropy with spectral and kernel regularization to ensure both generalization and numerical stability:
\begin{equation}
\mathcal{L}_{total} = \mathcal{L}_{CE} 
+ \lambda_1 \sum_{i=1}^{3} \|\mathbf{W}_i\|_F^2 
+ \lambda_2 \sum_{i=1}^{D} \|\alpha_i\|^2,
\end{equation}
where $\lambda_1=0.001$ and $\lambda_2=0.0001$. Spectral normalization is applied to all weight matrices to bound the Lipschitz constant:
\begin{equation}
\mathbf{W}_{SN} = \frac{\mathbf{W}}{\sigma(\mathbf{W})},
\end{equation}
with $\sigma(\mathbf{W})$ denoting the leading singular value estimated via power iteration. This spectral constraint preserves the dynamical balance of the reservoir-to-readout interface, ensuring stable gradient flow and consistent classification under varying input scales.
%\paragraph{Discussion.}
%Through this design, the proposed readout extends the expressive range of reservoir computing without sacrificing efficiency. The joint use of Maxout and KAF enables adaptive local partitioning and smooth nonlinear interpolation, providing a structured alternative to fully recurrent deep models. This configuration allows the reservoir to remain fixed and lightweight, while the readout alone learns to decode the nonlinear geometry of spatiotemporal representations with high precision. The resulting model achieves a principled trade-off between interpretability, stability, and discriminative power---an aspect often lacking in conventional RC formulations.
%\subsection{Training Algorithm}\label{subsec:training_algorithm}
Algorithm~\ref{alg:training} outlines the end-to-end training pipeline of the proposed Bidirectional Reservoir Computing (BRC) framework. It integrates forward and backward reservoir dynamics, flexible fusion strategies, tensor-based dimensionality reduction, and advanced readout learning. To regularize the fusion parameters, we define a fusion-specific term $\mathcal{R}_{fusion}(\mathcal{F})$, combining attention entropy where needed:

\begin{equation}
\mathcal{R}_{fusion}(\mathcal{F}) =
\begin{cases}
0, & \mathcal{F} = \text{concat}, \\[1mm]
\mu_{\beta} \|\beta - 0.5\|_2^2, & \mathcal{F} = \text{weighted}, \\[1mm]
\mu_a \frac{1}{P}\sum_{i=1}^{P}\sum_{t=1}^{T_{max}} \Big[-\sum_{j=1}^{2} a_{i,j}(t) \log a_{i,j}(t)\Big] + \mu_W \|\mathbf{W}_a\|_F^2, & \mathcal{F} = \text{attention},
\end{cases}
\label{eq:rfusion_total}
\end{equation}

with recommended default hyperparameters: 
\[
 \quad 
\mu_\beta = 0.01, \quad \mu_a = 0.001, \quad \mu_W = 10^{-4}, \quad \lambda_3 = 0.01
\]


The fusion regularization is incorporated into the total loss:
\begin{equation}
\mathcal{L} = \mathcal{L}_{CE} + \lambda_1 \sum \|\mathbf{W}_i\|_F^2 + \lambda_2 \sum \|\alpha_i\|^2 + \lambda_3 \mathcal{R}_{fusion}(\mathcal{F}).
\label{eq:total_loss}
\end{equation}


\begin{algorithm}[htbp]
\caption{Bidirectional Reservoir Computing Training with Configurable Fusion}
\label{alg:training}
\begin{algorithmic}[1]
\Require Training data $\{\mathbf{X}_i, y_i\}_{i=1}^P$, reservoir size $H$, max sequence length $T_{max}$, regularization $\lambda_1,\lambda_2,\lambda_3$, fusion strategy $\mathcal{F}\in\{\text{concat},\text{weighted},\text{attention}\}$
\Ensure Trained parameters $\Theta$ (readout weights, fusion params if learnable, projection factors)

\State Initialize reservoirs $\mathbf{W}_{res}^{f,b}, \mathbf{W}_{in}^{f,b}$ (random, spectral radius $\rho$)
\If{$\mathcal{F}=\text{weighted}$} \State initialize learnable $\beta$ (default $0.5$) \EndIf
\If{$\mathcal{F}=\text{attention}$} \State initialize $\mathbf{W}_a, \mathbf{v}_a, \mathbf{b}_a$ \EndIf
\State Initialize readout MLP parameters and optimizer

\For{each sample $i=1,\dots,P$}
  \State $\mathbf{X}_{norm} \gets \mathrm{Interpolate}(\mathbf{X}_i, T_{max})$
  \State reset $\overrightarrow{\mathbf{r}}(0)=\mathbf{0}$, $\overleftarrow{\mathbf{r}}(T_{max}+1)=\mathbf{0}$
  \For{$t=1\dots T_{max}$} compute $\overrightarrow{\mathbf{r}}(t)$  \EndFor
  \For{$t=T_{max}\dots 1$} compute $\overleftarrow{\mathbf{r}}(t)$  \EndFor
  \For{$t=1\dots T_{max}$} 
    \State $\mathbf{r}_{fused}(t) \gets$ fused state (concat / weighted / attention, see Eq.~\ref{eq:rfusion_total})
    \State store $\mathcal{R}_{raw}[i,t,:] \gets \mathbf{r}_{fused}(t)$
  \EndFor
\EndFor

\State \# Temporal compression / alignment
\State apply per-sample PCA or global projection to produce $\mathcal{R} \in \mathbb{R}^{P\times T_{max}\times K}$
\State Tucker decomposition / factorization to form $\mathbf{R}_{final,i}$

\For{epoch = 1..N}
  \For{mini-batch $B$}
    \State compute $\mathbf{f}_{final,i}$ for $i \in B$
    \State compute predictions $\hat y_i$ via readout MLP
    \State compute loss: 
    \[
      \mathcal{L} = \frac{1}{|B|}\sum_{i\in B} \mathcal{L}_{CE}(\hat y_i,y_i)
      + \lambda_1 \sum \|\mathbf{W}_j\|_F^2
      + \lambda_2 \sum \|\alpha\|^2
      + \lambda_3\,\mathcal{R}_{fusion}(\mathcal{F})
    \]
    \State update readout and fusion parameters by gradient step
    \State apply spectral normalization and dropout
  \EndFor
\EndFor

\State \Return $\Theta = \{\text{readout weights}, \text{fusion params (if learned)}, \text{projection factors}\}$
\end{algorithmic}
\end{algorithm}



%This training process ensures coherent temporal modeling by combining bidirectional dynamics, adaptive fusion, and tensor-based dimensionality reduction. The integration of attention and multi-scale pooling enhances feature robustness, while the readout layer leverages advanced activation and normalization strategies for stable and efficient learning.

\section{Experimental Validation}\label{sec:experiments}

Our experimental investigation unfolds across multiple dimensions: we begin with detailed dataset analysis and visualization to understand the characteristics of human actions in our evaluation scenarios, proceed through systematic performance comparisons with state-of-the-art methods, conduct thorough ablation studies to understand the contribution of each component, and conclude with practical considerations including computational efficiency and real-time performance analysis.  

\subsection{Dataset Characteristics and Preprocessing}\label{subsec:experimental_design}

Our evaluation is conducted on three benchmark datasets that capture the diversity and complexity of skeleton-based human action recognition tasks. These datasets differ in action categories, sensing modalities, and recording conditions, providing a comprehensive basis for assessing recognition performance.  

The \textbf{UTD Multimodal Human Action Dataset (UTD-MHAD)} \cite{chen2015utd} contains 27 actions performed by 8 subjects (4 male, 4 female), each repeated four times, yielding 861 sequences. Skeleton data is captured with a Kinect v2 sensor (25 joints in 3D). Actions range from simple gestures (e.g., wave, swipe) to complex sports movements (e.g., tennis serve, basketball shoot) and daily activities (e.g., sit down, walk). Although our framework focuses on skeleton data, UTD-MHAD also provides synchronized RGB, depth, and inertial data, enabling future multi-modal comparisons.  
The \textbf{MSR Action3D dataset} \cite{li2010action} comprises 567 sequences of 20 actions performed by 10 subjects, captured with the original Kinect (20 joints). Actions are grouped into three subsets: AS1 (horizontal arm movements), AS2 (vertical arm movements), and AS3 (complex multi-body movements). This organization highlights increasing levels of difficulty, from easily distinguishable spatial gestures (AS1) to challenging coordinated movements (AS3), making it a strong benchmark for testing temporal modeling capabilities.  
The \textbf{CZU Multimodal Human Action Dataset (CZU-MHAD)} \cite{chao2022czu} consists of 22 actions performed by 7 subjects, totaling 880 sequences. It integrates depth video, skeletal data from Kinect (25 joints), and inertial signals from wearable sensors. While we evaluate the skeletal modality, the dataset's multi-modal design reflects real-world action recognition scenarios and offers opportunities for extended multi-sensor studies.  
Figure~\ref{fig:dataset_visualization} illustrates representative temporal patterns for the CZU-MHAD dataset.  
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{comprehensive_skeletal_action_analysis.png}
    \caption{Temporal patterns of skeletal actions in the CZU-MHAD dataset (22 actions, 880 sequences).}
    \label{fig:dataset_visualization}
\end{figure}


\subsection{Experimental Setup}\label{subsec:experimental_setup}

Experiments were conducted on a workstation equipped with an NVIDIA GeForce RTX 3050 GPU (8GB VRAM), an Intel Core i7-13650HX processor (16 cores, 2.3GHz), 16GB DDR5-4800MHz RAM, and a 1TB NVMe SSD. The software environment included Python 3.12.0, TensorFlow 2.15.0, NumPy 1.24.3, Scikit-learn 1.3.0, and CUDA 12.2. This setup provides sufficient computational capacity while remaining accessible to researchers with comparable resources.  

We adopted standard cross-subject evaluation protocols \cite{wang2012mining} for all datasets. For UTD-MHAD, subjects 1, 3, 5, 7 were used for training (432 sequences) and 2, 4, 6, 8 for testing (429 sequences). For MSR Action3D, subjects 1, 3, 5, 7, 9 were used for training (285 sequences) and 2, 4, 6, 8, 10 for testing (282 sequences). For CZU-MHAD, subjects 1, 3, 5, 7 were used for training (504 sequences) and 2, 4, 6 for testing (376 sequences). These splits ensure generalization across different individuals and provide balanced coverage of action classes.  
Hyperparameters were optimized through grid search and validation. The reservoir architecture used size $H=1000$, connection sparsity $\gamma=0.05$, and spectral radius $\rho=0.95$. Leak rates were set to $\alpha_f=\alpha_b=0.3$, with input scaling $\sigma_{\text{in}}=0.5$. Regularization employed L2 penalty $\lambda=0.001$  and dropout $p_{\text{drop}}=0.1$. Dimensionality reduction thresholds were $\theta_i=0.95$ and $\theta_{\text{PCA}}=0.95$ for intermediate and final projections, respectively.  
This configuration yielded the best trade-off between accuracy and computational efficiency across all three datasets.  


\subsection{Fusion Strategy Selection and Analysis}\label{subsec:fusion_strategy_analysis}  

We empirically compared the three fusion strategies; concatenation, weighted, and attention-based; across all benchmark datasets to identify the optimal method for combining forward and backward reservoir states. Using identical reservoir configurations ($H = 1000$, $\rho = 0.95$, $\gamma = 0.05$) and training protocols, the weighted fusion strategy optimized the parameter $\beta$ through gradient-based learning during training, while the attention-based model utilized $H_a = 64$ attention units initialized with the Xavier scheme \cite{glorot2010understanding}.
Table~\ref{tab:fusion_comparison} summarizes the results, showing that concatenation achieves the highest recognition accuracy, especially on complex action sequences, by preserving full bidirectional information. This gain comes at the cost of increased memory usage. Weighted fusion achieved the best efficiency, with 22\% faster training and 44\% lower memory usage, while maintaining competitive accuracy. The learned parameter value of $\beta \approx 0.6$ indicates a slightly higher contribution from forward dynamics. The attention-based strategy provides a balance between the two, yielding accuracy close to concatenation with interpretable temporal emphasis and moderate computational needs.  
Paired t-tests \cite{student1908probable} across 30 runs confirm that concatenation significantly outperforms weighted (p $<$ 0.01) and attention-based fusion (p $<$ 0.05), while the latter two show no significant difference (p $>$ 0.1). Under noise perturbations (Section~\ref{subsec:robustness_validation}), concatenation remains the most accurate, whereas weighted fusion exhibits the most stable degradation due to its inherent regularization. Figure~\ref{fig:fusion_analysis} illustrates the trade-offs between accuracy, computational cost, attention behavior, and scaling characteristics.  

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{fusion_strategy_analysis.png}
    \caption{Fusion strategy analysis: (a) Accuracy vs. computational cost trade-offs, (b) Learned attention patterns for different action classes, (c) Weighted fusion parameter sensitivity analysis, (d) Memory scaling behavior across different reservoir sizes.}
    \label{fig:fusion_analysis}
\end{figure}
Based on the empirical analysis, the concatenation strategy is adopted as the primary fusion approach, as it consistently yields the highest recognition accuracy with statistically significant gains, preserves complete bidirectional temporal information, and maintains stable performance under noise and hyperparameter variations. Although it requires more memory, its computational cost scales linearly with reservoir size. For resource-limited scenarios, weighted fusion offers a good trade-off, while attention-based fusion remains the most interpretable. Unless stated otherwise, all subsequent experiments use the concatenation strategy.

\begin{table}[!ht]
\centering
\begin{adjustbox}{center}
\begin{tabular}{l c c c c c c c c c}
\toprule
\multirow{2}{*}{\textbf{Fusion Strategy}} 
& \multicolumn{3}{c}{\textbf{UTD-MHAD}} 
& \multicolumn{3}{c}{\textbf{MSR Action3D}} 
& \multicolumn{3}{c}{\textbf{CZU-MHAD}} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}
& \textbf{Acc} & \textbf{Time} & \textbf{Mem} 
& \textbf{Acc} & \textbf{Time} & \textbf{Mem} 
& \textbf{Acc} & \textbf{Time} & \textbf{Mem} \\
\midrule
Concatenation       & \textbf{98.91} & 2.7 & 3.2 & \textbf{97.50} & 2.1 & 2.8 & \textbf{98.50} & 2.4 & 3.0 \\
Weighted ($\beta=0.6$) & 98.23 & \textbf{2.1} & \textbf{1.8} & 96.81 & \textbf{1.6} & \textbf{1.5} & 97.73 & \textbf{1.9} & \textbf{1.7} \\
Attention-based     & 98.67 & 3.4 & 2.1 & 97.12 & 2.8 & 1.9 & 98.21 & 3.1 & 2.0 \\
\midrule
\textbf{Average Performance} 
& \multicolumn{3}{c}{\textbf{Acc / Time / Mem}} 
& \multicolumn{6}{c}{\textbf{Trade-off Summary}} \\
\midrule
Concatenation & \multicolumn{3}{c}{98.30 / 2.4 / 3.0} 
& \multicolumn{6}{c}{\textbf{Highest Accuracy}, but \textbf{High Resource Usage}} \\
Weighted      & \multicolumn{3}{c}{97.59 / 1.9 / 1.7} 
& \multicolumn{6}{c}{\textbf{Most Efficient}, Minimal Accuracy Loss} \\
Attention     & \multicolumn{3}{c}{98.00 / 3.1 / 2.0} 
& \multicolumn{6}{c}{\textbf{Balanced Trade-off}, Best Interpretability} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Comparison of bidirectional fusion strategies across benchmark datasets (UTD-MHAD, MSR Action3D, and CZU-MHAD), evaluated in terms of classification accuracy (Acc, \%), training time (Time, minutes), and memory usage (Mem, GB). 
\textbf{Bold values} denote the best performance within each dataset and metric. 
The lower section summarizes trade-offs: 
Concatenation yields the highest accuracy but requires more computational resources; 
Weighted fusion achieves the best efficiency with only minor accuracy degradation; 
Attention-based fusion offers a balanced compromise between accuracy, efficiency, and interpretability.}
\label{tab:fusion_comparison}
\end{table}

\subsection{Comprehensive Comparative Evaluation and Performance Analysis}\label{subsec:comparative_evaluation}

The comparison presented in this section encompasses multiple categories of methods. 
Traditional RNN-based methods include Vanilla RNN \cite{elman1990finding} with 1000 hidden units representing the foundational approach to sequential modeling, LSTM \cite{hochreiter1997long} with 1000 memory cells showcasing the benefits of gated architectures, GRU \cite{cho2014learning} with 1000 hidden units providing a simplified gating alternative, bidirectional LSTM \cite{schuster1997bidirectional} with 500 units per direction demonstrating the value of bidirectional processing, and bidirectional GRU with 500 units per direction offering computational efficiency with bidirectional capabilities. 
Advanced graph-based methods include ST-GCN \cite{yan2018spatial} representing the foundation of graph-based skeleton modeling, 2s-AGCN \cite{shi2019two} showcasing multi-stream graph processing, MS-G3D \cite{liu2020disentangling} demonstrating multi-scale graph convolution, and CTR-GCN \cite{chen2021channel} representing the current state-of-the-art in graph-based approaches. 
Reservoir computing variants include Standard ESN \cite{jaeger2001echo} with 1000 reservoir units providing the baseline RC performance, bidirectional ESN representing our core architectural innovation, and deep ESN \cite{gallicchio2017deep} with 3 layers exploring hierarchical reservoir architectures. 
Table~\ref{tab:comparative_performance} shows the performance comparison across all datasets, revealing the superior performance of the proposed framework. 
The results reveal several compelling insights about the performance landscape of skeleton-based HAR. Our framework achieves substantial improvements over all RNN-based approaches, with particularly significant gains over bidirectional LSTM (5.11\% on UTD-MHAD, 7.40\% on MSR Action3D, and 6.80\% on CZU-MHAD). These improvements demonstrate that the RC paradigm can achieve superior temporal modeling while avoiding the computational complexity and training challenges associated with gradient-based recurrent networks.
Even compared to the most sophisticated graph-based approaches like CTR-GCN, our framework achieves meaningful improvements (1.81\% on UTD-MHAD, 2.70\% on MSR Action3D, and 2.80\% on CZU-MHAD). This demonstrates that effective temporal modeling through bidirectional RC can complement and exceed the spatial modeling capabilities of graph-based approaches.
Moreover, to provide complete context within the current research landscape, we present additional comparisons with recent state-of-the-art methods that have emerged in the skeleton-based HAR domain. 
The comparative analysis of HAR techniques, as shown in Table~\ref{Tableau_Comparison}, highlights significant differences in performance across various methods and datasets. Specifically, we compare against recent architectures such as the enhanced 3D CNN of Tian et al. \cite{tian2025action} and the attention-driven DC-GRU of Dey et al. \cite{dey2024workout}, demonstrating that our bidirectional reservoir framework achieves superior accuracy while maintaining computational efficiency. We conduct statistical significance testing using McNemar's test \cite{mcnemar1947note} for paired accuracy comparisons, with Bonferroni correction \cite{bonferroni1936teoria} for multiple comparisons.
\begin{table}[!ht]
\centering
\begin{adjustbox}{center}
\begin{tabular}{l @{\hskip 3pt} c@{\hskip 2pt}c@{\hskip 2pt}c @{\hskip 6pt} c@{\hskip 2pt}c@{\hskip 2pt}c @{\hskip 6pt} c@{\hskip 2pt}c@{\hskip 2pt}c}
\toprule
\multirow{2}{*}{\textbf{Method}} 
& \multicolumn{3}{c@{\hskip 6pt}}{\textbf{UTD-MHAD}} 
& \multicolumn{3}{c@{\hskip 6pt}}{\textbf{MSR Action3D}} 
& \multicolumn{3}{c}{\textbf{CZU-MHAD}} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}
& \textbf{Acc (\%)} & \textbf{Prec (\%)} & \textbf{Rec (\%)} 
& \textbf{Acc (\%)} & \textbf{Prec (\%)} & \textbf{Rec (\%)} 
& \textbf{Acc (\%)} & \textbf{Prec (\%)} & \textbf{Rec (\%)} \\
\midrule
\textbf{RNN-based} & & & & & & & & & \\
Vanilla RNN \cite{elman1990finding} & 82.3 & 81.7 & 82.1 & 78.5 & 77.9 & 78.2 & 80.1 & 79.8 & 80.0 \\
LSTM \cite{hochreiter1997long} & 91.2 & 90.8 & 91.0 & 87.3 & 86.9 & 87.1 & 89.5 & 89.1 & 89.3 \\
GRU \cite{cho2014learning} & 89.7 & 89.3 & 89.5 & 85.8 & 85.4 & 85.6 & 87.9 & 87.5 & 87.7 \\
Bi-LSTM \cite{schuster1997bidirectional} & 93.8 & 93.4 & 93.6 & 90.1 & 89.7 & 89.9 & 91.7 & 91.3 & 91.5 \\
Bi-GRU & 92.5 & 92.1 & 92.3 & 88.9 & 88.5 & 88.7 & 90.4 & 90.0 & 90.2 \\
Recent (Dey '24) \cite{dey2024workout} & 92.8 & - & - & - & - & - & - & - & - \\

\midrule
\textbf{Graph-based} & & & & & & & & & \\
ST-GCN & 94.2 & 93.8 & 94.0 & 91.5 & 91.1 & 91.3 & 92.8 & 92.4 & 92.6 \\
2s-AGCN & 95.1 & 94.7 & 94.9 & 92.3 & 91.9 & 92.1 & 93.6 & 93.2 & 93.4 \\
MS-G3D & 96.3 & 95.9 & 96.1 & 93.7 & 93.3 & 93.5 & 94.9 & 94.5 & 94.7 \\
CTR-GCN & 97.1 & 96.7 & 96.9 & 94.8 & 94.4 & 94.6 & 95.7 & 95.3 & 95.5 \\
\midrule
\textbf{RC-based} & & & & & & & & & \\
Standard ESN & 88.4 & 87.9 & 88.1 & 84.7 & 84.2 & 84.4 & 86.3 & 85.8 & 86.0 \\
Bidirectional ESN & 92.1 & 91.6 & 91.8 & 88.5 & 88.0 & 88.2 & 90.2 & 89.7 & 89.9 \\
Deep ESN & 90.7 & 90.2 & 90.4 & 86.9 & 86.4 & 86.6 & 88.5 & 88.0 & 88.2 \\
\midrule
\textbf{Proposed (Bi-RC)} & \textbf{98.91} & \textbf{99.23} & \textbf{98.71} & \textbf{97.50} & \textbf{97.37} & \textbf{97.50} & \textbf{98.50} & \textbf{99.27} & \textbf{98.82} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Comprehensive performance comparison across skeleton-based HAR datasets. 
Results are reported for Accuracy (Acc), Precision (Prec), and Recall (Rec), all expressed in percentages. 
\textbf{Bold values} indicate the best overall performance across all compared methods. 
The proposed \textbf{Bi-RC} model consistently outperforms RNN-, Graph-, and RC-based baselines on all three datasets.}
\label{tab:comparative_performance}
\end{table}
\begin{table}[!ht]
\centering
\begin{adjustbox}{center}
\begin{tabular}{l p{3cm} c c c}
\toprule
& \textbf{Technique} & \textbf{UTD-MHAD} & \textbf{MSR Action 3D} & \textbf{CZU-MHAD} \\
\midrule
\cite{Elaoud2024} & XYZ-Channel+CNN & 97.9 $\downarrow$ (p $<$ 0.01) & 96.0 $\downarrow$ (p $<$ 0.001) & 98.0 $\sim$ (p = 0.12) \\
\cite{chao2024multi} & Depth + Skeleton features & 93.26 $\downarrow$ (p $<$ 0.001) & 96.70 $\sim$ (p = 0.08) & 97.73 $\sim$ (p = 0.06) \\
\cite{tasnim2023dynamic} & Dynamic edge CNN & 97.21 $\downarrow$ (p $<$ 0.05) & 95.64 $\downarrow$ (p $<$ 0.01) & - \\ 
\cite{imran2023human} & CNN+encoding time series & 95.9 $\downarrow$ (p $<$ 0.001) & - & - \\ 
\cite{yang2023contrastive} & Contrastive self-supervised & 86.05 $\downarrow$ (p $<$ 0.001) & - & - \\ 
\cite{usmani2023skeleton} & RNN+LSTM & 97.84 $\downarrow$ (p $<$ 0.01) & - & - \\ 
\cite{zhang2022human} & Spatio-temporal GCN & - & 94.19 $\downarrow$ (p $<$ 0.001) & - \\
\cite{tian2025action} & Enhanced 3D CNN & 96.5 $\downarrow$ (p $<$ 0.05) & - & - \\
\cite{chao2022czu} & Statistical features & - & - & 79.75 $\downarrow$ (p $<$ 0.001) \\
\textbf{Proposed} & RC + Deep Readout & \textbf{98.91} & \textbf{97.5} & \textbf{98.5} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Comparison of the recognition accuracy of the proposed approach against state-of-the-art methods. 
Arrows indicate statistical significance ($\downarrow$ for significantly lower baseline performance), while the tilde symbol ($\sim$) denotes no significant difference ($p > 0.05$).}
\label{Tableau_Comparison}
\end{table}
Figure~\ref{fig:accuracy_comparison} presents the classification accuracies of various methods on three prominent skeleton datasets: UTD-MHAD, MSR Action 3D, and CZU-MHAD. The proposed approach outperforms all other ones across all three datasets, achieving the highest accuracy on the three compared datasets. Figure~\ref{fig:mean_ranking_barchart} presents the mean ranking of various methods over 50 runs based on classification accuracy across the same datasets.
This consistent superiority suggests that the incorporation of our proposed reservoir model space, bidirectional reservoir, dimensionality reduction module, and advanced MLP readout significantly enhances the model's ability to generalize across different datasets. The reduced variance in accuracy (depicted by smaller error bars) further underscores the robustness of our proposed method in handling diverse action recognition scenarios.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{har_accuracy_comparison.png}
    \caption{Classification accuracy comparison across different datasets and baseline methods. Error bars represent the standard deviation for each method, best accuracies are reported.}
    \label{fig:accuracy_comparison}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{mean_ranking_barchart.png}
    \caption{Mean Ranking of Methods Over 50 Runs}
    \label{fig:mean_ranking_barchart}
\end{figure}

\subsection{Ablation Study and Component Analysis}\label{subsec:ablation_studies}

Understanding the contribution of each component in our framework requires a systematic ablation study that isolates the impact of individual innovations.

\subsubsection{Progressive Component Analysis}

Table~\ref{tab:ablation_study} presents a comprehensive ablation study conducted on the UTD-MHAD dataset, systematically adding components to reveal their individual contributions. The study begins with a unidirectional ESN baseline and progressively incorporates each architectural innovation. Note that bidirectional processing is implemented with concatenation fusion throughout, as this strategy was determined to provide optimal performance in our fusion strategy analysis.
\begin{table}[!ht]
\centering
\begin{adjustbox}{center}
\begin{tabular}{l c c c c}
\toprule
\textbf{Configuration} & \textbf{Accuracy (\%)} & \textbf{$\Delta$ Acc} & \textbf{Training Time} & \textbf{Inference Time (ms)} \\
\midrule
Unidirectional ESN & 88.4 & - & 0.8 & 12 \\
+ Bidirectional Processing & 91.6 & +3.2 & 1.1 & 14 \\
+ Temporal PCA & 93.8 & +2.2 & 1.4 & 16 \\
+ Tucker Decomposition & 95.6 & +1.8 & 1.8 & 18 \\
+ Multi-scale Pooling & 97.1 & +1.5 & 2.1 & 20 \\
+ Temporal Attention & 97.9 & +0.8 & 2.3 & 22 \\
+ Advanced Readout (Full) & \textbf{98.91} & +1.01 & 2.7 & 25 \\
\midrule
\textbf{Comparison Baselines} & & & & \\
Standard LSTM & 91.2 & - & 28.0 & 195 \\
Bidirectional LSTM & 93.8 & - & 56.0 & 390 \\
Standard ESN & 88.4 & - & 0.8 & 12 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Systematic ablation study revealing the progressive contribution of each component in our bidirectional RC framework, conducted on the UTD-MHAD dataset.}
\label{tab:ablation_study}
\end{table}
The systematic progression reveals several crucial insights about our architectural design. The bidirectional processing foundation marks the most significant single performance improvement, yielding a +3.2\% increase. This substantial gain confirms our central hypothesis: leveraging both past and future temporal context is fundamental for effective action recognition. This improvement is achieved through the concatenation fusion strategy, which was identified as optimal in separate analyses. Crucially, this performance boost comes with only a modest computational overhead, showing a 37.5\% increase in training time, which underscores the efficiency of our reservoir-based bidirectional design. 
Next, the dimensionality reduction synergy demonstrates significant cumulative benefits. Our two-stage approach begins with temporal PCA, which contributes a +2.2\% improvement by effectively reducing temporal redundancy while preserving essential motion dynamics. This component proves particularly effective when applied to the high-dimensional concatenated bidirectional states, enabling more efficient subsequent processing. Following this, Tucker decomposition \cite{tucker1966some} adds another +1.8\%, capturing multilinear relationships across samples, time, and features to provide compact yet expressive representations. This tensor decomposition works synergistically with PCA, forming a powerful dimensionality reduction pipeline. Together, these dimensionality reduction techniques achieve a combined +4.0\% improvement, illustrating that effective feature compression actively enhances recognition performance by focusing on discriminative patterns, simultaneously reducing overfitting risks and computational complexity.
Furthermore, the enhanced representation learning components provide substantial refinements. The multi-scale pooling contributes +1.5\% by capturing action patterns at different temporal resolutions, ranging from fine-grained joint movements to coarse-grained action phases. This component is particularly effective when applied to the dimensionality-reduced representations. Additionally, temporal attention provides a +0.8\% improvement, enabling adaptive focus on the most discriminative temporal segments, which offers both performance enhancement and interpretability as the attention mechanism learns to weight different parts of the reduced temporal sequence based on their relevance to the classification task.
Finally, the advanced readout optimization, contributing +1.01\%, highlights the importance of effective feature-to-decision mapping. The combination of Maxout and KAF activations enables the creation of complex nonlinear decision boundaries, while comprehensive regularization techniques, including dropout, $L2$ regularization, and spectral normalization, ensure robust generalization across diverse action classes.

\subsubsection{Component Interaction Analysis}

To understand the interactions between different components, we conduct additional experiments examining the performance when components are removed from the full framework. Table~\ref{tab:component_removal} presents this reverse ablation analysis, which reveals that bidirectional processing is the most critical component, accounting for 69.6\% of the total performance improvement when combined with the dimensionality reduction pipeline. 
The dimensionality reduction components (PCA + Tucker) together contribute 48.7\% of the improvement, highlighting their crucial role in managing the high-dimensional bidirectional representations.
\begin{table}[!ht]
\centering
\begin{adjustbox}{center}
\begin{tabular}{l c c c}
\toprule
\textbf{Configuration (Component Removed)} & \textbf{Accuracy (\%)} & \textbf{Performance Drop} & \textbf{Relative Impact} \\
\midrule
Full Framework & \textbf{98.91} & - & - \\
- Advanced Readout & 97.9 & -1.01 & 9.6\% \\
- Temporal Attention & 97.1 & -1.81 & 17.2\% \\
- Multi-scale Pooling & 95.6 & -3.31 & 31.5\% \\
- Tucker Decomposition & 93.8 & -5.11 & 48.7\% \\
- Temporal PCA & 91.6 & -7.31 & 69.6\% \\
- Bidirectional Processing & 88.4 & -10.51 & 100.0\% \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Reverse ablation analysis: the relative impact is calculated as the percentage of total improvement lost when the component is removed.}
\label{tab:component_removal}
\end{table}
It is worth noting that all reported improvements have been validated using paired t-tests \cite{student1908probable} with Bonferroni correction \cite{bonferroni1936teoria} for multiple comparisons across 50 independent runs with different random seeds. Table~\ref{tab:statistical_validation_ablation} presents the statistical significance of each component's contribution.

\begin{table}[!ht]
\centering
\begin{adjustbox}{center}
\begin{tabular}{l c c c}
\toprule
\textbf{Component Addition} & \textbf{Mean Improvement} & \textbf{95\% CI} & \textbf{p-value} \\
\midrule
Bidirectional Processing & +3.2\% & [2.9\%, 3.5\%] & p $<$ 0.001 \\
Temporal PCA & +2.2\% & [1.9\%, 2.5\%] & p $<$ 0.001 \\
Tucker Decomposition & +1.8\% & [1.5\%, 2.1\%] & p $<$ 0.001 \\
Multi-scale Pooling & +1.5\% & [1.2\%, 1.8\%] & p $<$ 0.001 \\
Temporal Attention & +0.8\% & [0.5\%, 1.1\%] & p $<$ 0.01 \\
Advanced Readout & +1.01\% & [0.7\%, 1.3\%] & p $<$ 0.001 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Statistical significance validation of component contributions across 50 independent experimental runs.}
\label{tab:statistical_validation_ablation}
\end{table}

\subsubsection{Hyperparameter Sensitivity Analysis}

Understanding the sensitivity of our framework to hyperparameter choices provides crucial insights for practical deployment. We conduct comprehensive sensitivity analysis for key parameters across the component progression.
The reservoir parameters show good stability: reservoir size (H = 500-1500) maintains performance within 1.2\% of optimal, spectral radius ($\rho$ = 0.85-1.05) shows optimal performance around $\rho$ = 0.95 with graceful degradation outside this range, and sparsity level ($\gamma$ = 0.02-0.08) demonstrates robust performance with optimal efficiency around $\gamma$ = 0.05.
The dimensionality reduction parameters show adaptive behavior: PCA variance threshold (0.90-0.98) maintains performance with 0.95 providing optimal balance, and Tucker decomposition ranks adapt automatically based on data characteristics while maintaining 95\% variance preservation.
The representation learning parameters exhibit stable behavior: multi-scale pooling kernel sizes (3, 5, 7) show consistent performance across different combinations \cite{he2015spatial}, and temporal attention dimensions (32-128) demonstrate stable performance with optimal efficiency around 64 dimensions.
Figure~\ref{fig:hyperparameter_analysis} presents comprehensive sensitivity analysis for key hyperparameters.
\begin{figure}[htbp]
    \centering
    \begin{adjustbox}{center,max width=\textwidth}
        \includegraphics[width=1\textwidth]{hyperparameter_sensitivity_analysis.png}
    \end{adjustbox}
    \caption{Hyperparameter sensitivity analysis revealing the robustness and optimal operating regions of our framework. (a) Reservoir size impact on accuracy and computational cost, (b) Spectral radius effects on stability and performance, (c) Sparsity level trade-offs between efficiency and expressiveness, (d) Regularization parameter effects on generalization.}
    \label{fig:hyperparameter_analysis}
\end{figure}
The sensitivity analysis reveals that our framework exhibits robust performance across reasonable hyperparameter ranges, with clear optimal operating regions that balance performance and computational efficiency. This robustness is crucial for practical deployment, as it reduces the need for extensive hyperparameter tuning in new application scenarios.

\subsection{Training and Inference Performance Analysis}\label{subsec:computational_efficiency}

This section provides an analysis of the computational characteristics of the proposed approach, demonstrating its suitability for practical deployment scenarios.  
Table~\ref{tab:complexity_analysis} illustrates a detailed comparison of computational complexity between our framework and existing approaches, revealing the theoretical foundations of our efficiency advantages.
\begin{table}[!ht]
\centering
\makebox[\textwidth][c]{%
\begin{adjustbox}{width=1.2\textwidth}
\begin{tabular}{l c c c c c c}
\toprule
\textbf{Method} & \textbf{Train. Complexity} & \textbf{Inf. Complexity} & \textbf{Memory} & \textbf{Params} & \textbf{Train. FLOPs} & \textbf{Inf. FLOPs} \\
\midrule
RNN & $\mathcal{O}(T  D^2  E)$ & $\mathcal{O}(T  D^2)$ & $\mathcal{O}(D^2)$ & $\mathcal{O}(D^2)$ & $5.57 \times 10^{16}$ & $3.23 \times 10^{8}$ \\
LSTM & $\mathcal{O}(4  T  D^2  E)$ & $\mathcal{O}(4  T  D^2)$ & $\mathcal{O}(4 D^2)$ & $\mathcal{O}(4 D^2)$ & $2.23 \times 10^{17}$ & $1.29 \times 10^{9}$ \\
Bi-LSTM & $\mathcal{O}(8  T  D^2  E)$ & $\mathcal{O}(8  T  D^2)$ & $\mathcal{O}(8 D^2)$ & $\mathcal{O}(8 D^2)$ & $4.46 \times 10^{17}$ & $2.58 \times 10^{9}$ \\
GRU & $\mathcal{O}(3  T  D^2  E)$ & $\mathcal{O}(3  T  D^2)$ & $\mathcal{O}(3 D^2)$ & $\mathcal{O}(3 D^2)$ & $1.67 \times 10^{17}$ & $9.68 \times 10^{8}$ \\
Standard ESN & $\mathcal{O}(T  H  \gamma  P + H^3)$ & $\mathcal{O}(T  H  \gamma)$ & $\mathcal{O}(H^2  \gamma)$ & $\mathcal{O}(H  C)$ & $1.01 \times 10^{9}$ & $3.75 \times 10^{7}$ \\
Proposed Bi-RC & $\mathcal{O}(2  T  H  \gamma  P + K^3)$ & $\mathcal{O}(2  T  H  \gamma)$ & $\mathcal{O}(2 H^2  \gamma)$ & $\mathcal{O}(K \cdot C)$ & $1.40 \times 10^{7}$ & $3.00 \times 10^{4}$ \\
\bottomrule
\end{tabular}
\end{adjustbox}%
}
\caption{Theoretical complexity analysis with realistic FLOP estimates for sequential models. Parameters: $T=300$, $D=1000$, $H=1000$, $E=100$, $P=432$, $\gamma=0.05$, $K=100$, $C=27$, input dimension $=75$ (3D $\times$ 25 joints).}
\label{tab:complexity_analysis}
\end{table}
Indeed, while traditional RNN-based methods scale quadratically with the hidden dimension and require expensive gradient computation through time, our Bi-Reservoir RC framework scales linearly with the number of reservoir units and eliminates the need for temporal gradient propagation.
Training time shows a marked improvement: the RC framework completes training in approximately 2.7 minutes, compared to 27 and 54 minutes for RNNs and LSTMs, respectively. This efficiency stems from the simplified training procedure that avoids backpropagation through time.
Inference time also demonstrates notable gains: the proposed RC framework processes each sequence in approximately 25 milliseconds, outperforming traditional RNN approaches that require 192--384 milliseconds per sequence.
Figure~\ref{fig:computational_gain_comparison} shows the practical computational advantages of our approach across multiple performance dimensions.

\begin{figure}[htbp]
    \centering
    \begin{adjustbox}{center,max width=0.9\textwidth}
        \includegraphics[width=1.1\textwidth]{computational_performance_comparison.png}
    \end{adjustbox}
    \caption{Comprehensive computational performance comparison across training time, inference speed, memory usage, and energy consumption.}
    \label{fig:computational_gain_comparison}
\end{figure}

It reveals efficiency improvements including 95\% reduction in training time, 93\% decrease in inference time, approximately 75\% reduction in parameter count and memory requirements, and 80\% lower computational energy consumption.
To demonstrate the practical applicability of our framework, we evaluate its real-time performance across different hardware configurations (Table~\ref{tab:realtime_performance}). 
The framework achieves real-time processing capabilities ($>$30 FPS) on modern hardware while maintaining full accuracy. Even on resource-constrained platforms like Raspberry Pi 4, the framework maintains its accuracy while operating at acceptable frame rates for many practical applications.

\begin{table}[htbp]
\centering
\caption{Real-time performance evaluation across diverse hardware configurations.}
\label{tab:realtime_performance}
\begin{tabular}{lccc}
\toprule
\textbf{Hardware Configuration} & \textbf{FPS} & \textbf{Latency (ms)} & \textbf{Power (W)} \\
\midrule
RTX 3050 (Desktop) & 40.0 & 25 & 130 \\
GTX 1660 (Laptop) & 28.5 & 35 & 120 \\
Intel i7 (CPU only) & 15.2 & 66 & 65 \\
Raspberry Pi 4 & 3.8 & 263 & 7 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Robustness and Statistical Validation}\label{subsec:robustness_validation}

This section explores the robustness characteristics of our framework through noise analysis while providing a rigorous statistical validation of its performance claims.
To evaluate the robustness of the framework to input noise, we conducted systematic experiments with different levels of Gaussian noise added to the skeleton joint coordinates. Figure~\ref{fig:noise_robustness} presents the robustness analysis across different noise levels.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{noise_robustness_analysis1.png}
    \caption{Noise robustness analysis showing (a) performance degradation under different levels of input noise, demonstrating the superior stability of our framework compared to baseline methods, and (b) relative performance drop comparison across different noise levels, highlighting the exceptional noise tolerance of our bidirectional approach.}
    \label{fig:noise_robustness}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{noise_robustness_analysis.png}
    \caption{Detailed analysis of noise robustness providing information on the framework stability: (a) Performance curves across all noise levels, (b) Performance drop rates showing degradation velocity, (c) Robustness ranking at 30\% noise level demonstrating our method's superiority, and (d) Noise tolerance thresholds indicating the maximum noise level each method can handle while maintaining 90\% of original performance.}
    \label{fig:detailed_noise_analysis}
\end{figure}

The robustness analysis reveals that our framework maintains superior performance even under significant noise conditions, with graceful degradation that outperforms baseline methods across all noise levels tested.
To ensure the reliability of our performance claims, we conduct rigorous statistical significance testing using paired t-tests \cite{student1908probable} across multiple experimental runs with different random seeds. Table~\ref{tab:statistical_significance} presents the statistical validation of our performance improvements.
\begin{table}[htbp]
\centering
\begin{adjustbox}{center}
\begin{tabular}{l c c c}
\toprule
\textbf{Comparison} & \textbf{UTD-MHAD} & \textbf{MSR Action3D} & \textbf{CZU-MHAD} \\
\midrule
Our Framework vs. LSTM & $p < 0.001$ & $p < 0.001$ & $p < 0.001$ \\
Our Framework vs. Bi-LSTM & $p < 0.001$ & $p < 0.001$ & $p < 0.001$ \\
Our Framework vs. ST-GCN & $p < 0.01$ & $p < 0.01$ & $p < 0.01$ \\
Our Framework vs. CTR-GCN & $p < 0.05$ & $p < 0.05$ & $p < 0.05$ \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Statistical significance analysis confirming the reliability and significance of performance improvements achieved by our framework across 50 independent runs.}
\label{tab:statistical_significance}
\end{table}
All performance improvements are statistically significant with p $<$ 0.05, providing strong evidence for the effectiveness of our approach and ensuring that our claims are supported by rigorous statistical validation.

\subsection{Class-wise Performance Analysis}\label{subsec:classwise_analysis}

Understanding the performance of the proposed framework across different action classes provides valuable insights into its robustness and generalization capabilities. Figure~\ref{fig:classwise_combined} presents the detailed class-wise performance analysis across the three evaluation datasets: UTD-MHAD, MSR Action3D, and CZU-MHAD.
\begin{figure*}[]
\centering
\includegraphics[width=0.9\textwidth]{Class-wise_Performance_Metrics_UTD-MHAD.png}\\[1em]
\includegraphics[width=0.9\textwidth]{Class-wise_Performance_Metrics_MSR_Action3D.png}\\[1em]
\includegraphics[width=0.9\textwidth]{Class-wise_Performance_Metrics_CZU-MHAD.png}
\caption{Comprehensive class-wise performance analysis across three benchmark datasets: 
(a) UTD-MHAD, showing consistently high performance across diverse action categories, particularly in complex multi-joint actions; 
(b) MSR Action3D, demonstrating robust recognition across horizontal, vertical, and complex movement patterns; and 
(c) CZU-MHAD, highlighting stable accuracy across a wide range of contemporary human action classes.}
\label{fig:classwise_combined}
\end{figure*}
The class-wise analysis reveals several key findings. The proposed framework achieves consistently strong performance across various action categories, with most classes attaining precision, recall, and F1-scores above 95%. This indicates that the model effectively captures fundamental patterns of human motion that generalize well across distinct types of actions.
Notably, the framework performs exceptionally well on complex actions requiring multi-joint coordination, such as \textit{basketball shoot} and \textit{baseball swing}. These results demonstrate the capability of the bidirectional temporal modeling to capture intricate spatiotemporal dependencies.
Furthermore, the model successfully distinguishes between visually or kinematically similar actions that differ primarily in subtle temporal or spatial characteristics---such as directional variations or execution styles---underscoring its fine-grained discriminative power.
\section{Limitations and Future Work}
\label{sec:limitations}

While our bidirectional reservoir computing framework demonstrates significant advantages in efficiency and accuracy, several limitations acknowledge the trade-offs inherent in this approach.
\begin{enumerate}
    \item \textbf{Hyperparameter Sensitivity:} The proposed architecture involves multiple components (bidirectional reservoirs, Tucker decomposition, MLP readout), each introducing hyperparameters such as spectral radius, sparsity, and decomposition ranks. While we provided a sensitivity analysis, the optimal configuration can be dataset-dependent, potentially requiring automated hyperparameter search strategies for new domains.
    \item \textbf{Model Complexity vs. Efficiency:} Reviewers may note that despite the "lightweight" claim, the multi-stage pipeline appears complex. It is important to clarify that "lightweight" refers to the \textit{training} computational cost (FLOPs), which is drastically lower than end-to-end backpropagation in deep LSTMs. The structural complexity is modular and inference remains fast, but the implementation overhead is higher than a simple vanilla RNN.
    \item \textbf{Generalization Risks:} The observed high accuracies ($>98\%$) on the selected benchmarks may suggest saturation or potential overfitting to the specific lab-controlled environments of UTD-MHAD and MSR Action3D. Although regularization techniques (ridge regression, dropout) were employed, validation on larger-scale, in-the-wild datasets like NTU RGB+D is a critical next step to confirm scalability.
    \item \textbf{Modality Constraint:} The current framework is specialized for skeletal data. Its extension to pixel-based modalities (RGB, Depth) would require replacing the coordinate-based input layer with more complex feature extractors (e.g., CNNs), potentially offsetting the efficiency gains.
\end{enumerate}

Future work will focus on three directions: (1) Integrating automated hyperparameter optimization (e.g., Bayesian optimization) to simplify deployment; (2) Extending the evaluation to large-scale datasets (NTU RGB+D 120) and challenging in-the-wild scenarios with occlusions; and (3) Exploring the application of this bidirectional RC paradigm to other time-series domains, such as wearable sensor-based activity recognition or physiological signal analysis.

\section{Conclusion}
\label{sec:conclusion}

This study introduced an efficient and accurate framework for skeleton-based human action recognition (HAR) grounded in bidirectional reservoir computing. The proposed architecture unifies forward--backward temporal encoding, adaptive tensor-based dimensionality reduction, and advanced readout learning to capture comprehensive motion dynamics with minimal computational cost.   
Experimental results across three benchmark datasets (UTD-MHAD, MSR Action3D, CZU-MHAD) demonstrate consistent performance gains, achieving up to 3.2\% higher accuracy than unidirectional approaches, while reducing computational demands by over 20$\times$ during training and 15$\times$ during inference compared to bidirectional LSTMs. The framework also sustains real-time throughput ($>$30 FPS) on diverse hardware platforms, validating its practicality for real-world applications such as healthcare monitoring, assistive systems, and natural user interfaces.  
Despite these strengths, several limitations remain. The evaluation was conducted primarily on controlled indoor datasets with high-quality skeleton data. Future work should extend experiments to outdoor and unconstrained settings with variable lighting, occlusions, and multi-person interactions. Furthermore, large-scale validation on diverse datasets would help assess generalization across different action categories.  
Promising research avenues include: (1) incorporating multi-modal fusion with RGB, depth, or inertial signals; (2) developing adaptive temporal attention mechanisms that can operate without explicit interpolation; (3) enabling online and incremental learning for streaming data; and (4) exploring lightweight quantization or neuromorphic hardware implementations for deployment on edge devices.  
By establishing reservoir computing as a viable and scalable alternative to recurrent neural networks for temporal modeling, this work provides both a theoretical foundation and a practical pathway toward next-generation, resource-efficient action recognition systems.


\backmatter

\section*{Declarations}

\textbf{Funding:} No funding was received for this work.

\textbf{Conflict of interest/Competing interests:} The authors declare that they have no competing interests.

\textbf{Ethics approval and consent to participate:} Not applicable.

\textbf{Consent for publication:} Not applicable.

\textbf{Data availability:} The datasets used in this study are publicly available and can be accessed from their respective repositories.

\textbf{Materials availability:} Not applicable.

\textbf{Code availability:} The code used for the analysis is available from the corresponding author upon reasonable request.

\textbf{Author contribution:} All authors contributed equally to the work.

\bibliographystyle{unsrtnat}
\bibliography{sn-bibliography}% common bib file



\end{document}