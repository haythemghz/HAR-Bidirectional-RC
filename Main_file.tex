\documentclass[pdflatex]{sn-jnl} % Math and Physical Sciences Numbered Reference Style

% Standard Packages
\usepackage{graphicx}
\graphicspath{{figures/}}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage[title]{appendix}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{manyfoot}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{tabularx}
\usepackage{ulem}
\usepackage{array}
\usepackage{adjustbox}
\usepackage[numbers,sort&compress]{natbib}
% \usepackage[bookmarks=false]{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{multicol}
\usepackage{eqparbox}
\usepackage{diagbox}
\usepackage{color, colortbl}
\usepackage{epsf}
\usepackage{pdfpages}
\usepackage{calc}
\usepackage{psfrag}
\usepackage{url}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{lipsum}
\usepackage{stfloats}
\usepackage{verbatim}

\begin{document}

\title[Bidirectional Reservoir Computing for Enhanced Human Action Recognition Using Skeleton Data]{Bidirectional Reservoir Computing for Enhanced Human Action Recognition Using Skeleton Data}

\author[1,2]{\fnm{Haythem} \sur{Ghazouani}}\email{haythem.ghazouani@enicar.u-carthage.tn}

\author[1,2]{\fnm{Walid} \sur{Barhoumi}}\email{walid.barhoumi@enicarthage.rnu.tn}

\equalcont{These authors contributed equally to this work.}

\affil[1]{\orgdiv{Universit\'{e} de Tunis El Manar}, \orgname{Institut Sup\'{e}rieur d'Informatique, Research Team on Intelligent Systems in Imaging and Artificial Vision (SIIVA), LR16ES06 Laboratoire de recherche en Informatique, Mod\'{e}lisation et Traitement de l'Information et de la Connaissance (LIMTIC)}, \orgaddress{\street{2 Rue Abou Rayhane Bayrouni}, \city{Ariana}, \postcode{2080}, \country{Tunisia}}}

\affil[2]{\orgdiv{Universit\'{e} de Carthage}, \orgname{Ecole Nationale d'Ing\'{e}nieurs de Carthage}, \orgaddress{\street{45 Rue des Entrepreneurs}, \city{Tunis-Carthage}, \postcode{2035}, \country{Tunisia}}}

\abstract{
Skeleton-based human action recognition (HAR) addresses privacy and computational concerns but remains challenged by the temporal modeling limitations of conventional recurrent networks like LSTMs. We propose a bidirectional RC framework leveraging the \textbf{synergistic integration} of three key mechanisms: a bidirectional reservoir architecture for full temporal context; adaptive multi-view dimensionality reduction (PCA + Tucker); and a Maxout-enhanced readout. Through extensive experimentation, our framework demonstrates a superior trade-off between architectural efficiency and recognition accuracy. It attains \textbf{$98.9 \pm 0.1\%$} on UTD-MHAD and \textbf{$95.7 \pm 0.2\%$} on CZU-MHAD, while on large-scale benchmarks it achieves \textbf{$93.2 \pm 0.2\%$} on NTU60 (X-Sub) and \textbf{$38.7 \pm 0.5\%$} on Kinetics-Skeleton. The framework reduces training time by 95\% and inference latency by 94.6\% compared to bidirectional recurrent baselines, providing a modular and computationally lightweight alternative for edge deployment. Our primary novelty lies in the \textbf{spatiotemporal synergy} achieved by decoupling temporal feature extraction from nonlinear mapping, allowing for high-dimensional sequence modeling without gradient-based optimization. Source code is available at \url{https://github.com/haythemghz/HAR-Bidirectional-RC}.
}

\keywords{Human Action Recognition, Skeleton Data, Reservoir Computing, Bidirectional Processing, Echo State Networks, Temporal Modeling}

\maketitle

\section{Introduction}
Human Action Recognition (HAR) has emerged as a cornerstone of intelligent systems, enabling applications ranging from healthcare monitoring to automated surveillance. While foundational approaches relied on RGB video, the field is increasingly shifting towards skeleton-based methods to address privacy concerns and computational constraints.

The shift from RGB-based to skeleton-based HAR is driven by three critical advantages: privacy preservation, reduced computational cost, and environmental robustness. Unlike RGB video, which captures sensitive personal details and is susceptible to lighting variations and background clutter, skeleton data provides a compact, anonymity-preserving representation of human pose. This geometric abstraction significantly lowers input dimensionality, enabling efficient processing while maintaining high recognition accuracy across diverse environments and subject appearances.
Despite these advantages, effective temporal modeling remains a challenge. Standard sequential models like LSTMs suffer from high computational overhead and unidirectional constraints when processing long action sequences, while deep alternatives like Transformers/GCNs introduce latency bottlenecks on edge devices.

\textbf{Research Gap:} Despite these advancements, a critical gap remains. While recent methods aim to lower GCN complexity, they still rely on deep iterative layers that require expensive gradient-based optimization. There is currently no framework that effectively combines the \textit{architectural efficiency} of Reservoir Computing—which serves as a non-iterative, fixed backbone—with the advanced temporal modeling capabilities required for competitive performance.

In response, we propose a bidirectional Reservoir Computing framework that addresses the efficiency-accuracy trade-off by decoupling temporal feature extraction from supervised learning. While our pipeline incorporates sophisticated components (Tucker decomposition, multi-view reduction), these are designed as efficient, fixed-cost linear or tensor operations. This design choice allows us to shift the computational burden away from the iterative training loop (Backpropagation Through Time), resulting in a model that is \textit{modularly structured but computationally lightweight}.

The proposed framework integrates three synergistic contributions:
1. A bidirectional reservoir architecture that captures full temporal context without doubling the memory cost associated with gradient-tracking in BPTT.
2. An adaptive multi-view dimensionality reduction module (PCA + Tucker) that distills essential motion patterns from high-dimensional reservoir states, acting as a principled "feature compressor."
3. A Maxout-enhanced readout mechanism that enables high-precision classification from fixed temporal features.

The \textbf{core novelty} of this work resides in the \textit{systematic synchronization} of these components: the bidirectional dynamics ensure temporal completeness, the multilinear decomposition preserves the spatiotemporal topology of joints, and the Maxout activation handles the non-convexities of compressed action manifolds. This synergy allows RC to match the performance of deeply optimized GCNs on several benchmarks while requiring only a fraction of the computational budget.
Indeed, we demonstrate that high performance and remarkable efficiency can be effectively balanced, achieving competitive accuracy across multiple benchmark datasets alongside substantial improvements in computational throughput.

The remainder of this paper unfolds as follows. Section~\ref{sec:related_work} provides a comprehensive survey of related work, positioning our contributions within the broader research landscape. Section~\ref{sec:background} establishes the theoretical foundations that underpin our approach. Section~\ref{sec:methodology} presents the proposed bidirectional reservoir computing framework in detail. Section~\ref{sec:experiments} reports extensive experimental results that validate the proposed approach. Finally, Section~\ref{sec:conclusion} concludes the paper and outlines future research directions.

\section{Related Work}\label{sec:related_work}
The development of effective HAR has been driven by continuous innovation and evolving methodologies, with a persistent focus on accurately modeling the temporal dynamics of human movement. This section reviews the progression of skeleton-based HAR techniques, from early handcrafted feature approaches to deep learning models, and highlights the emerging potential of reservoir computing methods.

Early research in skeleton-based HAR primarily focused on handcrafted feature extraction to identify geometric and temporal patterns manually. Xia et al. \cite{xia2012view} developed histograms of 3D joint locations for view-invariant recognition, while Wang et al. \cite{wang2012mining} introduced actionlets to decompose complex actions into simpler motion primitives. Mathematically elegant approaches, such as representing sequences as curves in the Lie group SE(3) \cite{vemulapalli2014human}, established foundational geometric principles. However, these traditional methods were ultimately limited by their reliance on manual feature engineering and domain expertise, which constrained their scalability to larger, more complex datasets.


The advent of deep learning enabled architectures to automatically learn temporal and spatial relationships. Early RNN-based successes included hierarchical models \cite{du2015hierarchical} and part-aware LSTMs \cite{shahroudy2016ntu} that addressed body-part specialized dynamics. The field subsequently moved toward Graph Convolutional Networks (GCNs) to explicitly model the human body's topological structure. Spatial-Temporal GCN (ST-GCN) \cite{yan2018spatial} established a baseline by treating skeletons as graphs, while adaptive variants like 2s-AGCN \cite{shi2019two} and Shift-GCN \cite{cheng2020skeleton} further refined spatial-temporal feature extraction through multi-scale and multi-stream modeling. Recently, Attention-based transformers \cite{plizzari2021spatial} have demonstrated competitive performance by capturing long-range dependencies, albeit at significant computational cost.


\subsection{Reservoir Computing for Temporal Sequence Processing}\label{subsec:reservoir_computing_approaches}
While the mainstream HAR research community was exploring complex deep learning architectures, a parallel stream of research was investigating RC as an alternative paradigm for temporal sequence processing. This approach, rooted in the principles of dynamical systems theory, offered a fundamentally different perspective on temporal modeling.
The theoretical foundations of RC, established by Jaeger \cite{jaeger2001echo} and Maass \cite{maass2002real}, demonstrate that fixed, randomly initialized recurrent networks can project input sequences into high-dimensional spaces for linear classification. This paradigm shift avoids the vanishing gradient issues of BPTT \cite{lukosevicius2009reservoir} and has been extended to deep architectures \cite{gallicchio2017deep} to capture multi-scale temporal dynamics while maintaining superior computational efficiency compared to standard RNN approaches \cite{verstraeten2007experimental}.
The application of RC to HAR has been limited but promising. Picco et al. \cite{PICCO2023662} introduced novel training methods for RC in HAR contexts, using "timesteps of interest" to effectively combine short and long time scales. Their approach achieved high accuracy on video-based datasets while maintaining real-time processing capabilities.
Antonik et al. \cite{Antonik2019} explored photonic hardware implementations of RC for HAR, demonstrating the potential for ultra-fast processing using optical components. Most relevant to our work, Gallicchio et al. \cite{gallicchio2016reservoir} developed reservoir computing approaches for human gesture recognition from Kinect data, representing one of the few works directly addressing skeleton-based HAR with RC. However, their approach lacked comprehensive evaluation and comparison with state-of-the-art deep learning methods.
Overall, the survey of the literature reveals several significant research gaps that motivate the proposed work. Limited bidirectional processing in RC exists, as while bidirectional processing has proven valuable in RNN-based approaches, most RC-based HAR methods employ unidirectional processing, potentially missing valuable future context information that could improve recognition accuracy. Insufficient comparative analysis is evident, as existing RC studies for HAR lack comprehensive comparison with state-of-the-art deep learning methods, particularly bidirectional LSTMs and GRU networks, making it difficult to assess the true potential of RC approaches.
Scalability challenges persist, as current approaches struggle with the high-dimensional reservoir states generated by complex temporal sequences, requiring more sophisticated dimensionality reduction strategies that can preserve essential temporal dynamics while improving computational efficiency. Limited theoretical understanding exists, as most studies lack detailed computational complexity analysis and theoretical justification for design choices, hindering the development of principled approaches to RC for HAR.
The proposed work addresses these gaps by introducing a comprehensive bidirectional RC framework that uniquely specializes reservoir computing for the spatiotemporal complexities of skeleton data. While the individual components—bidirectional reservoirs, Tucker decomposition, and Maxout activations—are known in isolation across different machine learning domains, our primary novelty lies in their \textbf{synergistic integration} into a unified architecture specifically mapped to the high-dimensional temporal manifold of skeleton sequences. This synergy is twofold: (1) the bidirectional fixed-dynamics reservoir captures long-term temporal context without BPTT, (2) the rank-adaptive Tucker decomposition physically compresses these high-dimensional reservoir states while preserving the multilinear spectral energy, and (3) the Maxout-enhanced readout allows the compressed features to be mapped to convex decision boundaries with minimal parameters. This specific architectural combination allows us to position RC not just as a "fast alternative," but as an efficiency-optimal paradigm for skeleton-based HAR. Table~\ref{tab:related_work_summary} summarizes the evolution of skeleton-based HAR methods and positions our contribution within this research landscape.
This historical perspective positions our work as a natural progression in the evolution of HAR research, uniting the computational efficiency of RC with the advanced temporal modeling strategies developed over decades of research in RNN-based and graph-based methods.
\begin{table}[!htbp]
\centering
\caption{Evolution of skeleton-based HAR approaches: from traditional methods to the proposed framework.}
\label{tab:related_work_summary}
\begin{tabular}{p{2cm}p{3cm}p{3cm}p{3cm}}
\toprule
\textbf{Era} & \textbf{Representative Approaches} & \textbf{Key Innovations} & \textbf{Limitations} \\
\midrule
\textbf{Traditional Machine Learning} & Handcrafted features \cite{xia2012view}, Actionlets \cite{wang2012mining}, Lie groups \cite{vemulapalli2014human} & View invariance, hierarchical decomposition, mathematical foundations & Manual engineering, limited scalability, poor generalization \\
\midrule
\textbf{RNN Era} & Hierarchical RNNs \cite{du2015hierarchical}, Part-aware LSTMs \cite{shahroudy2016ntu}, View-adaptive RNNs \cite{zhang2017view} & Automatic feature learning, attention mechanisms, view adaptation & Vanishing gradients, computational complexity, hyperparameter sensitivity \\
\midrule
\textbf{Graph Era} & ST-GCN \cite{yan2018spatial}, Two-stream GCNs \cite{shi2019two}, Multi-scale approaches \cite{cheng2020skeleton} & Structural modeling, multi-stream processing, multi-scale patterns & Complex design, high memory requirements, limited interpretability \\
\midrule
\textbf{Transformer Era} & Skeleton transformers \cite{plizzari2021spatial} & Long-range dependencies, interpretable attention & Data requirements, computational complexity \\
\midrule
\textbf{RC Exploration} & Timesteps of interest \cite{PICCO2023662}, Photonic RC \cite{Antonik2019}, Bidirectional ESNs \cite{gallicchio2016reservoir} & Computational efficiency, hardware potential, bidirectional processing & Limited evaluation, lack of deep learning comparison \\
\bottomrule
\end{tabular}
\end{table}



Recent advancements in skeleton-based HAR continue to push the boundaries of accuracy. Important contributions include EfficientGCN \cite{song2023efficientgcn} which focuses on model efficiency, and PoseConv3D \cite{duan2022poseconv3d} which leverages 3D CNNs for skeleton-based recognition. These works provide strong baselines for both accuracy and efficiency.


\section{Theoretical Background for Temporal Dynamics Modeling}


\label{sec:background}

\subsection{Mathematical Representation of Human Actions}\label{subsec:mathematical_representation}

Human actions, when viewed through the %lens of 
skeleton-based representation, can be modeled as structured spatiotemporal patterns that evolve through time and space. Each moment in an action sequence captures a snapshot of human pose through the 3D coordinates of anatomical joints, creating a sequence of spatiotemporal feature vectors that encodes the essence of human movement. 
%Consider the mathematical representation of this temporal evolution. A
Formally, at any discrete time step $t$, the human skeleton configuration can be captured through a collection of $N$ anatomical joints $\mathcal{J} = \{j_1, j_2, \ldots, j_N\}$, where each joint $j_i$ is characterized by its Cartesian coordinates $(x_i, y_i, z_i)$ in %three-dimensional 
3D space. This spatial configuration gives rise to a feature vector $\mathbf{x}(t) \in \mathbb{R}^{3N}$ that encapsulates the complete pose information: % at time $t$:

\begin{equation}
\mathbf{x}(t) = [x_1(t), y_1(t), z_1(t), x_2(t), y_2(t), z_2(t), \ldots, x_N(t), y_N(t), z_N(t)]^T
\end{equation}

The temporal dimension emerges as we observe the evolution of these pose configurations across time. A complete action sequence spanning $T$ time steps forms a spatiotemporal matrix $\mathbf{X} \in \mathbb{R}^{T \times 3N}$ that captures the full trajectory of human motion: % movement:

\begin{equation}
\mathbf{X} = [\mathbf{x}(1), \mathbf{x}(2), \ldots, \mathbf{x}(T)]^T
\end{equation}

This mathematical representation reveals the fundamental challenge of skeleton-based HAR: learning a mapping function $f: \mathbb{R}^{T \times 3N} \rightarrow \{1, 2, \ldots, C\}$ that can assign each spatiotemporal sequence $\mathbf{X}$ to one of $C$ action classes. The complexity of this mapping lies not merely in the high-dimensional nature of the input space, but in the intricate temporal dependencies that characterize human movement patterns.

\subsection{Reservoir Computing Principles}

The limitations of traditional gradient-based temporal models—specifically the computational cost of Backpropagation Through Time (BPTT) and the saturation of gating mechanisms in LSTMs/GRUs \cite{hochreiter1997long,cho2014learning}—have motivated alternative processing paradigms. Reservoir Computing (RC) represents a fundamental departure from these iterative optimization schemes, offering a framework that separates temporal feature extraction from supervised learning.
The core insight underlying %Reservoir Computing 
RC is that effective temporal processing does not necessarily require the optimization of all network parameters through gradient descent. Instead, a fixed randomly initialized recurrent network (the reservoir) can serve as a rich dynamical system that projects input sequences into high-dimensional spaces where simple linear readout layers can perform classification or regression. 
This separation of concerns offers several theoretical and practical advantages. From a theoretical perspective, it eliminates the need for gradient propagation through the temporal sequence, avoiding the vanishing and exploding gradient problems that plague traditional RNNs. From a practical perspective, it dramatically reduces computational complexity by limiting parameter optimization to the final readout layer. 
%Echo State Networks 
ESNs represent the most widely studied implementation of %Reservoir Computing 
RC principles. An ESN consists of three components: an input layer that projects inputs to the reservoir space, a reservoir of recurrently connected processing units, and a readout layer that maps reservoir states to outputs. 
The reservoir state evolution follows a simple update rule:

\begin{equation}
\mathbf{r}(t) = (1-\alpha)\mathbf{r}(t-1) + \alpha \cdot f(\mathbf{W}_{in}\mathbf{x}(t) + \mathbf{W}_{res}\mathbf{r}(t-1) + \mathbf{b}_{res})
\end{equation}

where $\mathbf{r}(t) \in \mathbb{R}^H$ is the reservoir state, $\alpha \in %(
[0,1]$ is the leak rate, $\mathbf{W}_{in} \in \mathbb{R}^{H \times 3N}$ is the input weight matrix, $\mathbf{W}_{res} \in \mathbb{R}^{H \times H}$ is the reservoir weight matrix, and $f(\cdot)$ is the reservoir activation function. 
The key insight is that $\mathbf{W}_{in}$ and $\mathbf{W}_{res}$ are fixed and randomly initialized, never updated during training. Only the readout layer weights are learned through simple linear regression:

\begin{equation}
\mathbf{W}_{out}^* = \arg\min_{\mathbf{W}} \|\mathbf{R}\mathbf{W}^T - \mathbf{Y}\|_F^2 + \lambda\|\mathbf{W}\|_F^2
\end{equation}

where $\mathbf{R} \in \mathbb{R}^{P \times H}$ contains reservoir states for $P$ training samples, $\mathbf{Y} \in \mathbb{R}^{P \times C}$ contains target labels, and $\lambda$ is the regularization parameter. 
The effectiveness of reservoir computing depends critically on the Echo State Property (ESP), which ensures that reservoir dynamics depend only on recent inputs rather than initial conditions. Mathematically, the ESP requires that for any two reservoir trajectories $\mathbf{r}_1(t)$ and $\mathbf{r}_2(t)$ driven by the same input sequence but starting from different initial states:

\begin{equation}
\lim_{t \rightarrow \infty} \|\mathbf{r}_1(t) - \mathbf{r}_2(t)\| = 0
\end{equation}

A sufficient condition for the ESP is that the spectral radius $\rho(\mathbf{W}_{res}) < 1$, where $\rho(\mathbf{W}_{res})$ is the largest absolute eigenvalue of the reservoir weight matrix. This condition ensures that the reservoir operates in a stable dynamical regime where perturbations decay over time.
\subsection{Computational Complexity Analysis}
\label{subsec:computational_complexity}

Traditional sequential models trained with BPTT exhibit complexities of $\mathcal{O}(T \cdot D^2 \cdot E)$ for vanilla RNNs, $\mathcal{O}(4 \cdot T \cdot D^2 \cdot E)$ for LSTMs, and $\mathcal{O}(8 \cdot T \cdot D^2 \cdot E)$ for Bi-LSTMs, where $D$ is the hidden size and $E$ is the number of epochs. 
In contrast, RC significantly reduces this footprint by avoiding iterative gradient updates. For a reservoir of size $H$ and $P$ samples, the training complexity is $\mathcal{O}(T \cdot H \cdot s \cdot P + H^3)$, where $s$ is sparsity. The $H^3$ term from the ridge regression matrix inversion is typically negligible compared to the total BPTT cost across epochs.

\begin{table}[htbp]
\centering
\caption{Computational complexity comparison of sequential models. Complexity values are expressed in terms of sequence length $T$, hidden size $D$, reservoir size $H$, sparsity of reservoir connections $s$, number of training samples $P$, number of output classes $C$, and number of training epochs $E$.}
\label{tab:theory_complexity}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Training Complexity} & \textbf{Inference Complexity} \\
\midrule
Vanilla RNN & $\mathcal{O}(T \cdot D^2 \cdot E)$ & $\mathcal{O}(T \cdot D^2)$ \\
LSTM & $\mathcal{O}(4 \cdot T \cdot D^2 \cdot E)$ & $\mathcal{O}(T \cdot D^2)$ \\
Bi-LSTM & $\mathcal{O}(8 \cdot T \cdot D^2 \cdot E)$ & $\mathcal{O}(2 \cdot T \cdot D^2)$ \\
Reservoir Computing & $\mathcal{O}(T \cdot H \cdot s \cdot P + H^3)$ & $\mathcal{O}(T \cdot H \cdot s + H \cdot C)$ \\
Bidirectional RC & $\mathcal{O}(2 \cdot T \cdot H \cdot s \cdot P + (2H)^3)$ & $\mathcal{O}(2 \cdot T \cdot H \cdot s + 2 \cdot H \cdot C)$ \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item \textbf{Notes:} 
\begin{itemize}
    \item $T$ = sequence length; $D$ = hidden state dimension (RNN/LSTM); $H$ = reservoir size (RC); 
    \item $s$ = sparsity of reservoir recurrent connections; $P$ = number of training samples; 
    \item $C$ = number of output classes; $E$ = number of training epochs.
    \item The $H^3$ term in RC corresponds to the matrix inversion in ridge regression for the readout layer.
\end{itemize}
\end{tablenotes}
\end{table}


Bidirectional RC can be implemented by using two separate reservoirs: one for the forward pass and another for the backward pass:

\begin{align}
\begin{bmatrix}
\overrightarrow{\mathbf{r}}(t) \\[2pt]
\overleftarrow{\mathbf{r}}(t)
\end{bmatrix}
&=
\begin{bmatrix}
f(\mathbf{W}_{in}^f \mathbf{x}(t) + \mathbf{W}_{res}^f \overrightarrow{\mathbf{r}}(t-1)) \\[2pt]
f(\mathbf{W}_{in}^b \mathbf{x}(t) + \mathbf{W}_{res}^b \overleftarrow{\mathbf{r}}(t+1))
\end{bmatrix}, 
\qquad
\mathbf{r}(t) = [\overrightarrow{\mathbf{r}}(t); \overleftarrow{\mathbf{r}}(t)].
\end{align}

The total training complexity of bidirectional RC is $\mathcal{O}(2 \cdot T \cdot H \cdot s \cdot P + (2H)^3)$, which remains significantly more efficient than Bi-LSTM. This efficiency motivates the proposed bidirectional RC framework, which combines the low training cost of reservoirs with the enhanced temporal modeling provided by bidirectionality.



\section{Proposed Method}\label{sec:methodology}

The proposed framework, illustrated in Figure~\ref{fig:framework_architecture}, unifies bidirectional reservoir computing with tensor-based dimensionality reduction and non-linear readout learning. This architecture is designed to capture bidirectional temporal dependencies in skeleton sequences while maintaining the computational efficiency inherent to the RC paradigm.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{flowchart.png}
    \caption{Architecture of the proposed bidirectional RC framework.}
    \label{fig:framework_architecture}
\end{figure}

\subsection{Bidirectional Reservoir Architecture}\label{subsec:bidirectional_reservoir}

The framework employs a dual-stream reservoir architecture to capture temporal context from both past and future dynamics. Two parallel reservoirs process the input sequence in opposite temporal directions, generating a latent representation that encodes the complete motion evolution. 
To address the practical challenge of variable sequence lengths common in skeleton-based HAR datasets, we first normalize all sequences to a fixed length $T_{max}$ through temporal interpolation:
\begin{equation}
\mathbf{X}_{norm} = \text{Interpolate}(\mathbf{X}, T_{max})
\label{eq:interpolation}
\end{equation}
where $T_{max}$ is set to the 95th percentile of sequence lengths in the training set to minimize information loss while ensuring computational tractability. 

The bidirectional architecture consists of two specialized reservoir streams, each optimized for processing temporal information in its respective direction. The forward reservoir processes the normalized skeleton sequence $\mathbf{X}_{norm} = [\mathbf{x}(1), \mathbf{x}(2), \ldots, \mathbf{x}(T_{max})]$ in chronological order, accumulating information about the temporal evolution of the action from its beginning toward its completion. Simultaneously, the backward reservoir processes the same sequence in reverse chronological order, capturing information about how the action unfolds when viewed from its completion toward its beginning.

\begin{equation}
\begin{aligned}
\overrightarrow{\mathbf{r}}(t) &= (1-\alpha_f)\overrightarrow{\mathbf{r}}(t-1) + 
\alpha_f \tanh(\mathbf{W}_{in}^f\mathbf{x}(t) + \mathbf{W}_{res}^f\overrightarrow{\mathbf{r}}(t-1) + \mathbf{b}^f), \\[1mm]
\overleftarrow{\mathbf{r}}(t) &= (1-\alpha_b)\overleftarrow{\mathbf{r}}(t+1) + 
\alpha_b \tanh(\mathbf{W}_{in}^b\mathbf{x}(t) + \mathbf{W}_{res}^b\overleftarrow{\mathbf{r}}(t+1) + \mathbf{b}^b).
\end{aligned}
\label{eq:bidirectional_reservoir}
\end{equation}


Here, $\overrightarrow{\mathbf{r}}(t)$ and $\overleftarrow{\mathbf{r}}(t) \in \mathbb{R}^H$ represent the forward and backward reservoir states, $\alpha_f, \alpha_b \in [0,1]$ are the respective leak rates, and $\mathbf{W}_{in}^{f,b} \in \mathbb{R}^{H \times 3N}$, $\mathbf{W}_{res}^{f,b} \in \mathbb{R}^{H \times H}$ are the input and reservoir weight matrices for each stream. 

This dual-stream structure ensures that the preparatory and follow-through phases of an action are captured simultaneously. For example, in a "throwing" action, the forward stream encodes the buildup of motion, while the backward stream captures the deceleration and completion phases. 
The effectiveness of our bidirectional reservoir architecture depends critically on proper parameter initialization and configuration. We employ a principled approach to reservoir design that ensures optimal dynamical properties while maintaining computational efficiency. The input weight matrices are drawn from a uniform distribution with carefully chosen bounds:
\begin{equation}
\mathbf{W}_{in}^{f,b} \sim \mathcal{U}(-\sigma_{in}, \sigma_{in})
\label{eq:input_weights}
\end{equation}
where $\sigma_{in}$ is the input scaling parameter that controls the magnitude of input projections into the reservoir space. This parameter is crucial for ensuring that the reservoir operates in an appropriate dynamical regime: too small values lead to linear dynamics that cannot capture complex temporal patterns, while too large values can drive the reservoir into chaotic regimes difficult to control. 
The reservoir weight matrices are constructed as sparse random matrices with carefully controlled spectral properties:
\begin{equation}
\mathbf{W}_{res}^{f,b} \sim \rho \cdot \frac{\text{SparseRandom}(\gamma, \sigma_{res})}{\rho(\text{SparseRandom}(\gamma, \sigma_{res}))}.
\label{eq:reservoir_init}
\end{equation}

where $\gamma \in [0.01, 0.1]$ is the sparsity level (percentage of non-zero connections), $\sigma_{res}$ controls the magnitude of reservoir connections, and $\rho \in [0.8, 1.2]$ is the desired spectral radius. The spectral radius normalization (Eq.~\ref{eq:reservoir_init}) ensures that the reservoir operates near the edge of stability, maximizing its computational capacity while maintaining the echo state property (ESP).
\textbf{Stability Analysis:} Since the forward and backward reservoirs operate independently without recurrent cross-connections, the global stability of the bidirectional system is guaranteed provided that each individual reservoir satisfies the ESP ($\rho < 1$).


\subsection{Bidirectional State Fusion Strategies}\label{subsec:fusion_strategies}

The integration of forward and backward reservoir states represents a critical design choice that significantly impacts the framework's performance. The challenge lies in effectively combining the complementary temporal perspectives captured by each reservoir stream while maintaining computational efficiency and preserving the most discriminative information for action recognition.
We explore three  integration strategies, each offering different trade-offs between representational capacity, computational efficiency, and recognition performance. The choice of fusion strategy fundamentally determines how the bidirectional temporal information is synthesized into a unified representation suitable for downstream processing.

The concatenation strategy combines the forward and backward states through a simple concatenation at each time step (31). It preserves all information from both temporal directions but doubles the dimensionality of the state representation, creating complete temporal sequences $\mathbf{R}_{concat} = [\mathbf{r}_{concat}(1), \mathbf{r}_{concat}(2), \ldots, \mathbf{r}_{concat}(T_{max})] \in \mathbb{R}^{T_{max} \times 2H}$ for each sample. 
The primary advantage of concatenation lies in its information preservation properties. By maintaining separate representations for forward and backward temporal contexts, this approach allows downstream components to learn optimal combinations of temporal information without imposing any a priori assumptions about the relative importance of different temporal directions. Unlike averaging, which might cancel out opposing dynamics, concatenation preserves the distinct features of both the preparatory and follow-through phases.
This flexibility is particularly valuable for complex actions where discriminative information may be distributed across different temporal phases. 
However, the doubled dimensionality creates computational challenges, particularly for the subsequent dimensionality reduction and classification stages. The increased feature space requires more sophisticated regularization strategies and can lead to higher memory consumption during training and inference.

The weighted combination strategy employs a learnable parameter $\beta \in [0,1]$ to fuse the forward and backward reservoir states at each time step:
\begin{equation}
\mathbf{r}_{weighted}(t) = \beta \, \overrightarrow{\mathbf{r}}(t) + (1-\beta) \, \overleftarrow{\mathbf{r}}(t), \quad
\beta \leftarrow \beta - \eta \frac{\partial \mathcal{L}}{\partial \beta},
\end{equation}
where $\eta$ is the learning rate and $\mathcal{L}$ is the classification loss. 
This strategy maintains the original dimensionality, adaptively balances temporal directions, and allows the system to discover the optimal contribution of forward and backward information. The learned $\beta$ provides interpretable insights: values near $0.5$ indicate balanced importance, while values closer to $0$ or $1$ suggest dominance of backward or forward information.
%\subsubsection{Attention-based Fusion Strategy}

The attention-based fusion strategy dynamically integrates forward and backward reservoir states at each time step, enabling the system to adaptively focus on the most informative temporal direction. Letting $\mathbf{r}_{att}(t)$ denote the fused reservoir state, the attention mechanism can be written as:

\begin{equation}
\mathbf{r}_{att}(t) = 
\text{softmax}\Big(\mathbf{v}_a^T \tanh(\mathbf{W}_a[\overrightarrow{\mathbf{r}}(t); \overleftarrow{\mathbf{r}}(t)] + \mathbf{b}_a)\Big) \odot 
\begin{bmatrix}
\overrightarrow{\mathbf{r}}(t) \\ 
\overleftarrow{\mathbf{r}}(t)
\end{bmatrix} 
\in \mathbb{R}^H,
\end{equation}

where the softmax produces attention weights $\mathbf{a}(t) = [a_1(t), a_2(t)]$ for the forward and backward reservoirs, and $\odot$ denotes element-wise weighting. The fused sequence $\mathbf{R}_{att} \in \mathbb{R}^{T_{max} \times H}$ is then used for downstream temporal modeling. Learnable parameters $\mathbf{W}_a \in \mathbb{R}^{H_a \times 2H}$, $\mathbf{v}_a \in \mathbb{R}^{H_a}$, and $\mathbf{b}_a \in \mathbb{R}^{H_a}$ allow the system to capture complex, time-dependent interactions between forward and backward contexts. The attention weights provide interpretability: high $a_1(t)$ indicates forward context dominance, while high $a_2(t)$ indicates backward context dominance.
%\subsubsection{Fusion Strategy Selection Criteria}

The choice of fusion strategy balances representation richness, efficiency, and interpretability. Concatenation preserves full forward and backward context, weighted combination introduces a single learnable parameter for adaptive yet lightweight temporal balancing, and attention-based fusion highlights the most relevant temporal phases for interpretable reasoning. All approaches remain compatible with downstream dimensionality reduction and classification, maintaining the efficiency of the reservoir computing framework. The strategies will be evaluated experimentally, with the best-performing approach selected.






\subsection{Adaptive Multi-view Dimensionality Reduction}\label{subsec:dimensionality_reduction}

The high-dimensional sequences generated by the bidirectional reservoirs require efficient compression to prevent overfitting and ensure real-time performance. Our two-stage reduction module first applies temporal compression and then utilizes multilinear tensor decomposition.
The bidirectional reservoir produces a complete temporal sequence for each input sample. Depending on the integration strategy, the resulting dimensionality can vary: 
$\mathbf{R}_i \in \mathbb{R}^{T_{max} \times 2H}$ for concatenation, and 
$\mathbf{R}_i \in \mathbb{R}^{T_{max} \times H}$ for weighted or attention-based fusion. In typical configurations, $T_{max} = 300$ and $H = 1000$. These dimensions already indicate a large feature space.
When extended to multiple samples, this results in substantial computational and storage demands. High-dimensional feature spaces often lead to sparse data distributions, making learning more difficult and reducing generalization. The increased number of features also raises computational costs during training and inference. Moreover, such rich representations increase the risk of overfitting, especially with limited training data. Finally, the high-dimensional feature matrices require significant memory, making storage and scalability more challenging.

\subsubsection{Stage 1: Temporal Principal Component Analysis}

We apply PCA along the temporal dimension of each reservoir sequence to compress temporal dynamics.  
For each sample \(i\) compute the temporal covariance
\begin{equation}
\mathbf{C}_i \;=\; \frac{1}{T_{max}-1}\,(\mathbf{R}_i-\boldsymbol{\mu}_i)^\top(\mathbf{R}_i-\boldsymbol{\mu}_i) \in \mathbb{R}^{d\times d},
\label{eq:cov}
\end{equation}
where \(\boldsymbol{\mu}_i\) is the temporal mean and \(d\) is the per-time-step feature dimension (\(H\) or \(2H\)).

Perform eigendecomposition and retain the minimal number of components \(K_i\) that preserve fraction \(\theta_{PCA}\) of the variance:
\begin{equation}
\mathbf{V}_i,\boldsymbol{\Lambda}_i = \mathrm{eig}(\mathbf{C}_i), \qquad
K_i = \min\!\left\{ k : \frac{\sum_{j=1}^{k}\lambda_{i,j}}{\sum_{j=1}^{d}\lambda_{i,j}} \ge \theta_{PCA} \right\},
\label{eq:local_pca}
\end{equation}
with \(\lambda_{i,j}\) sorted descendingly. The per-sample reduced temporal representation is \(\mathbf{R}_{PCA,i}=\mathbf{R}_i\mathbf{V}_i[:,1:K_i]\).

To obtain a consistent projection dimension across samples, compute a global rank and projection from the pooled covariance:
\begin{equation}
K = \mathrm{median}\{K_1,\dots,K_P\}, \quad
\mathbf{C}_{global} = \frac{1}{P}\sum_{i=1}^P \mathbf{C}_i,\quad
\mathbf{V}_{global},\boldsymbol{\Lambda}_{global}=\mathrm{eig}(\mathbf{C}_{global}),
\label{eq:global_pca}
\end{equation}
and form the final, fixed-size temporal embedding
\begin{equation}
\mathbf{R}_{PCA,i} \;=\; \mathbf{R}_i\,\mathbf{V}_{global}[:,1:K] \in \mathbb{R}^{T_{max}\times K}.
\label{eq:final_projection}
\end{equation}

This preserves dominant temporal dynamics while enforcing a uniform dimensionality \(K\) for downstream tensor operations.



\subsubsection{Stage 2: Tucker Tensor Decomposition}

Following temporal PCA, we construct a three-dimensional tensor $\mathcal{R} \in \mathbb{R}^{P \times T_{max} \times K}$ from all PCA-reduced samples and apply Tucker decomposition to capture multilinear relationships across samples, time, and features:
\begin{equation}
\mathcal{R} \approx \mathcal{G} \times_1 \mathbf{U}^{(1)} \times_2 \mathbf{U}^{(2)} \times_3 \mathbf{U}^{(3)},
\end{equation}
Specifically, the self-attention layer is applied to the reduced reservoir states $\mathcal{G} \in \mathbb{R}^{R_1 \times R_2 \times R_3}$, but unlike standard Transformer architectures, these attention weights are trained only within the supervised readout framework. This ensures that the Echo State property of the reservoir is maintained and prevents the high computational cost of backpropagation through time (BPTT). Consequently, the attention mechanism adds minimal learnable parameters—typically less than 2\% of the total model configuration—while significantly improving temporal focus.
where $\mathcal{G} \in \mathbb{R}^{R_1 \times R_2 \times R_3}$ is the core tensor capturing the essential interactions between different modes, and $\mathbf{U}^{(1)} \in \mathbb{R}^{P \times R_1}$, $\mathbf{U}^{(2)} \in \mathbb{R}^{T_{max} \times R_2}$, $\mathbf{U}^{(3)} \in \mathbb{R}^{K \times R_3}$ are the mode-wise factor matrices that encode the principal directions of variation. 

Tucker decomposition provides a powerful framework for simultaneous dimensionality reduction across all tensor modes while preserving the intrinsic multilinear structure of the data. The decomposition is typically performed via Higher-Order Singular Value Decomposition (HOSVD), where each factor matrix is obtained from the singular value decomposition (SVD) of the corresponding mode unfolding:
\begin{equation}
\mathbf{U}^{(i)} = \text{SVD}(\mathcal{R}_{(i)})[:, :R_i], \quad i = 1, 2, 3,
\end{equation}
with $\mathcal{R}_{(i)}$ denoting the mode-$i$ matricization of $\mathcal{R}$. The reduced dimensions $R_1 \ll P$, $R_2 \ll T_{max}$, and $R_3 \ll K$ are adaptively selected to preserve a target proportion of variance in each mode:
\begin{equation}
R_i = \arg\min_{r} \left\{ \frac{\sum_{j=1}^r \sigma_{i,j}^2}{\sum_{j=1}^{d_i} \sigma_{i,j}^2} \geq \theta_i \right\}, \quad \theta_i \in [0.9, 0.99],
\end{equation}
where $\sigma_{i,j}$ are the singular values. The threshold $\theta_i$ was determined empirically via grid search; we observed that preserving 95\% of the spectral energy ($\theta_i=0.95$) achieves the optimal balance between reconstruction fidelity and compression ratio, systematically filtering out high-frequency skeletal noise. 

Finally, each sample's low-dimensional representation is obtained by projection through the learned factor matrices:
\begin{equation}
\mathbf{R}_{final,i} = \mathcal{R}_i \times_2 \mathbf{U}^{(2)} \times_3 \mathbf{U}^{(3)} \in \mathbb{R}^{R_2 \times R_3}.
\end{equation}
This two-stage dimensionality reduction pipeline preserves the most informative spatiotemporal patterns while typically reducing the original dimensionality by over 99\% and retaining more than 95\% of the variance. 



%This two-stage dimensionality reduction approach ensures that we preserve the most important spatiotemporal patterns while achieving substantial dimensionality reduction, typically reducing the original dimensionality by 90-\% while retaining \% of the variance.

\subsection{Enhanced Representation Learning}\label{subsec:representation_learning}

The dimensionality-reduced reservoir states serve as the foundation for enhanced representation learning, designed to capture action dynamics at multiple temporal scales. Human actions inherently exhibit structure across different temporal resolutions, ranging from fine-grained joint movements to coarse-grained action phases. To address this, our multi-scale temporal pooling and attention strategy aggregates discriminative features from the reduced temporal sequences $\mathbf{R}_{final,i} \in \mathbb{R}^{R_2 \times R_3}$.

\paragraph{Global Statistical Pooling.}
We first compute global statistics that summarize the entire temporal sequence:
\begin{equation}
\mathbf{f}_{global} = \tfrac{1}{R_2}\!\sum_{t=1}^{R_2}\!\mathbf{R}_{final,i}(t,:), \quad
\mathbf{f}_{max} = \max_{t}\mathbf{R}_{final,i}(t,:), \quad
\mathbf{f}_{std} = \sqrt{\tfrac{1}{R_2 - 1}\!\sum_{t=1}^{R_2}\!\big(\mathbf{R}_{final,i}(t,:) - \mathbf{f}_{global}\big)^2}.
\label{eq:stat_features}
\end{equation}


Here, $\mathbf{f}_{global}$ captures overall action characteristics invariant to execution speed, $\mathbf{f}_{max}$ emphasizes salient moments (e.g., action peaks), and $\mathbf{f}_{std}$ quantifies temporal variability.

\paragraph{Local Multi-Scale Pattern Extraction.}
To model local temporal dependencies, we apply 1D convolutions with multiple kernel sizes, each followed by global max pooling:
\begin{multline}
\mathbf{f}_{local} = 
\big[ 
\text{GlobalMaxPool}(\text{Conv1D}(\mathbf{R}_{final,i}, k{=}3)); \\ 
\text{GlobalMaxPool}(\text{Conv1D}(\mathbf{R}_{final,i}, k{=}5)); \\ 
\text{GlobalMaxPool}(\text{Conv1D}(\mathbf{R}_{final,i}, k{=}7))
\big].
\end{multline}

This multi-kernel approach captures fine-to-coarse temporal patterns, where smaller kernels detect rapid transitions and larger ones capture broader motion phases.

\paragraph{Temporal Attention Mechanism.}
Recognizing that not all time steps contribute equally to action recognition, we incorporate a learnable temporal attention mechanism:
\begin{equation}
\begin{aligned}
\mathbf{e}(t) &= \mathbf{v}_a^{\top} \tanh(\mathbf{W}_a \mathbf{R}_{final,i}(t, :) + \mathbf{b}_a), \\
\alpha(t) &= \frac{\exp(\mathbf{e}(t))}{\sum_{k=1}^{R_2} \exp(\mathbf{e}(k))}, \\
\mathbf{f}_{attention} &= \sum_{t=1}^{R_2} \alpha(t)\, \mathbf{R}_{final,i}(t, :).
\end{aligned}
\label{eq:attention}
\end{equation}

where $\mathbf{W}_a \in \mathbb{R}^{H_a \times R_3}$, $\mathbf{v}_a \in \mathbb{R}^{H_a}$, and $\mathbf{b}_a \in \mathbb{R}^{H_a}$ are learnable parameters (with $H_a = 64$). This mechanism adaptively highlights the most discriminative temporal segments, enhancing both accuracy and interpretability.

\subsection{Dynamical Systems Perspective and Synergistic Rationale}\label{subsec:dynamical_perspective}

The effectiveness of the proposed framework stems from a principled synergy between bidirectional dynamics and multilinear compression, which we analyze through the lens of dynamical systems theory.

\paragraph{Global Temporal Attractors.}
In standard unidirectional RC, the reservoir state $\mathbf{r}(t)$ is an "echo" of the past input history. By integrating bidirectional processing, we effectively define a \textbf{Global Temporal Attractor} $\mathcal{A} = \{\overrightarrow{\mathbf{r}}(t) \oplus \overleftarrow{\mathbf{r}}(t)\}_{t=1}^T$. This combined manifold ensures that the representation at any time $t$ is topologically constrained by both the causal (past) and anti-causal (future) trajectories of the skeletal sequence. Formally, this increases the \textit{Memory Capacity} (MC) of the system, as the bidirectional mixing provides a "look-ahead" capability that resolves temporal ambiguities (e.g., distinguishing "sitting down" from "standing up" at the midpoint of the action) without requiring expensive gradient propagation.

\paragraph{Multilinear Rank Preservation.}
A critical challenge in reservoir computing for HAR is the high-dimensionality of the reservoir manifold, which often leads to overfitting in the readout. While standard PCA identifies the directions of maximum variance, it treats the reservoir states as flattened vectors, ignoring the inherent factorizable structure of the joint-wise and time-wise correlations. 
Our use of \textbf{Tucker decomposition} serves as a multilinear rank-reduction operator that preserves the \textit{mode-specific} geometry of the reservoir output. By maintaining a Core Tensor $\mathcal{G}$, we ensure that the "Readout" only operates on the discriminative subspace of the attractor. This can be quantified via the \textbf{Participation Ratio} (PR) of the reservoir eigenvalues:
\begin{equation}
PR = \frac{(\sum \lambda_i)^2}{\sum \lambda_i^2},
\end{equation}
where a higher PR indicates a more expressive, high-dimensional representation. The Tucker module adaptively prunes the "noisy" dimensions while maximizing the PR of the action-relevant components, providing a structured denoising mechanism that is mathematically superior to linear filtering.

\paragraph{Design Rationale Summary.}
The synergy established between these modules ensures that the \textbf{Bidirectional RC} provides the foundational temporal memory with guaranteed stability via the Echo State Property, while \textbf{Tucker Dimensionality Reduction} acts as a principled multilinear compression engine. Finally, the \textbf{Advanced Readout} projects these structured manifolds into a discriminative class-probability space. This combination allows the framework to achieve the discriminative power of deep architectures while maintaining the $\mathcal{O}(1)$ training efficiency of reservoir computing.


The readout stage replaces the traditional linear mapping with a structured Multi-Layer Perceptron (MLP) to decode the non-linear manifolds produced by the reservoir. The architecture utilizes BatchNorm and structured dropout ($p=0.3$) for stabilization, followed by two complementary activation layers:
\begin{equation}
\begin{aligned}
\mathbf{z} &= \text{Maxout}(\text{BatchNorm}(\mathbf{W}_1 \mathbf{f}_{final} + \mathbf{b}_1)), \\
\mathbf{y} &= \text{softmax}(\mathbf{W}_2 \mathbf{KAF}(\mathbf{z}) + \mathbf{b}_2).
\end{aligned}
\label{eq:mlp_simplified}
\end{equation}
This design enables adaptive feature partitioning via Maxout and smooth interpolation via Kernel Activation Functions (KAF) with minimal parameter overhead.




%\paragraph{Maxout for Piecewise Linear Partitioning.}
The first nonlinear stage employs the Maxout activation, which partitions the feature space into multiple locally linear regions:
\begin{equation}
\text{Maxout}(\mathbf{x}) = \max_{i \in [1,k]} (\mathbf{W}_i^{maxout} \mathbf{x} + \mathbf{b}_i^{maxout}),
\end{equation}
where $k=5$ determines the number of partitions. This choice is theoretically grounded: Maxout units can approximate any convex function given sufficient partitions. In our fixed-reservoir framework, where the temporal mixing is frozen, this capability allows the readout layer to learn complex, non-linear decision boundaries necessary to disentangle the reservoir's high-dimensional projection, effectively compensating for the lack of recurrent weight training.
%\paragraph{Kernel Activation Function for Adaptive Nonlinearity.}
The second nonlinear transformation introduces data-driven flexibility through the Kernel Activation Function (KAF), which learns smooth nonlinear mappings shaped by the underlying feature distribution:
\begin{equation}
\text{KAF}(\mathbf{x}) = \sum_{i=1}^{D} \alpha_i \, \phi(\|\mathbf{x} - \mathbf{c}_i\|),
\end{equation}
where $\alpha_i$ are learnable coefficients, $\mathbf{c}_i$ are kernel centers initialized via k-means, $D = 20$ denotes the number of kernels, and $\phi(z) = \exp(-\gamma z^2)$ is a Gaussian kernel with learnable bandwidth $\gamma$. By integrating KAF into the readout, the model gains the capacity to adjust activation curvature dynamically, yielding improved discrimination between temporally similar actions.

%\paragraph{Regularization and Spectral Stabilization.}
The training objective of the readout combines standard cross-entropy with spectral and kernel regularization to ensure both generalization and numerical stability:
\begin{equation}
\mathcal{L}_{total} = \mathcal{L}_{CE} 
+ \lambda_1 \sum_{i=1}^{3} \|\mathbf{W}_i\|_F^2 
+ \lambda_2 \sum_{i=1}^{D} \|\alpha_i\|^2,
\end{equation}
where $\lambda_1=0.001$ and $\lambda_2=0.0001$. Spectral normalization is applied to all weight matrices to bound the Lipschitz constant:
\begin{equation}
\mathbf{W}_{SN} = \frac{\mathbf{W}}{\sigma(\mathbf{W})},
\end{equation}
with $\sigma(\mathbf{W})$ denoting the leading singular value estimated via power iteration. This spectral constraint preserves the dynamical balance of the reservoir-to-readout interface, ensuring stable gradient flow and consistent classification under varying input scales.
\subsection{Training Algorithm}\label{subsec:training_algorithm}
Algorithm~\ref{alg:training} outlines the end-to-end training pipeline of the proposed Bidirectional Reservoir Computing (BRC) framework. It integrates forward and backward reservoir dynamics, flexible fusion strategies, tensor-based dimensionality reduction, and advanced readout learning. To regularize the fusion parameters, we define a fusion-specific term $\mathcal{R}_{fusion}(\mathcal{F})$, combining attention entropy where needed:

\begin{equation}
\mathcal{R}_{fusion}(\mathcal{F}) =
\begin{cases}
0, & \mathcal{F} = \text{concat}, \\[1mm]
\mu_{\beta} \|\beta - 0.5\|_2^2, & \mathcal{F} = \text{weighted}, \\[1mm]
\mu_a \frac{1}{P}\sum_{i=1}^{P}\sum_{t=1}^{T_{max}} \Big[-\sum_{j=1}^{2} a_{i,j}(t) \log a_{i,j}(t)\Big] + \mu_W \|\mathbf{W}_a\|_F^2, & \mathcal{F} = \text{attention},
\end{cases}
\label{eq:rfusion_total}
\end{equation}

with recommended default hyperparameters: 
\begin{equation}
\mu_\beta = 0.01, \quad \mu_a = 0.001, \quad \mu_W = 10^{-4}, \quad \lambda_3 = 0.01.
\end{equation}


The fusion regularization is incorporated into the total loss:
\begin{equation}
\mathcal{L} = \mathcal{L}_{CE} + \lambda_1 \sum \|\mathbf{W}_i\|_F^2 + \lambda_2 \sum \|\alpha_i\|^2 + \lambda_3 \mathcal{R}_{fusion}(\mathcal{F}).
\label{eq:total_loss}
\end{equation}


\begin{algorithm}[htbp]
\caption{Bidirectional Reservoir Computing Training with Configurable Fusion}
\label{alg:training}
\begin{algorithmic}[1]
\Require Training data $\{\mathbf{X}_i, y_i\}_{i=1}^P$, reservoir size $H$, max sequence length $T_{max}$, regularization $\lambda_1,\lambda_2,\lambda_3$, fusion strategy $\mathcal{F}\in\{\text{concat},\text{weighted},\text{attention}\}$
\Ensure Trained parameters $\Theta$ (readout weights, fusion params if learnable, projection factors)

\State Initialize reservoirs $\mathbf{W}_{res}^{f,b}, \mathbf{W}_{in}^{f,b}$ (random, spectral radius $\rho$)
\If{$\mathcal{F}=\text{weighted}$} \State initialize learnable $\beta$ (default $0.5$) \EndIf
\If{$\mathcal{F}=\text{attention}$} \State initialize $\mathbf{W}_a, \mathbf{v}_a, \mathbf{b}_a$ \EndIf
\State Initialize readout MLP parameters and optimizer

\For{each sample $i=1,\dots,P$}
  \State $\mathbf{X}_{norm} \gets \mathrm{Interpolate}(\mathbf{X}_i, T_{max})$
  \State reset $\overrightarrow{\mathbf{r}}(0)=\mathbf{0}$, $\overleftarrow{\mathbf{r}}(T_{max}+1)=\mathbf{0}$
  \For{$t=1\dots T_{max}$} compute $\overrightarrow{\mathbf{r}}(t)$  \EndFor
  \For{$t=T_{max}\dots 1$} compute $\overleftarrow{\mathbf{r}}(t)$  \EndFor
  \For{$t=1\dots T_{max}$} 
    \State $\mathbf{r}_{fused}(t) \gets$ fused state (concat / weighted / attention, see Eq.~\ref{eq:rfusion_total})
    \State store $\mathcal{R}_{raw}[i,t,:] \gets \mathbf{r}_{fused}(t)$
  \EndFor
\EndFor

\State \# Temporal compression / alignment
\State apply per-sample PCA or global projection to produce $\mathcal{R} \in \mathbb{R}^{P\times T_{max}\times K}$
\State Tucker decomposition / factorization to form $\mathbf{R}_{final,i}$

\For{epoch = 1..N}
  \For{mini-batch $B$}
    \State compute $\mathbf{f}_{final,i}$ for $i \in B$
    \State compute predictions $\hat y_i$ via readout MLP
    \State compute loss: 
    \[
      \mathcal{L} = \frac{1}{|B|}\sum_{i\in B} \mathcal{L}_{CE}(\hat y_i,y_i)
      + \lambda_1 \sum \|\mathbf{W}_j\|_F^2
      + \lambda_2 \sum \|\alpha\|^2
      + \lambda_3\,\mathcal{R}_{fusion}(\mathcal{F})
    \]
    \State update readout and fusion parameters by gradient step
    \State apply spectral normalization and dropout
  \EndFor
\EndFor

\State \Return $\Theta = \{\text{readout weights}, \text{fusion params (if learned)}, \text{projection factors}\}$
\end{algorithmic}
\end{algorithm}



%This training process ensures coherent temporal modeling by combining bidirectional dynamics, adaptive fusion, and tensor-based dimensionality reduction. The integration of attention and multi-scale pooling enhances feature robustness, while the readout layer leverages advanced activation and normalization strategies for stable and efficient learning.

\section{Experimental Validation}\label{sec:experiments}

Our experimental investigation unfolds across multiple dimensions: we begin with detailed dataset analysis and visualization to understand the characteristics of human actions in our evaluation scenarios, proceed through systematic performance comparisons with state-of-the-art methods, conduct thorough ablation studies to understand the contribution of each component, and conclude with practical considerations including computational efficiency and real-time performance analysis.  

\subsection{Datasets}\label{subsec:experimental_design}

Our evaluation is conducted on five benchmark datasets that capture the diversity and complexity of skeleton-based human action recognition tasks. These datasets—ranging from small-scale laboratory recordings to massive, in-the-wild collections—differ in action categories, sensing modalities, and recording conditions, providing a comprehensive basis for assessing recognition performance.  

The \textbf{UTD Multimodal Human Action Dataset (UTD-MHAD)} \cite{chen2015utd} contains 27 actions performed by 8 subjects (4 male, 4 female), each repeated four times, yielding 861 sequences. Skeleton data is captured with a Kinect v2 sensor (25 joints in 3D). Actions range from simple gestures (e.g., wave, swipe) to complex sports movements (e.g., tennis serve, basketball shoot) and daily activities (e.g., sit down, walk). Although our framework focuses on skeleton data, UTD-MHAD also provides synchronized RGB, depth, and inertial data, enabling future multi-modal comparisons.  
The \textbf{MSR Action3D dataset} \cite{li2010action} comprises 567 sequences of 20 actions performed by 10 subjects, captured with the original Kinect (20 joints). Actions are grouped into three subsets: AS1 (horizontal arm movements), AS2 (vertical arm movements), and AS3 (complex multi-body movements). This organization highlights increasing levels of difficulty, from easily distinguishable spatial gestures (AS1) to challenging coordinated movements (AS3), making it a strong benchmark for testing temporal modeling capabilities.  
The \textbf{CZU Multimodal Human Action Dataset (CZU-MHAD)} \cite{chao2022czu} consists of 22 actions performed by 7 subjects, totaling 880 sequences. It integrates depth video, skeletal data from Kinect (25 joints), and inertial signals from wearable sensors. While we evaluate the skeletal modality, the dataset's multi-modal design reflects real-world action recognition scenarios and offers opportunities for extended multi-sensor studies.  

The \textbf{NTU RGB+D 60 Dataset} \cite{shahroudy2016ntu} is a large-scale benchmark containing 56,880 video samples of 60 action classes performed by 40 subjects. It provides skeletal data with 25 joints. We follow the standard Cross-Subject (X-Sub) and Cross-View (X-View) evaluation protocols. The \textbf{NTU RGB+D 120 Dataset} \cite{liu2019ntu} extends NTU 60 with 120 action classes and 114,480 samples from 106 subjects, using Cross-Subject (X-Sub) and Cross-Set (X-Set) protocols for rigorous evaluation. These large-scale datasets test the scalability and robustness of our bidirectional RC framework beyond smaller benchmarks. Additionally, we utilize the \textbf{Kinetics-Skeleton} dataset, a large-scale collection of YouTube videos. Skeletal data for this dataset was extracted using the standard \textbf{OpenPose} toolbox with 18 keypoints, providing a challengingly noisy "in-the-wild" evaluation environment.




\subsection{Experimental Setup}\label{subsec:experimental_setup}

Experiments were conducted on a workstation equipped with an NVIDIA GeForce RTX 3050 GPU (8GB VRAM), an Intel Core i7-13650HX processor (16 cores, 2.3GHz), 16GB DDR5-4800MHz RAM, and a 1TB NVMe SSD. The software environment included Python 3.12.0, TensorFlow 2.15.0, NumPy 1.24.3, Scikit-learn 1.3.0, and CUDA 12.2. This setup provides sufficient computational capacity while remaining accessible to researchers with comparable resources.  

We adopted standard cross-subject evaluation protocols \cite{wang2012mining} for all datasets. For UTD-MHAD, subjects 1, 3, 5, 7 were used for training (432 sequences) and 2, 4, 6, 8 for testing (429 sequences). For MSR Action3D, subjects 1, 3, 5, 7, 9 were used for training (285 sequences) and 2, 4, 6, 8, 10 for testing (282 sequences). For CZU-MHAD, subjects 1, 3, 5, 7 were used for training (504 sequences) and 2, 4, 6 for testing (376 sequences). For NTU RGB+D 60, the Cross-Subject protocol uses 20 subjects for training and 20 for testing, while Cross-View uses camera 1 for testing and cameras 2 and 3 for training. For NTU RGB+D 120, Cross-Subject uses 53 subjects for training and 53 for testing, and Cross-Set uses even setup IDs for training and odd ones for testing. These splits ensure generalization across different individuals and provide balanced coverage of action classes.  
Hyperparameters were optimized through grid search and validation. The reservoir architecture used size $H=1000$, connection sparsity $\gamma=0.05$, and spectral radius $\rho=0.95$. Leak rates were set to $\alpha_f=\alpha_b=0.3$, with input scaling $\sigma_{\text{in}}=0.5$. Regularization employed L2 penalty $\lambda=0.001$  and dropout $p_{\text{drop}}=0.1$. Dimensionality reduction thresholds were $\theta_i=0.95$ and $\theta_{\text{PCA}}=0.95$ for intermediate and final projections, respectively.  
This configuration yielded the best trade-off between accuracy and computational efficiency across all three datasets.  


\subsection{Fusion Strategy Selection and Analysis}\label{subsec:fusion_strategy_analysis}  

We empirically compared the three fusion strategies; concatenation, weighted, and attention-based; across all benchmark datasets to identify the optimal method for combining forward and backward reservoir states. Using identical reservoir configurations ($H = 1000$, $\rho = 0.95$, $\gamma = 0.05$) and training protocols, the weighted fusion strategy optimized the parameter $\beta$ through gradient-based learning during training, while the attention-based model utilized $H_a = 64$ attention units initialized with the Xavier scheme \cite{glorot2010understanding}.
Table~\ref{tab:fusion_comparison} summarizes the results, showing that concatenation achieves the highest recognition accuracy, especially on complex action sequences, by preserving full bidirectional information. This gain comes at the cost of increased memory usage. Weighted fusion achieved the best efficiency, with \textbf{22.8\%} faster training and \textbf{24.1\%} lower memory usage compared to concatenation, while maintaining competitive accuracy. The learned parameter value of $\beta \approx 0.6$ indicates a slightly higher contribution from forward dynamics. The attention-based strategy represents a balance between the two, yielding accuracy close to concatenation with interpretable temporal emphasis and moderate computational needs.  
Paired t-tests \cite{student1908probable} across 30 runs confirm that concatenation significantly outperforms weighted (p $<$ 0.01) and attention-based fusion (p $<$ 0.05), while the latter two show no significant difference (p $>$ 0.1). Under noise perturbations (Section~\ref{subsec:robustness_validation}), concatenation remains the most accurate, whereas weighted fusion exhibits the most stable degradation due to its inherent regularization. Figure~\ref{fig:fusion_analysis} illustrates the trade-offs between accuracy, computational cost, attention behavior, and scaling characteristics.  

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{fusion_strategy_analysis.png}
    \caption{Fusion strategy analysis: (a) Accuracy vs. computational cost trade-offs, (b) Learned attention patterns for different action classes, (c) Weighted fusion parameter sensitivity analysis, (d) Memory scaling behavior across different reservoir sizes.}
    \label{fig:fusion_analysis}
\end{figure}
Based on the empirical analysis, the concatenation strategy is adopted as the primary fusion approach, as it consistently yields the highest recognition accuracy with statistically significant gains, preserves complete bidirectional temporal information, and maintains stable performance under noise and hyperparameter variations. Although it requires more memory, its computational cost scales linearly with reservoir size. For resource-limited scenarios, weighted fusion offers a good trade-off, while attention-based fusion remains the most interpretable. Unless stated otherwise, all subsequent experiments use the concatenation strategy.

\begin{table}[htbp]
\centering
\caption{Comparison of bidirectional fusion strategies across benchmark datasets (UTD-MHAD, MSR Action3D, and CZU-MHAD), evaluated in terms of classification accuracy (Acc, %), training time (Time, minutes), and memory usage (Mem, GB). 
\textbf{Bold values} denote the best performance within each dataset and metric. 
The lower section summarizes trade-offs: 
Concatenation yields the highest accuracy but requires more computational resources; 
Weighted fusion achieves the best efficiency, reducing training time by 20.8--23.8\% (averaging 21\%) compared to concatenation with only minor accuracy degradation (average 0.71\%); 
Attention-based fusion offers a balanced compromise between accuracy, efficiency, and interpretability.}
\label{tab:fusion_comparison}
\footnotesize
\begin{tabular}{l c c c c c c c c c}
\toprule
\multirow{2}{*}{\textbf{Fusion Strategy}} 
& \multicolumn{3}{c}{\textbf{UTD-MHAD}} 
& \multicolumn{3}{c}{\textbf{MSR Action3D}} 
& \multicolumn{3}{c}{\textbf{CZU-MHAD}} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}
& \textbf{Acc} & \textbf{Time} & \textbf{Mem} 
& \textbf{Acc} & \textbf{Time} & \textbf{Mem} 
& \textbf{Acc} & \textbf{Time} & \textbf{Mem} \\
\midrule
Concatenation       & \textbf{98.91} & 2.7 & 3.2 & \textbf{97.50} & 2.1 & 2.8 & \textbf{98.50} & 2.4 & 3.0 \\
Weighted ($\beta=0.6$) & 98.23 & \textbf{2.1} & \textbf{1.8} & 96.81 & \textbf{1.6} & \textbf{1.5} & 97.73 & \textbf{1.9} & \textbf{1.7} \\
Attention-based     & 98.67 & 3.4 & 2.1 & 97.12 & 2.8 & 1.9 & 98.21 & 3.1 & 2.0 \\
\midrule
\textbf{Average Performance} 
& \multicolumn{3}{c}{\textbf{Acc / Time / Mem}} 
& \multicolumn{6}{c}{\textbf{Trade-off Summary}} \\
\midrule
Concatenation & \multicolumn{3}{c}{98.30 / 2.4 / 3.0} 
& \multicolumn{6}{c}{\textbf{Highest Accuracy}, but \textbf{High Resource Usage}} \\
Weighted      & \multicolumn{3}{c}{97.59 / 1.9 / 1.7} 
& \multicolumn{6}{c}{\textbf{Most Efficient}, Minimal Accuracy Loss} \\
Attention     & \multicolumn{3}{c}{98.00 / 3.1 / 2.0} 
& \multicolumn{6}{c}{\textbf{Balanced Trade-off}, Best Interpretability} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Comprehensive Comparative Evaluation and Performance Analysis}\label{subsec:comparative_evaluation}

The comparison presented in this section encompasses multiple categories of methods. 
Traditional RNN-based methods include Vanilla RNN \cite{elman1990finding} with 1000 hidden units representing the foundational approach to sequential modeling, LSTM \cite{hochreiter1997long} with 1000 memory cells showcasing the benefits of gated architectures, GRU \cite{cho2014learning} with 1000 hidden units providing a simplified gating alternative, bidirectional LSTM \cite{schuster1997bidirectional} with 500 units per direction demonstrating the value of bidirectional processing, and bidirectional GRU with 500 units per direction offering computational efficiency with bidirectional capabilities. 
Advanced graph-based methods include ST-GCN \cite{yan2018spatial} representing the foundation of graph-based skeleton modeling, 2s-AGCN \cite{shi2019two} showcasing multi-stream graph processing, MS-G3D \cite{liu2020disentangling} demonstrating multi-scale graph convolution, and CTR-GCN \cite{chen2021channel} representing the current state-of-the-art in graph-based approaches. 
Reservoir computing variants include Standard ESN \cite{jaeger2001echo} with 1000 reservoir units providing the baseline RC performance, bidirectional ESN representing the core temporal modeling component of our architecture, and deep ESN \cite{gallicchio2017deep} with 3 layers exploring hierarchical reservoir architectures. 
Table~\ref{tab:small_benchmarks} and Table~\ref{tab:large_scale_benchmarks} show the performance comparison across all datasets, revealing the superior performance of the proposed framework. 
The results reveal several compelling insights about the performance landscape of skeleton-based HAR. Our framework achieves substantial improvements over all RNN-based approaches, with particularly significant gains over bidirectional LSTM (3.0\% on UTD-MHAD, 4.5\% on MSR Action3D, and 4.0\% on CZU-MHAD). These improvements demonstrate that the RC paradigm can achieve superior temporal modeling while avoiding the computational complexity and training challenges associated with gradient-based recurrent networks.
Even compared to highly optimized graph-based approaches like CTR-GCN, our framework remains highly competitive. While CTR-GCN maintains a slight accuracy advantage on certain small-scale benchmarks (+0.2\% on MSR Action3D and +0.4\% on Northwestern-UCLA), our approach achieves identical performance on CZU-MHAD (95.7\%) while requiring orders of magnitude fewer resources and no backpropagation through time. This trade-off demonstrates that effective temporal modeling through bidirectional RC can provide comparative accuracy while exceeding the spatial modeling capabilities of graph-based approaches in terms of efficiency-to-performance ratio.

\noindent\textbf{Theoretical Insight: Bi-RC vs. Bi-LSTM.} The marked efficiency advantage of Bi-RC over Bi-LSTM stems from differing memory mechanisms. Bi-LSTMs rely on gating mechanisms trained via Backpropagation Through Time (BPTT) to mitigate vanishing gradients, a process that is computationally expensive ($\mathcal{O}(T)$ steps with deep gradient chains). In contrast, Bi-RC utilizes a fixed dynamical system with the "fading memory" property. The bidirectional mixing effectively "refreshes" the context from both past and future directions without requiring gradient propagation, allowing it to capture long-term dependencies at a fraction of the cost ($\mathcal{O}(1)$ training relative to sequence steps).
Beyond the small-scale datasets, we evaluate our framework on large-scale benchmarks including NTU RGB+D 60/120, Kinetics-Skeleton, and Northwestern-UCLA. For NTU60, our method achieves 93.2\% (X-Sub) and 97.4\% (X-View), surpassing several recent graph-based models such as MS-AAGCN \cite{shi2019skeleton} and RA-GCN \cite{zhu2021recurrent}. The strong performance on the X-View protocol (97.4\%) highlights the view-invariance induced by our bidirectional temporal modeling. On the more challenging NTU120 dataset, the framework achieves 92.8\% (X-Sub) and 91.8\% (X-Set), demonstrating excellent scalability with a realistic 1--2\% improvement over established baselines. 

Furthermore, on the large-scale real-world Kinetics-Skeleton dataset, our model attains a Top-1 accuracy of 38.7\%, reflecting the robustness to noise observed in our earlier ablation studies. Finally, on the multi-view Northwestern-UCLA benchmark, our approach reaches 96.1\% accuracy, consistent with the high precision achieved on smaller multi-modal datasets. These results, summarized in Table~\ref{tab:small_benchmarks} and Table~\ref{tab:large_scale_benchmarks}, confirm that the bidirectional RC paradigm is both efficient and highly competitive across diverse scales and protocols of human action recognition.

It is important to analyze the performance gap observed between the small-scale, lab-controlled datasets (UTD-MHAD, CZU-MHAD) and the large-scale benchmarks (NTU-RGBD, Kinetics-Skeleton). The drop in relative accuracy on Kinetics-Skeleton (38.7\%) compared to laboratory benchmarks is primarily attributed to the high environmental diversity, significant skeletal noise, and the "in-the-wild" nature of the data compared to the structured laboratory settings of smaller datasets. Similarly, the slight drop in NTU-RGBD performance reflects the increased complexity of handling 60 to 120 action classes, diverse camera viewpoints, and varying subject morphologies. Despite these challenges, our model maintains a consistent 1.5--2.0\% edge over conventional bidirectional recurrent baselines, validating its scalability.
    
The comparative analysis of HAR techniques, as shown in Table~\ref{tab:small_benchmarks} for small-to-mid scale datasets and Table~\ref{tab:large_scale_benchmarks} for large-scale benchmarks, highlights significant differences in performance across various methods and datasets. Specifically, we compare against established architectures, demonstrating that our bidirectional reservoir framework achieves competitive accuracy while maintaining significant computational efficiency. We conduct statistical significance testing using McNemar's test \cite{mcnemar1947note} for paired accuracy comparisons, with Bonferroni correction \cite{bonferroni1936teoria} for multiple comparisons.

\begin{table}[!htbp]
\centering
\footnotesize
\caption{Comparison of recognition accuracy on small-to-mid scale benchmark datasets (UTD-MHAD, MSR Action3D, CZU-MHAD, and Northwestern-UCLA). \textbf{Bold} values denote best performance.}
\label{tab:small_benchmarks}
\begin{tabular}{l c c c c c}
\toprule
\textbf{Ref.} & \textbf{Method} & \textbf{UTD} & \textbf{MSR} & \textbf{CZU} & \textbf{N-UCLA} \\
\midrule
\cite{du2015hierarchical} & Hierarchical RNN & 79.04 & 88.89 & 96.30 & - \\
\cite{yan2018spatial} & ST-GCN & 85.5 & 93.2 & - & 94.1 \\
\cite{shi2019two} & 2s-AGCN (2-stream) & - & - & - & 95.1 \\
\cite{cheng2020skeleton} & Shift-GCN & - & - & - & - \\
\cite{chen2021channel} & CTR-GCN & - & 94.8 & 95.7 & 96.5 \\
\cite{chi2022infogcn} & InfoGCN (Single) & - & - & - & 97.0 \\
\midrule
\textbf{Proposed} & RC+Readout & \textbf{$98.9 \pm 0.1$} & \textbf{$94.6 \pm 0.2$} & \textbf{$95.7 \pm 0.2$} & \textbf{$96.1 \pm 0.2$} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!htbp]
\centering
\footnotesize
\caption{Comparative evaluation on large-scale benchmarks (NTU RGB+D 60, NTU RGB+D 120, and Kinetics-Skeleton). Results for NTU are reported for Cross-Subject (X-S), Cross-View (X-V), and Cross-Set (X-Set) protocols as applicable.}
\label{tab:large_scale_benchmarks}
\begin{tabular}{l c c c c c}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{NTU-60}} & \multicolumn{2}{c}{\textbf{NTU-120}} & \textbf{Kin.} \\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-6}
& \textbf{X-S} & \textbf{X-V} & \textbf{X-S} & \textbf{X-Set} & \textbf{T1} \\
\midrule
\cite{shahroudy2016ntu} Part-aware LSTM & 62.9 & 70.3 & - & - & - \\
\cite{yan2018spatial} ST-GCN & 81.5 & 88.3 & 73.2 & 74.5 & 30.7 \\
\cite{shi2019two} 2s-AGCN & 88.5 & 95.1 & 82.3 & 83.1 & 35.1 \\
\cite{cheng2020skeleton} Shift-GCN & 90.7 & 96.5 & 85.9 & 87.6 & - \\
\cite{chen2021channel} CTR-GCN & 92.4 & 96.8 & 88.9 & 90.6 & 37.6 \\
\cite{chen2025twostream} SA-TDGFormer & - & - & - & - & \textbf{39.0} \\
\cite{shi2020decoupled} DSTA-Net & 91.5 & 96.4 & 86.6 & 89.0 & - \\
\cite{plizzari2021spatial} ST-TR & 89.9 & 96.1 & 82.7 & 84.9 & - \\
\cite{chi2022infogcn} InfoGCN & 93.0 & - & 89.8 & - & - \\
\cite{song2023efficientgcn} EfficientGCN & 91.7 & 95.7 & 88.7 & 88.9 & - \\
\cite{duan2022poseconv3d} PoseConv3D & 93.1 & 97.7 & 90.3 & 91.7 & 47.7 \\
\cite{zhang2024blockgcn} BlockGCN & 93.1 & 97.0 & - & - & - \\
\cite{hong2024skateformer} SkateFormer & 91.9 & - & 93.5 & 97.8 & - \\
\midrule
\textbf{Proposed (RC+Readout)} & \textbf{$93.2 \pm 0.2$} & \textbf{$97.4 \pm 0.1$} & \textbf{$92.8 \pm 0.3$} & \textbf{$91.8 \pm 0.2$} & \textbf{$38.7 \pm 0.5$} \\
\bottomrule
\end{tabular}
\end{table}
Figure~\ref{fig:accuracy_comparison} presents the classification accuracies of various methods on three prominent skeleton datasets: UTD-MHAD, MSR Action 3D, and CZU-MHAD. The proposed approach outperforms all other ones across all three datasets, achieving the highest accuracy on the three compared datasets. Figure~\ref{fig:mean_ranking_barchart} presents the mean ranking of various methods over 50 runs based on classification accuracy across the same datasets.
This consistent superiority suggests that the incorporation of our proposed reservoir model space, bidirectional reservoir, dimensionality reduction module, and advanced MLP readout significantly enhances the model's ability to generalize across different datasets. The reduced variance in accuracy (depicted by smaller error bars) further underscores the robustness of our proposed method in handling diverse action recognition scenarios.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{har_accuracy_comparison.png}
    \caption{Classification accuracy comparison across different datasets and baseline methods. Error bars represent the standard deviation for each method, best accuracies are reported.}
    \label{fig:accuracy_comparison}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{mean_ranking_barchart.png}
    \caption{Mean Ranking of Methods Over 50 Runs}
    \label{fig:mean_ranking_barchart}
\end{figure}

\subsection{Ablation Study and Component Analysis}\label{subsec:ablation_studies}

Understanding the contribution of each component in our framework requires a systematic ablation study that isolates the impact of individual innovations.

\subsubsection{Progressive Component Analysis}

Table~\ref{tab:unified_ablation} presents a unified ablation study conducted on the UTD-MHAD dataset. To ensure the reliability of our results and reproducibility, all experiments were conducted using 5 fixed random seeds (42, 123, 2024, 777, 999), and we report the mean accuracy with 95\% confidence intervals. This consolidated analysis confirms that each component of the synergistic integration provides a statistically significant improvement ($p < 0.01$) to the overall recognition accuracy.

\begin{table}[!ht]
\centering
\footnotesize
\caption{Cross-dataset ablation study isolating the impact of key components on accuracy (\%). Improvements on the large-scale NTU-60 benchmark demonstrate the scalability of the proposed synergistic mechanisms.}
\label{tab:unified_ablation}
\begin{tabular}{l c c c c}
\toprule
\textbf{Configuration} & \textbf{UTD-MHAD} & \textbf{NTU-60 (X-S)} & \textbf{$\Delta$ (NTU)} & \textbf{Gain Source} \\
\midrule
Unidirectional ESN Baseline      & $88.4 \pm 0.4$ & $84.1 \pm 0.6$ & -     & - \\
+ Bidirectional Processing       & $91.6 \pm 0.3$ & $88.5 \pm 0.4$ & +4.4  & Temporal Context \\
+ Tucker Decomposition           & $95.6 \pm 0.2$ & $90.8 \pm 0.3$ & +2.3  & Correlation Mix \\
+ Multi-scale Pooling            & $97.1 \pm 0.2$ & $91.5 \pm 0.3$ & +0.7  & Scale Invariance \\
+ Maxout Readout (\textbf{Full}) & \textbf{$98.9 \pm 0.1$} & \textbf{$93.2 \pm 0.2$} & \textbf{+1.7} & Non-linear Mapping \\
\bottomrule
\end{tabular}
\end{table}

The cross-dataset progression in Table~\ref{tab:unified_ablation} demonstrates the scaling behavior of our synergistic mechanisms. Bidirectional processing provides the largest single boost (+4.4\% on NTU60), confirming that context from both directions is critical for complex action sequences. The multilinear correlation preservation of Tucker decomposition further adds +2.3\%, while the Maxout Readout consistently provides a non-trivial improvement (+1.7\%) by efficiently mapping the compressed reservoir states to action classes.


\subsubsection{Hyperparameter Sensitivity Analysis}

Understanding the sensitivity of our framework to hyperparameter choices provides crucial insights for practical deployment. We conduct comprehensive sensitivity analysis for key parameters across the component progression.
The reservoir parameters show good stability: reservoir size (H = 500-1500) maintains performance within 1.5\% of optimal, spectral radius ($\rho$ = 0.85-1.05) shows optimal performance around $\rho$ = 0.95 with graceful degradation outside this range, and sparsity level ($\gamma$ = 0.02-0.08) demonstrates robust performance with optimal efficiency around $\gamma$ = 0.05.
The dimensionality reduction parameters show adaptive behavior: PCA variance threshold (0.90-0.98) maintains performance with 0.95 providing optimal balance, and Tucker decomposition ranks adapt automatically based on data characteristics while maintaining 95\% variance preservation.
The representation learning parameters exhibit stable behavior: multi-scale pooling kernel sizes (3, 5, 7) show consistent performance across different combinations \cite{he2015spatial}, and temporal attention dimensions (32-128) demonstrate stable performance with optimal efficiency around 64 dimensions.
Figure~\ref{fig:hyperparameter_analysis} presents comprehensive sensitivity analysis for key hyperparameters.
\begin{figure}[htbp]
    \centering
    \begin{adjustbox}{center,max width=\textwidth}
        \includegraphics[width=1\textwidth]{hyperparameter_sensitivity_analysis.png}
    \end{adjustbox}
    \caption{Hyperparameter sensitivity analysis revealing the robustness and optimal operating regions of our framework. (a) Reservoir size impact on accuracy and computational cost, (b) Spectral radius effects on stability and performance, (c) Sparsity level trade-offs between efficiency and expressiveness, (d) Regularization parameter effects on generalization.}
    \label{fig:hyperparameter_analysis}
\end{figure}
The sensitivity analysis reveals that our framework exhibits robust performance across reasonable hyperparameter ranges, with clear optimal operating regions that balance performance and computational efficiency. Specifically, the spectral scaling factor $\mu_\beta = 0.01$ provides the optimal balance between forward and backward dynamics, ensuring that the reservoir operates near the "edge of chaos" while maintaining stability for long-term dependencies. This robustness is crucial for practical deployment, as it reduces the need for extensive hyperparameter tuning in new application scenarios.

\subsection{Training and Inference Performance Analysis}\label{subsec:computational_efficiency}

This section provides an analysis of the computational characteristics of the proposed approach, demonstrating its suitability for practical deployment scenarios.  
Table~\ref{tab:empirical_complexity} illustrates a detailed comparison of computational complexity between our framework and existing approaches, revealing the theoretical foundations of our efficiency advantages.
\begin{table}[!ht]
\centering
\footnotesize
\caption{Theoretical complexity analysis with realistic FLOP estimates for sequential models. Parameters: $T=300$, $D=1000$, $H=1000$, $E=100$, $P=432$, $\gamma=0.05$, $K=100$, $C=27$, input dimension $=75$ (3D $\times$ 25 joints).}
\label{tab:empirical_complexity}
\begin{tabular}{l c c c c c c}
\toprule
\textbf{Method} & \textbf{Train. Complexity} & \textbf{Inf. Complexity} & \textbf{Memory} & \textbf{Params} & \textbf{Train. FLOPs} & \textbf{Inf. FLOPs} \\
\midrule
RNN & $\mathcal{O}(T  D^2  E)$ & $\mathcal{O}(T  D^2)$ & $\mathcal{O}(D^2)$ & $\mathcal{O}(D^2)$ & $5.57 \times 10^{16}$ & $3.23 \times 10^{8}$ \\
LSTM & $\mathcal{O}(4  T  D^2  E)$ & $\mathcal{O}(4  T  D^2)$ & $\mathcal{O}(4 D^2)$ & $\mathcal{O}(4 D^2)$ & $2.23 \times 10^{17}$ & $1.29 \times 10^{9}$ \\
Bi-LSTM & $\mathcal{O}(8  T  D^2  E)$ & $\mathcal{O}(8  T  D^2)$ & $\mathcal{O}(8 D^2)$ & $\mathcal{O}(8 D^2)$ & $4.46 \times 10^{17}$ & $2.58 \times 10^{9}$ \\
GRU & $\mathcal{O}(3  T  D^2  E)$ & $\mathcal{O}(3  T  D^2)$ & $\mathcal{O}(3 D^2)$ & $\mathcal{O}(3 D^2)$ & $1.67 \times 10^{17}$ & $9.68 \times 10^{8}$ \\
Standard ESN & $\mathcal{O}(T  H  \gamma  P + H^3)$ & $\mathcal{O}(T  H  \gamma)$ & $\mathcal{O}(H^2  \gamma)$ & $\mathcal{O}(H  C)$ & $1.01 \times 10^{9}$ & $3.75 \times 10^{7}$ \\
Proposed Bi-RC & $\mathcal{O}(2  T  H  \gamma  P + K^3)$ & $\mathcal{O}(2  T  H  \gamma)$ & $\mathcal{O}(2 H^2  \gamma)$ & $\mathcal{O}(K \cdot C)$ & $1.40 \times 10^{7}$ & $3.00 \times 10^{4}$ \\
\bottomrule
\end{tabular}
\end{table}
Indeed, while traditional RNN-based methods scale quadratically with the hidden dimension and require expensive gradient computation through time, our Bi-Reservoir RC framework scales linearly with the number of reservoir units and eliminates the need for temporal gradient propagation.
Training time shows a marked improvement: the RC framework completes training in approximately 2.7 minutes, compared to 27 and 54 minutes for RNNs and LSTMs, respectively. This efficiency stems from the simplified training procedure that avoids backpropagation through time.
Inference time also demonstrates notable gains: the proposed RC framework processes each sequence in approximately 25 milliseconds, outperforming traditional RNN approaches that require 192--384 milliseconds per sequence.
Figure~\ref{fig:computational_gain_comparison} shows the practical computational advantages of our approach across multiple performance dimensions.

\begin{figure}[htbp]
    \centering
    \begin{adjustbox}{center,max width=0.9\textwidth}
        \includegraphics[width=1.1\textwidth]{computational_performance_comparison.png}
    \end{adjustbox}
    \caption{Comprehensive computational performance comparison across training time, inference speed, memory usage, and energy consumption.}
    \label{fig:computational_gain_comparison}
\end{figure}
\subsection{Computational Overhead and Pareto Analysis}
\label{subsec:comp_overhead}

A critical advantage of the proposed Bi-RC framework is its exceptional computational efficiency, which we quantify through wall-clock measurements and memory profiling. As shown in Table~\ref{tab:computational_overhead}, our model maintains a minimal memory footprint and ultra-low latency compared to deep GCN and LSTM baselines. 

\begin{table}[!htbp]
\centering
\footnotesize
\caption{Consolidated computational complexity, resource utilization, and accuracy comparison on NTU-60 (Cross-Subject). Our Bi-RC framework defines the efficiency frontier, achieving superior accuracy while providing an average 15$\times$ reduction in parameters and 20$\times$ reduction in inference latency compared to efficient GCN baselines.}
\label{tab:computational_overhead}
\begin{tabular}{l c c c c c}
\toprule
\textbf{Model} & \textbf{Params (M)} & \textbf{FLOPs (G)} & \textbf{Train (min)} & \textbf{Inf. (ms)} & \textbf{Acc (\%)} \\
\midrule
ST-GCN \cite{yan2018spatial} & 3.1 & 16.3 & $\sim$480 & 18.2 & 81.5 \\
CTR-GCN \cite{chen2021channel} & 1.5 & 2.1 & $\sim$240 & 5.4 & 92.4 \\
Bi-LSTM \cite{schuster1997bidirectional} & 2.8 & 0.3 & 54 & 4.1 & 62.9\footnotemark[1] \\
EfficientGCN \cite{song2023efficientgcn} & 1.3 & 4.5 & $\sim$180 & 5.1 & 91.7 \\
\midrule
\textbf{Proposed Bi-RC} & \textbf{0.08} & \textbf{0.015} & \textbf{2.7} & \textbf{0.22} & \textbf{$93.2 \pm 0.2$} \\
\bottomrule
\end{tabular}
\footnotetext[1]{Accuracy reported for Part-aware LSTM baseline on NTU-60 Cross-Subject.}
\end{table}

The wall-clock overhead for the Tucker decomposition module is particularly noteworthy, requiring only 0.22 ms per sequence during inference, which represents less than 1.5\% of the total processing pipeline. This efficiency enables the model to operate at over 1000 FPS on standard CPU hardware, making it ideal for edge deployment.

\noindent\textbf{Comparison with Efficient Baselines:} While recent methods like EfficientGCN \cite{song2023efficientgcn} and PoseConv3D \cite{duan2022poseconv3d} achieve competitive efficiency through optimized graph convolutions or 3D volumes, they still rely on learned features that require gradient-based updates. In contrast, our Bi-RC approach decouples temporal feature extraction from gradient-based optimization, utilizing a fixed-dynamics reservoir that (100$\times$) faster to train. 

The Pareto Frontier analysis in Figure~\ref{fig:pareto_analysis} further illustrates our contribution. While top-tier GCNs (e.g., HD-GCN) achieve slightly higher accuracy on NTU120, they require nearly 100$\times$ more FLOPs, positioning our Bi-RC model as the optimal choice for resource-constrained applications.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{pareto_analysis.png}
    \caption{Pareto Analysis of Accuracy vs. Computational Complexity (GFLOPs) on NTU60 Cross-Subject. Our Bi-RC model defines the ultra-efficient frontier, providing near-SOTA performance with negligible computational cost.}
    \label{fig:pareto_analysis}
\end{figure}
The framework achieves real-time processing capabilities ($>$30 FPS) on modern hardware while maintaining full accuracy. Even on resource-constrained platforms like Raspberry Pi 4, the framework maintains its accuracy while operating at acceptable frame rates for many practical applications.

\begin{table}[htbp]
\centering
\footnotesize
\caption{Real-time performance evaluation across diverse hardware configurations.}
\label{tab:realtime_performance}
\begin{tabular}{lccc}
\toprule
\textbf{Hardware Configuration} & \textbf{FPS} & \textbf{Latency (ms)} & \textbf{Power (W)} \\
\midrule
RTX 3050 (Desktop) & 40.0 & 25 & 130 \\
GTX 1660 (Laptop) & 28.5 & 35 & 120 \\
Intel i7 (CPU only) & 15.2 & 66 & 65 \\
Raspberry Pi 4\footnotemark & 18.0 & 55 & 7 \\
\bottomrule
\end{tabular}
\footnotetext{Performance measured using an optimized ONNX Runtime implementation, achieving real-time performance suitable for edge applications.}
\end{table}

\subsection{Robustness and Statistical Validation}\label{subsec:robustness_validation}

This section explores the robustness characteristics of our framework through noise analysis while providing a rigorous statistical validation of its performance claims.
To evaluate the robustness of the framework to input noise, we conducted systematic experiments with different levels of Gaussian noise added to the skeleton joint coordinates. Figure~\ref{fig:noise_robustness} presents the robustness analysis across different noise levels.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{noise_robustness_analysis1.png}
    \caption{Noise robustness analysis showing (a) performance degradation under different levels of input noise, demonstrating the superior stability of our framework compared to baseline methods, and (b) relative performance drop comparison across different noise levels, highlighting the exceptional noise tolerance of our bidirectional approach.}
    \label{fig:noise_robustness}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{noise_robustness_analysis.png}
    \caption{Detailed analysis of noise robustness providing information on the framework stability: (a) Performance curves across all noise levels, (b) Performance drop rates showing degradation velocity, (c) Robustness ranking at 10\% noise level demonstrating our method's superiority, and (d) Noise tolerance thresholds indicating the maximum noise level each method can handle while maintaining 90\% of original performance.}
    \label{fig:detailed_noise_analysis}
\end{figure}

The robustness analysis reveals that our framework maintains superior performance even under significant noise conditions, with graceful degradation that outperforms baseline methods across all noise levels tested.
To ensure the reliability of our performance claims, we conduct rigorous statistical significance testing using paired t-tests \cite{student1908probable} across multiple experimental runs with different random seeds. Table~\ref{tab:statistical_significance} presents the statistical validation of our performance improvements.
\begin{table}[htbp]
\centering
\footnotesize
\caption{Statistical significance analysis confirming the reliability and significance of performance improvements achieved by our framework across 50 independent runs.}
\label{tab:statistical_significance}
\begin{tabular}{l c c c}
\toprule
\textbf{Comparison} & \textbf{UTD-MHAD} & \textbf{MSR Action3D} & \textbf{CZU-MHAD} \\
\midrule
Our Framework vs. LSTM & $p < 0.001$ & $p < 0.001$ & $p < 0.001$ \\
Our Framework vs. Bi-LSTM & $p < 0.001$ & $p < 0.001$ & $p < 0.001$ \\
Our Framework vs. ST-GCN & $p < 0.01$ & $p < 0.01$ & $p < 0.01$ \\
Our Framework vs. CTR-GCN & $p < 0.05$ & $p < 0.05$ & $p < 0.05$ \\
\bottomrule
\end{tabular}
\end{table}
All performance improvements are statistically significant with p $<$ 0.05, providing strong evidence for the effectiveness of our approach and ensuring that our claims are supported by rigorous statistical validation.

\subsection{Class-wise Performance Analysis}\label{subsec:classwise_analysis}

Understanding the performance of the proposed framework across different action classes provides valuable insights into its robustness and generalization capabilities. Figure~\ref{fig:classwise_combined} presents the detailed class-wise performance analysis across the three evaluation datasets: UTD-MHAD, MSR Action3D, and CZU-MHAD.
\begin{figure*}[]
\centering
\includegraphics[width=0.9\textwidth]{Class-wise_Performance_Metrics_UTD-MHAD.png}\\[1em]
\includegraphics[width=0.9\textwidth]{Class-wise_Performance_Metrics_MSR_Action3D.png}\\[1em]
\includegraphics[width=0.9\textwidth]{Class-wise_Performance_Metrics_CZU-MHAD.png}
\caption{Comprehensive class-wise performance analysis across three benchmark datasets: 
(a) UTD-MHAD, showing consistently high performance across diverse action categories, particularly in complex multi-joint actions; 
(b) MSR Action3D, demonstrating robust recognition across horizontal, vertical, and complex movement patterns; and 
(c) CZU-MHAD, highlighting stable accuracy across a wide range of contemporary human action classes.}
\label{fig:classwise_combined}
\end{figure*}
The class-wise analysis reveals several key findings. The proposed framework achieves consistently strong performance across various action categories, with most classes attaining precision, recall, and F1-scores above 90\%. This indicates that the model effectively captures fundamental patterns of human motion that generalize well across distinct types of actions.
Notably, the framework performs exceptionally well on complex actions requiring multi-joint coordination, such as \textit{basketball shoot} and \textit{baseball swing}. These results demonstrate the capability of the bidirectional temporal modeling to capture intricate spatiotemporal dependencies.
Furthermore, the model successfully distinguishes between visually or kinematically similar actions that differ primarily in subtle temporal or spatial characteristics---such as directional variations or execution styles---underscoring its fine-grained discriminative power.
To further investigate the discriminative power of the proposed framework, we analyze the learned feature representations using t-distributed Stochastic Neighbor Embedding (t-SNE) across different benchmarks in Figure~\ref{fig:tsne_unified}, and evaluate class-wise confusion characteristics in Figure~\ref{fig:confusion_unified}.

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{tsne_utd.png}
        \caption{UTD-MHAD}
        \label{fig:tsne_utd}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{tsne_czu.png}
        \caption{CZU-MHAD}
        \label{fig:tsne_czu}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{tsne_ntu.png}
        \caption{NTU-RGBD 60}
        \label{fig:tsne_ntu}
    \end{subfigure}
    \caption{Unified t-SNE visualization of learned reservoir feature embeddings across diverse benchmarks. The clear spatial separation and tight clustering in the 2D latent space demonstrate the model's ability to capture discriminative temporal dynamics for both small-scale and large-scale action recognition tasks.}
    \label{fig:tsne_unified}
\end{figure*}

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{confusion_utd.png}
        \caption{UTD-MHAD}
        \label{fig:confusion_utd}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{confusion_czu.png}
        \caption{CZU-MHAD}
        \label{fig:confusion_czu}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{confusion_ntu.png}
        \caption{NTU-RGBD 60}
        \label{fig:confusion_ntu}
    \end{subfigure}
    \caption{Unified confusion matrix analysis across the evaluated datasets. The high diagonal dominance reflects robust recognition performance, with the model effectively minimizing inter-class errors even as the action category count scales from 22 (CZU) and 27 (UTD) to 60 (NTU).}
    \label{fig:confusion_unified}
\end{figure*}

As shown in Figure~\ref{fig:tsne_unified}, the features extracted from the bidirectional reservoir framework exhibit clear spatial separation in the reduced 2D space. This suggests that the model effectively maps raw skeletal sequences into a latent space where action-specific dynamics are well-clustered. Furthermore, the confusion matrices in Figure~\ref{fig:confusion_unified} confirm that the Advanced Readout mechanism with Maxout activation successfully minimizes inter-class errors, with most misclassifications occurring between kinematically similar activities (e.g., subtle differences in arm gestures in NTU-60 or gesture-based actions in CZU-MHAD).

\section{Limitations and Future Work}
\label{sec:limitations}

While our bidirectional reservoir computing framework demonstrates significant advantages in efficiency and accuracy, several limitations acknowledge the trade-offs inherent in this approach.
\begin{enumerate}
    \item \textbf{Hyperparameter Sensitivity:} The proposed architecture involves multiple components (bidirectional reservoirs, Tucker decomposition, MLP readout), each introducing hyperparameters such as spectral radius, sparsity, and decomposition ranks. While we provided a sensitivity analysis, the optimal configuration can be dataset-dependent, potentially requiring automated hyperparameter search strategies for new domains.
    \item \textbf{Model Complexity vs. Efficiency:} The proposed architecture is \textbf{modular and computationally lightweight}, even though the multi-stage pipeline involves several distinct components. It is important to distinguish between \textit{structural modularity} and \textit{computational cost}. While the implementation involves PCA, Tucker decomposition, and bidirectional reservoirs, each stage is mathematically lean and avoids the million-parameter optimization loops of deep GCNs. The structural complexity enables the "learning-free" temporal dynamics that provide the observed 95\% reduction in training cost.
    \item \textbf{Generalization Risks:} The observed high accuracies ($>90\%$)
 on the selected benchmarks may suggest saturation or potential overfitting to the specific lab-controlled environments of UTD-MHAD and MSR Action3D. Although regularization techniques (ridge regression, dropout) were employed, validation on larger-scale, in-the-wild datasets like NTU RGB+D is a critical next step to confirm scalability.
    \item \textbf{Modality Constraint:} The current framework is specialized for skeletal data. Its extension to pixel-based modalities (RGB, Depth) would require replacing the coordinate-based input layer with more complex feature extractors (e.g., CNNs), potentially offsetting the efficiency gains.
    \item \textbf{Multi-Person Scenarios:} The current framework processes subjects independently, potentially overlooking complex inter-person dynamics (e.g., "shaking hands") that require simultaneous joint modeling.
    \item \textbf{Attractor Sensitivity vs. Deep Denoising:} A fundamental trade-off of the "learning-free" reservoir dynamics is the reduced capacity for \textit{structural denoising} compared to end-to-end trained deep models. While architectures like CTR-GCN \cite{chen2021channel} can learn to dynamically ignore low-confidence OpenPose joints through backpropagation-tuned attention, our framework relies on fixed temporal attractors. In high-noise, "in-the-wild" scenarios (e.g., Kinetics-Skeleton), significant skeletal artifacts can destabilize the fading memory property of the reservoir, leading to performance degradation that purely optimized models are better equipped to mitigate.
\end{enumerate}

Future work will focus on three directions: (1) Integrating automated hyperparameter optimization (e.g., Bayesian optimization) to simplify deployment; (2) Extending the evaluation to "in-the-wild" skeletal data from wearable sensors or low-cost cameras; and (3) Exploring the application of this bidirectional RC paradigm to other time-series domains, such as physiological signal analysis or financial forecasting.

\subsection{Qualitative Analysis and Failure Case Analysis}
\label{subsec:failure_analysis}

While our Bi-RC framework demonstrates consistent performance gains, particularly in terms of efficiency, an analysis of misclassifications reveals specific environmental and motion-related limitations. 

On the \textbf{Kinetics-Skeleton} dataset, where accuracy reaches 38.7\%, common failure cases are associated with significant skeletal noise and "in-the-wild" artifacts. It is important to note that while this performance surpasses standard RNN baselines, it falls slightly below the best reported pure-skeleton accuracy of 39.0\% by SA-TDGFormer \cite{chen2025twostream}. This gap represents a fundamental trade-off: deeply trained models like SA-TDGFormer or CTR-GCN \cite{chen2021channel} require intensive GPU training to learn spatial-temporal refinements, whereas our Bi-RC framework completes training in minutes on a standard setup, yielding a highly competitive accuracy-to-cost ratio.
\begin{itemize}
    \item \textbf{Efficiency vs. Robustness Trade-off}: Quantitative analysis reveals that the fixed reservoir dynamics are sensitive to low-confidence skeletal data. Specifically, for sequences where the average OpenPose joint confidence is below 0.4, the framework's accuracy drops sharply to 42.1\% (compared to 68.3\% for sequences with confidence $>$ 0.7). In contrast, deep GCNs with learned attention can partially "ignore" low-confidence joints through backpropagation-tuned weights, maintaining higher robustness at a much higher training cost.
    \item \textbf{Motion Complexity}: We observe a distinct relationship between motion complexity (measured by joint velocity variance) and recognition precision. Atomic actions with low spatial variance (e.g., "sitting up") are recognized at 72.4\%, whereas high-entropy, complex interactions (e.g., "massaging back") drop to 31.5\%. This "complexity threshold" confirms that while RC is highly effective for canonical temporal dynamics, it lacks the compositional reasoning required for hierarchical, small-amplitude interactions.
\end{itemize}


On the \textbf{NTU RGB+D} dataset, misclassifications frequently occur between kinematically similar classes, such as "rubbing hands" versus "clapping" or "writing" versus "typing." These actions involve subtle, small-amplitude joint movements that are partially smoothed out by the Tucker dimensionality reduction module. Future work integrating multi-modal cues (RGB or depth) alongside the skeletal representation could mitigate these subtle confusion patterns.



\section{Conclusion and Future Work}\label{sec:conclusion}
This study introduced an efficient and accurate framework for skeleton-based human action recognition (HAR) grounded in bidirectional reservoir computing. The proposed architecture unifies forward--backward temporal encoding, adaptive tensor-based dimensionality reduction, and advanced readout learning to capture comprehensive motion dynamics with minimal computational cost.   
Experimental results across three benchmark datasets (UTD-MHAD, MSR Action3D, CZU-MHAD) demonstrate consistent performance gains, achieving up to 3\% higher accuracy than unidirectional approaches
, while reducing computational demands by over 20$\times$ during training and 15$\times$ during inference compared to bidirectional LSTMs. The framework also sustains real-time throughput ($>$30 FPS) on diverse hardware platforms, validating its practicality for real-world applications such as healthcare monitoring, assistive systems, and natural user interfaces.  
Despite these strengths, several limitations remain. The evaluation was conducted primarily on controlled indoor datasets with high-quality skeleton data. Future work should extend experiments to outdoor and unconstrained settings with variable lighting, occlusions, and multi-person interactions. Furthermore, large-scale validation on diverse datasets would help assess generalization across different action categories.  
Promising research avenues include: (1) incorporating multi-modal fusion with RGB, depth, or inertial signals; (2) developing adaptive temporal attention mechanisms that can operate without explicit interpolation; (3) enabling online and incremental learning for streaming data; and (4) exploring lightweight quantization or neuromorphic hardware implementations for deployment on edge devices.  
By establishing reservoir computing as a viable and scalable alternative to recurrent neural networks for temporal modeling, this work provides both a theoretical foundation and a practical pathway toward next-generation, resource-efficient action recognition systems.


\backmatter

\section*{Declarations}

\textbf{Funding:} No funding was received for this work.

\textbf{Conflict of interest/Competing interests:} The authors declare that they have no competing interests.

\textbf{Ethics approval and consent to participate:} Not applicable.

\textbf{Consent for publication:} Not applicable.

\textbf{Data availability:} All datasets used in this research are publicly available. The UTD-MHAD dataset can be accessed at \url{https://personal.utdallas.edu/~kehtar/UTD-MHAD.html}. The MSR Action3D dataset is available at \url{https://www.uow.edu.au/~wanqing/#Datasets}. The CZU-MHAD dataset is located at \url{https://github.com/daehyun-kim/CZU-MHAD}. The NTU RGB+D (60 and 120) datasets can be requested from \url{https://rose1.ntu.edu.sg/dataset/actionRecognition/}. Kinetics-Skeleton data is available through \url{https://github.com/yysijie/st-gcn}, and the Northwestern-UCLA dataset can be found at \url{https://github.com/Shao-Hwa-Chuang/Northwestern-UCLA-Dataset-Processing}.

\textbf{Materials availability:} Not applicable.

\textbf{Code availability:} The complete source code for the framework, including training scripts and implementation details, is publicly available on GitHub at \url{https://github.com/haythemghz/HAR-Bidirectional-RC}.

\textbf{Author contribution:} All authors contributed equally to the work.

\bibliographystyle{sn-mathphys-num}
\bibliography{sn-bibliography}% common bib file



\end{document}
